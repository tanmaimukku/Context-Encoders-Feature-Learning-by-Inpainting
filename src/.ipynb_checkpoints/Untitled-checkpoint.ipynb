{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91709\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ipdb\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from PIL import ImageFile\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader and Random Square Crop\n",
    "\n",
    "The data loader function defined as load_image, reads the image from the given path and reshapes it to the required height and width given by the use.\n",
    "\n",
    "The function crop_random helps us crop random square from the input image with the defined width and height and overlap. We iterate with changing the position and the given maximum overlap criteria for the training step. The test set is also cropped using the same function to obtain the corrupted images for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_image( path, height=128, width=128 ):\n",
    "def load_image( path, pre_height=146, pre_width=146, height=128, width=128 ):\n",
    "\n",
    "    try:\n",
    "        img = skimage.io.imread( path ).astype( float )\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    img /= 255.\n",
    "\n",
    "    if img is None: return None\n",
    "    if len(img.shape) < 2: return None\n",
    "    if len(img.shape) == 4: return None\n",
    "    if len(img.shape) == 2: img=np.tile(img[:,:,None], 3)\n",
    "    if img.shape[2] == 4: img=img[:,:,:3]\n",
    "    if img.shape[2] > 4: return None\n",
    "\n",
    "    short_edge = min( img.shape[:2] )\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy:yy+short_edge, xx:xx+short_edge]\n",
    "    resized_img = skimage.transform.resize( crop_img, [pre_height,pre_width] )\n",
    "\n",
    "    rand_y = np.random.randint(0, pre_height - height)\n",
    "    rand_x = np.random.randint(0, pre_width - width)\n",
    "\n",
    "    resized_img = resized_img[ rand_y:rand_y+height, rand_x:rand_x+width, : ]\n",
    "\n",
    "    return (resized_img * 2)-1 #(resized_img - 127.5)/127.5\n",
    "\n",
    "def crop_random(image_ori, width=64,height=64, x=None, y=None, overlap=7):\n",
    "\n",
    "    if image_ori is None: return None\n",
    "    random_y = np.random.randint(overlap,height-overlap) if x is None else x\n",
    "    random_x = np.random.randint(overlap,width-overlap) if y is None else y\n",
    "\n",
    "    image = image_ori.copy()\n",
    "    crop = image_ori.copy()\n",
    "    crop = crop[random_y:random_y+height, random_x:random_x+width]\n",
    "    image[random_y + overlap:random_y+height - overlap, random_x + overlap:random_x+width - overlap, 0] = 2*117. / 255. - 1.\n",
    "    image[random_y + overlap:random_y+height - overlap, random_x + overlap:random_x+width - overlap, 1] = 2*104. / 255. - 1.\n",
    "    image[random_y + overlap:random_y+height - overlap, random_x + overlap:random_x+width - overlap, 2] = 2*123. / 255. - 1.\n",
    "    return image, crop, random_x, random_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model:\n",
    "The model is defined as follows in the class Model() using tensorflow. Four functions for creating new conv layer, deconv layer, fully connected layer and channel wise fully connected layer are defined. The reconstruction function defines the bottle neck architecture of encoder and decoder with channel wise fully connected layer in between. And Adversial part is created accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def new_conv_layer( self, bottom, filter_shape, activation=tf.identity, padding='SAME', stride=1, name=None ):\n",
    "        with tf.compat.v1.variable_scope( name ):\n",
    "            w = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=filter_shape,\n",
    "                    initializer=tf.random_normal_initializer(0., 0.005))\n",
    "            b = tf.get_variable(\n",
    "                    \"b\",\n",
    "                    shape=filter_shape[-1],\n",
    "                    initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            conv = tf.nn.conv2d( bottom, w, [1,stride,stride,1], padding=padding)\n",
    "            bias = activation(tf.nn.bias_add(conv, b))\n",
    "\n",
    "        return bias #relu\n",
    "\n",
    "    def new_deconv_layer(self, bottom, filter_shape, output_shape, activation=tf.identity, padding='SAME', stride=1, name=None):\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "            W = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=filter_shape,\n",
    "                    initializer=tf.random_normal_initializer(0., 0.005))\n",
    "            b = tf.get_variable(\n",
    "                    \"b\",\n",
    "                    shape=filter_shape[-2],\n",
    "                    initializer=tf.constant_initializer(0.))\n",
    "            deconv = tf.nn.conv2d_transpose( bottom, W, output_shape, [1,stride,stride,1], padding=padding)\n",
    "            bias = activation(tf.nn.bias_add(deconv, b))\n",
    "\n",
    "        return bias\n",
    "\n",
    "    def new_fc_layer( self, bottom, output_size, name ):\n",
    "        shape = bottom.get_shape().as_list()\n",
    "        dim = np.prod( shape[1:] )\n",
    "        x = tf.reshape( bottom, [-1, dim])\n",
    "        input_size = dim\n",
    "\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "            w = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[input_size, output_size],\n",
    "                    initializer=tf.random_normal_initializer(0., 0.005))\n",
    "            b = tf.get_variable(\n",
    "                    \"b\",\n",
    "                    shape=[output_size],\n",
    "                    initializer=tf.constant_initializer(0.))\n",
    "            fc = tf.nn.bias_add( tf.matmul(x, w), b)\n",
    "\n",
    "        return fc\n",
    "\n",
    "    def channel_wise_fc_layer(self, input, name): # bottom: (7x7x512)\n",
    "        _, width, height, n_feat_map = input.get_shape().as_list()\n",
    "        input_reshape = tf.reshape( input, [-1, width*height, n_feat_map] )\n",
    "        input_transpose = tf.transpose( input_reshape, [2,0,1] )\n",
    "\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "            W = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[n_feat_map,width*height, width*height], # (512,49,49)\n",
    "                    initializer=tf.random_normal_initializer(0., 0.005))\n",
    "            output = tf.batch_matmul(input_transpose, W)\n",
    "\n",
    "        output_transpose = tf.transpose(output, [1,2,0])\n",
    "        output_reshape = tf.reshape( output_transpose, [-1, height, width, n_feat_map] )\n",
    "\n",
    "        return output_reshape\n",
    "\n",
    "    def leaky_relu(self, bottom, leak=0.1):\n",
    "        return tf.maximum(leak*bottom, bottom)\n",
    "\n",
    "    def batchnorm(self, bottom, is_train, epsilon=1e-8, name=None):\n",
    "        bottom = tf.clip_by_value( bottom, -100., 100.)\n",
    "        depth = bottom.get_shape().as_list()[-1]\n",
    "\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "\n",
    "            gamma = tf.get_variable(\"gamma\", [depth], initializer=tf.constant_initializer(1.))\n",
    "            beta  = tf.get_variable(\"beta\" , [depth], initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            batch_mean, batch_var = tf.nn.moments(bottom, [0,1,2], name='moments')\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "\n",
    "            def update():\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "            mean, var = tf.cond(\n",
    "                    is_train,\n",
    "                    update,\n",
    "                    lambda: (ema_mean, ema_var) )\n",
    "\n",
    "            normed = tf.nn.batch_norm_with_global_normalization(bottom, mean, var, beta, gamma, epsilon, False)\n",
    "        return normed\n",
    "\n",
    "    def build_reconstruction( self, images, is_train ):\n",
    "        batch_size = images.get_shape().as_list()[0]\n",
    "\n",
    "        with tf.compat.v1.variable_scope('GEN'):\n",
    "            conv1 = self.new_conv_layer(images, [4,4,3,64], stride=2, name=\"conv1\" )\n",
    "            bn1 = self.leaky_relu(self.batchnorm(conv1, is_train, name='bn1'))\n",
    "            conv2 = self.new_conv_layer(bn1, [4,4,64,64], stride=2, name=\"conv2\" )\n",
    "            bn2 = self.leaky_relu(self.batchnorm(conv2, is_train, name='bn2'))\n",
    "            conv3 = self.new_conv_layer(bn2, [4,4,64,128], stride=2, name=\"conv3\")\n",
    "            bn3 = self.leaky_relu(self.batchnorm(conv3, is_train, name='bn3'))\n",
    "            conv4 = self.new_conv_layer(bn3, [4,4,128,256], stride=2, name=\"conv4\")\n",
    "            bn4 = self.leaky_relu(self.batchnorm(conv4, is_train, name='bn4'))\n",
    "            conv5 = self.new_conv_layer(bn4, [4,4,256,512], stride=2, name=\"conv5\")\n",
    "            bn5 = self.leaky_relu(self.batchnorm(conv5, is_train, name='bn5'))\n",
    "            conv6 = self.new_conv_layer(bn5, [4,4,512,4000], stride=2, padding='VALID', name='conv6')\n",
    "            bn6 = self.leaky_relu(self.batchnorm(conv6, is_train, name='bn6'))\n",
    "\n",
    "            deconv4 = self.new_deconv_layer( bn6, [4,4,512,4000], conv5.get_shape().as_list(), padding='VALID', stride=2, name=\"deconv4\")\n",
    "            debn4 = tf.nn.relu(self.batchnorm(deconv4, is_train, name='debn4'))\n",
    "            deconv3 = self.new_deconv_layer( debn4, [4,4,256,512], conv4.get_shape().as_list(), stride=2, name=\"deconv3\")\n",
    "            debn3 = tf.nn.relu(self.batchnorm(deconv3, is_train, name='debn3'))\n",
    "            deconv2 = self.new_deconv_layer( debn3, [4,4,128,256], conv3.get_shape().as_list(), stride=2, name=\"deconv2\")\n",
    "            debn2 = tf.nn.relu(self.batchnorm(deconv2, is_train, name='debn2'))\n",
    "            deconv1 = self.new_deconv_layer( debn2, [4,4,64,128], conv2.get_shape().as_list(), stride=2, name=\"deconv1\")\n",
    "            debn1 = tf.nn.relu(self.batchnorm(deconv1, is_train, name='debn1'))\n",
    "            recon = self.new_deconv_layer( debn1, [4,4,3,64], [batch_size,64,64,3], stride=2, name=\"recon\")\n",
    "\n",
    "        return bn1, bn2, bn3, bn4, bn5, bn6, debn4, debn3, debn2, debn1, recon, tf.nn.tanh(recon)\n",
    "\n",
    "    def build_adversarial(self, images, is_train, reuse=None):\n",
    "        with tf.compat.v1.variable_scope('DIS', reuse=tf.AUTO_REUSE):\n",
    "            conv1 = self.new_conv_layer(images, [4,4,3,64], stride=2, name=\"conv1\" )\n",
    "            bn1 = self.leaky_relu(self.batchnorm(conv1, is_train, name='bn1'))\n",
    "            conv2 = self.new_conv_layer(bn1, [4,4,64,128], stride=2, name=\"conv2\")\n",
    "            bn2 = self.leaky_relu(self.batchnorm(conv2, is_train, name='bn2'))\n",
    "            conv3 = self.new_conv_layer(bn2, [4,4,128,256], stride=2, name=\"conv3\")\n",
    "            bn3 = self.leaky_relu(self.batchnorm(conv3, is_train, name='bn3'))\n",
    "            conv4 = self.new_conv_layer(bn3, [4,4,256,512], stride=2, name=\"conv4\")\n",
    "            bn4 = self.leaky_relu(self.batchnorm(conv4, is_train, name='bn4'))\n",
    "\n",
    "            output = self.new_fc_layer( bn4, output_size=1, name='output')\n",
    "\n",
    "        return output[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Paris-Streetview dataset\n",
    "We use the following hyperparameters as discussed in the paper:\n",
    "- epochs = 30\n",
    "- learning_rate_ = 0.002\n",
    "- weight_decay_rate =  0.00001\n",
    "- momentum = 0.9\n",
    "- batch_size = 100\n",
    "- lambda_recon = 0.999\n",
    "- lambda_adv = 0.001\n",
    "- overlap_size = 7\n",
    "- hiding_size = 64\n",
    "\n",
    "The dataset used is Paris street view dataset which has also been tested in the paper. The dataset consists of \n",
    "- Training images =14,900 images\n",
    "- Test images= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "n_epochs = 30\n",
    "learning_rate_val = 0.002\n",
    "weight_decay_rate =  0.00001\n",
    "momentum = 0.9\n",
    "batch_size = 100\n",
    "lambda_recon = 0.999\n",
    "lambda_adv = 0.001\n",
    "\n",
    "overlap_size = 7\n",
    "hiding_size = 64\n",
    "\n",
    "trainset_path = '../data/paris_trainset.pickle'\n",
    "testset_path  = '../data/paris_testset.pickle'\n",
    "dataset_path = 'C:/Users/91709/Desktop/CV/Paris_street_view'\n",
    "model_path = 'C:/Users/91709/Desktop/CV/models/Paris2/'\n",
    "result_path= '../results/Paris2/'\n",
    "pretrained_model_path = 'C:/Users/91709/Desktop/CV/models/Paris2/model-24'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs( model_path )\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs( result_path )\n",
    "\n",
    "if not os.path.exists( trainset_path ) or not os.path.exists( testset_path ):\n",
    "\n",
    "    trainset_dir = os.path.join( dataset_path, 'paris_train_original' )\n",
    "    testset_dir = os.path.join( dataset_path, 'paris_eval_gt' )\n",
    "\n",
    "    trainset = pd.DataFrame({'image_path': map(lambda x: os.path.join( trainset_dir, x ), os.listdir(trainset_dir))})\n",
    "    testset = pd.DataFrame({'image_path': map(lambda x: os.path.join( testset_dir, x ), os.listdir(testset_dir))})\n",
    "\n",
    "    trainset.to_pickle( trainset_path )\n",
    "    testset.to_pickle( testset_path )\n",
    "else:\n",
    "    trainset = pd.read_pickle( trainset_path )\n",
    "    testset = pd.read_pickle( testset_path )\n",
    "\n",
    "testset.index = range(len(testset))\n",
    "testset = testset.loc[np.random.permutation(len(testset))]\n",
    "is_train = tf.placeholder( tf.bool )\n",
    "\n",
    "learning_rate = tf.placeholder( tf.float32, [])\n",
    "images_tf = tf.placeholder( tf.float32, [batch_size, 128, 128, 3], name=\"images\")\n",
    "\n",
    "labels_D = tf.concat([tf.ones([batch_size]), tf.zeros([batch_size])],0)\n",
    "labels_G = tf.ones([batch_size])\n",
    "images_hiding = tf.placeholder( tf.float32, [batch_size, hiding_size, hiding_size, 3], name='images_hiding')\n",
    "\n",
    "model = Model()\n",
    "\n",
    "bn1, bn2, bn3, bn4, bn5, bn6, debn4, debn3, debn2, debn1, reconstruction_ori, reconstruction = model.build_reconstruction(images_tf, is_train)\n",
    "adversarial_pos = model.build_adversarial(images_hiding, is_train)\n",
    "adversarial_neg = model.build_adversarial(reconstruction, is_train, reuse=True)\n",
    "adversarial_all = tf.concat([adversarial_pos, adversarial_neg],0)\n",
    "\n",
    "# Applying bigger loss for overlapping region\n",
    "mask_recon = tf.pad(tf.ones([hiding_size - 2*overlap_size, hiding_size - 2*overlap_size]), [[overlap_size,overlap_size], [overlap_size,overlap_size]])\n",
    "mask_recon = tf.reshape(mask_recon, [hiding_size, hiding_size, 1])\n",
    "mask_recon = tf.concat([mask_recon]*3,2)\n",
    "mask_overlap = 1 - mask_recon\n",
    "\n",
    "loss_recon_ori = tf.square( images_hiding - reconstruction )\n",
    "loss_recon_center = tf.reduce_mean(tf.sqrt( 1e-5 + tf.reduce_sum(loss_recon_ori * mask_recon, [1,2,3])))  # Loss for non-overlapping region\n",
    "loss_recon_overlap = tf.reduce_mean(tf.sqrt( 1e-5 + tf.reduce_sum(loss_recon_ori * mask_overlap, [1,2,3]))) * 10. # Loss for overlapping region\n",
    "loss_recon = loss_recon_center + loss_recon_overlap\n",
    "\n",
    "loss_adv_D = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=adversarial_all, labels=labels_D))\n",
    "loss_adv_G = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=adversarial_neg, labels=labels_G))\n",
    "\n",
    "loss_G = loss_adv_G * lambda_adv + loss_recon * lambda_recon\n",
    "loss_D = loss_adv_D # * lambda_adv\n",
    "\n",
    "var_G = list(filter( lambda x: x.name.startswith('GEN'), tf.trainable_variables()))\n",
    "var_D = list(filter( lambda x: x.name.startswith('DIS'), tf.trainable_variables()))\n",
    "\n",
    "W_G = filter(lambda x: x.name.endswith('W:0'), var_G)\n",
    "W_D = filter(lambda x: x.name.endswith('W:0'), var_D)\n",
    "\n",
    "loss_G += weight_decay_rate * tf.reduce_mean(tf.stack( list(map(lambda x: tf.nn.l2_loss(x), W_G))))\n",
    "loss_D += weight_decay_rate * tf.reduce_mean(tf.stack( list(map(lambda x: tf.nn.l2_loss(x), W_D))))\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "optimizer_G = tf.train.AdamOptimizer( learning_rate=learning_rate, beta1=0.5 )\n",
    "grads_vars_G = optimizer_G.compute_gradients( loss_G, var_list=var_G )\n",
    "#grads_vars_G = map(lambda gv: [tf.clip_by_value(gv[0], -10., 10.), gv[1]], grads_vars_G)\n",
    "train_op_G = optimizer_G.apply_gradients( grads_vars_G )\n",
    "\n",
    "optimizer_D = tf.train.AdamOptimizer( learning_rate=learning_rate, beta1=0.5 )\n",
    "grads_vars_D = optimizer_D.compute_gradients( loss_D, var_list=var_D )\n",
    "#grads_vars_D = map(lambda gv: [tf.clip_by_value(gv[0], -10., 10.), gv[1]], grads_vars_D)\n",
    "train_op_D = optimizer_D.apply_gradients( grads_vars_D )\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "if pretrained_model_path is not None and os.path.exists( pretrained_model_path ):\n",
    "    saver.restore( sess, pretrained_model_path )\n",
    "\n",
    "iters = 0\n",
    "\n",
    "loss_D_val = 0.\n",
    "loss_G_val = 0.\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    trainset.index = range(len(trainset))\n",
    "    trainset = trainset.loc[np.random.permutation(len(trainset))]\n",
    "\n",
    "    for start,end in zip(\n",
    "            range(0, len(trainset), batch_size),\n",
    "            range(batch_size, len(trainset), batch_size)):\n",
    "\n",
    "        image_paths = trainset[start:end]['image_path'].values\n",
    "        images_ori = list(map(lambda x: load_image( x ), image_paths))\n",
    "\n",
    "        if iters % 2 == 0:\n",
    "            images_ori = list(map(lambda img: img[:,::-1,:], images_ori))\n",
    "\n",
    "        is_none = np.sum(list(map(lambda x: x is None, images_ori)))\n",
    "        if is_none > 0: continue\n",
    "\n",
    "        images_crops = list(map(lambda x: crop_random(x, x=32, y=32), images_ori))\n",
    "        images, crops,_,_ = zip(*images_crops)\n",
    "\n",
    "        # Printing activations every 100 iterations\n",
    "        if iters % 100 == 0:\n",
    "            test_image_paths = testset[:batch_size]['image_path'].values\n",
    "            test_images_ori = map(lambda x: load_image(x), test_image_paths)\n",
    "\n",
    "            test_images_crop = map(lambda x: crop_random(x, x=32, y=32), test_images_ori)\n",
    "            test_images, test_crops, xs,ys = zip(*test_images_crop)\n",
    "\n",
    "            reconstruction_vals, recon_ori_vals, bn1_val,bn2_val,bn3_val,bn4_val,bn5_val,bn6_val,debn4_val, debn3_val, debn2_val, debn1_val, loss_G_val, loss_D_val = sess.run(\n",
    "                    [reconstruction, reconstruction_ori, bn1,bn2,bn3,bn4,bn5,bn6,debn4, debn3, debn2, debn1, loss_G, loss_D],\n",
    "                    feed_dict={\n",
    "                        images_tf: test_images,\n",
    "                        images_hiding: test_crops,\n",
    "                        is_train: False\n",
    "                        })\n",
    "\n",
    "            # Generate result every 1000 iterations\n",
    "            if iters % 100 == 0:\n",
    "                ii = 0\n",
    "                for rec_val, img,x,y in zip(reconstruction_vals, test_images, xs, ys):\n",
    "                    rec_hid = (255. * (rec_val+1)/2.).astype(int)\n",
    "                    rec_con = (255. * (img+1)/2.).astype(int)\n",
    "\n",
    "                    rec_con[y:y+64, x:x+64] = rec_hid\n",
    "                    cv2.imwrite( os.path.join(result_path, 'img_'+str(ii)+'.'+str(int(iters/100))+'.jpg'), rec_con)\n",
    "                    ii += 1\n",
    "\n",
    "                if iters == 0:\n",
    "                    for test_image in test_images_ori:\n",
    "                        test_image = (255. * (test_image+1)/2.).astype(int)\n",
    "                        test_image[32:32+64,32:32+64] = 0\n",
    "                        cv2.imwrite( os.path.join(result_path, 'img_'+str(ii)+'.ori.jpg'), test_image)\n",
    "\n",
    "            print( \"========================================================================\")\n",
    "            print (bn1_val.max(), bn1_val.min())\n",
    "            print (bn2_val.max(), bn2_val.min())\n",
    "            print (bn3_val.max(), bn3_val.min())\n",
    "            print (bn4_val.max(), bn4_val.min())\n",
    "            print (bn5_val.max(), bn5_val.min())\n",
    "            print (bn6_val.max(), bn6_val.min())\n",
    "            print (debn4_val.max(), debn4_val.min())\n",
    "            print (debn3_val.max(), debn3_val.min())\n",
    "            print (debn2_val.max(), debn2_val.min())\n",
    "            print (debn1_val.max(), debn1_val.min())\n",
    "            print (recon_ori_vals.max(), recon_ori_vals.min())\n",
    "            print (reconstruction_vals.max(), reconstruction_vals.min())\n",
    "            print (loss_G_val, loss_D_val)\n",
    "            print (\"=========================================================================\")\n",
    "\n",
    "            if np.isnan(reconstruction_vals.min() ) or np.isnan(reconstruction_vals.max()):\n",
    "                print (\"NaN detected!!\")\n",
    "                ipdb.set_trace()\n",
    "\n",
    "        # Generative Part is updated every iteration\n",
    "        _, loss_G_val, adv_pos_val, adv_neg_val, loss_recon_val, loss_adv_G_val, reconstruction_vals, recon_ori_vals, bn1_val,bn2_val,bn3_val,bn4_val,bn5_val,bn6_val,debn4_val, debn3_val, debn2_val, debn1_val = sess.run([train_op_G, loss_G, adversarial_pos, adversarial_neg, loss_recon, loss_adv_G, reconstruction, reconstruction_ori, bn1,bn2,bn3,bn4,bn5,bn6,debn4, debn3, debn2, debn1],feed_dict={images_tf: images,images_hiding: crops,learning_rate: learning_rate_val,is_train: True})\n",
    "\n",
    "        _, loss_D_val, adv_pos_val, adv_neg_val = sess.run([train_op_D, loss_D, adversarial_pos, adversarial_neg],feed_dict={images_tf: images,images_hiding: crops,learning_rate: learning_rate_val/10.,is_train: True})\n",
    "\n",
    "        print (\"Iter:\", iters, \"Gen Loss:\", loss_G_val, \"Recon Loss:\", loss_recon_val, \"Gen ADV Loss:\", loss_adv_G_val,  \"Dis Loss:\", loss_D_val, \"||||\", adv_pos_val.mean(), adv_neg_val.min(), adv_neg_val.max())\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "\n",
    "    saver.save(sess, model_path + 'model', global_step=epoch)\n",
    "    learning_rate_val *= 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dogs dataset\n",
    "We use the following hyperparameters as discussed in the paper:\n",
    "- epochs = 30\n",
    "- learning_rate_ = 0.002\n",
    "- weight_decay_rate =  0.00001\n",
    "- momentum = 0.9\n",
    "- batch_size = 100\n",
    "- lambda_recon = 0.999\n",
    "- lambda_adv = 0.001\n",
    "- overlap_size = 7\n",
    "- hiding_size = 64\n",
    "\n",
    "The dataset used here is Dogs dataset **this is our contribution to the project** . \n",
    "The dataset consists of \n",
    "- Training images =4000 images\n",
    "- Test images= 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91709\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "========================================================================\n",
      "967.2388 -101.389755\n",
      "1000000.0 -100000.0\n",
      "1000000.0 -100000.0\n",
      "1000000.0 -100000.0\n",
      "1000000.0 -100000.0\n",
      "1000000.0 -100000.0\n",
      "1000000.0 0.0\n",
      "1000000.0 0.0\n",
      "1000000.0 0.0\n",
      "1000000.0 0.0\n",
      "214367.81 -219919.52\n",
      "1.0 -1.0\n",
      "1129.5441 144445.73\n",
      "=========================================================================\n",
      "Iter: 0 Gen Loss: 388.31412 Recon Loss: 388.70126 Gen ADV Loss: 0.767843 Dis Loss: 0.7092741 |||| -0.005881141 -0.03927228 0.4927073\n",
      "Iter: 1 Gen Loss: 385.50986 Recon Loss: 385.89267 Gen ADV Loss: 2.129755 Dis Loss: 0.3468411 |||| 0.8120401 -3.1758537 0.114046305\n",
      "Iter: 2 Gen Loss: 382.5323 Recon Loss: 382.91046 Gen ADV Loss: 3.6288002 Dis Loss: 0.24373095 |||| 2.2359152 -3.2161639 1.794172\n",
      "Iter: 3 Gen Loss: 376.28452 Recon Loss: 376.6563 Gen ADV Loss: 3.6364744 Dis Loss: 0.07217056 |||| 2.4522636 -7.432464 -2.5514746\n",
      "Iter: 4 Gen Loss: 372.54617 Recon Loss: 372.9133 Gen ADV Loss: 4.444803 Dis Loss: 0.037845608 |||| 3.5716393 -6.964196 -2.4466681\n",
      "Iter: 5 Gen Loss: 383.0651 Recon Loss: 383.4425 Gen ADV Loss: 4.656138 Dis Loss: 0.02519004 |||| 4.71951 -5.0472713 0.47906902\n",
      "Iter: 6 Gen Loss: 369.36584 Recon Loss: 369.72867 Gen ADV Loss: 5.528879 Dis Loss: 0.023004944 |||| 4.3911915 -7.556377 -2.7796445\n",
      "Iter: 7 Gen Loss: 369.1365 Recon Loss: 369.49826 Gen ADV Loss: 6.3444996 Dis Loss: 0.018455474 |||| 4.249012 -5.642771 -2.9863164\n",
      "Iter: 8 Gen Loss: 382.5791 Recon Loss: 382.9544 Gen ADV Loss: 6.197913 Dis Loss: 0.00939644 |||| 4.4857597 -6.5213633 -4.3452034\n",
      "Iter: 9 Gen Loss: 352.21564 Recon Loss: 352.55984 Gen ADV Loss: 6.8945155 Dis Loss: 0.022593038 |||| 4.848171 -6.5584083 -1.7070317\n",
      "Iter: 10 Gen Loss: 351.17987 Recon Loss: 351.5241 Gen ADV Loss: 5.8024974 Dis Loss: 0.09241474 |||| 3.9968026 -10.101942 -1.1729109\n",
      "Iter: 11 Gen Loss: 348.60028 Recon Loss: 348.9416 Gen ADV Loss: 6.115243 Dis Loss: 0.07926844 |||| 2.498446 -5.93939 -1.9606589\n",
      "Iter: 12 Gen Loss: 345.67902 Recon Loss: 346.02203 Gen ADV Loss: 1.4454222 Dis Loss: 0.6714316 |||| 5.466176 -3.8993325 3.1046557\n",
      "Iter: 13 Gen Loss: 326.94806 Recon Loss: 327.2676 Gen ADV Loss: 6.1167145 Dis Loss: 0.33249584 |||| 0.4759895 -9.685342 -2.6495044\n",
      "Iter: 14 Gen Loss: 326.78766 Recon Loss: 327.11224 Gen ADV Loss: 0.8456528 Dis Loss: 0.8912286 |||| 4.032674 -1.5854071 7.9334564\n",
      "Iter: 15 Gen Loss: 318.27884 Recon Loss: 318.5908 Gen ADV Loss: 4.8452272 Dis Loss: 0.6276036 |||| -0.76639247 -7.803343 -4.0561566\n",
      "Iter: 16 Gen Loss: 341.1672 Recon Loss: 341.50586 Gen ADV Loss: 0.98290986 Dis Loss: 0.72593176 |||| 3.1818705 -2.142951 8.369771\n",
      "Iter: 17 Gen Loss: 335.10117 Recon Loss: 335.43207 Gen ADV Loss: 2.5959089 Dis Loss: 0.16416322 |||| 2.0179021 -5.2752113 -0.09239254\n",
      "Iter: 18 Gen Loss: 321.75766 Recon Loss: 322.07358 Gen ADV Loss: 4.159204 Dis Loss: 0.09465123 |||| 2.514476 -6.1021705 -1.0534266\n",
      "Iter: 19 Gen Loss: 302.37128 Recon Loss: 302.66663 Gen ADV Loss: 5.2299805 Dis Loss: 0.101110555 |||| 2.3786967 -5.014723 -1.0816202\n",
      "Iter: 20 Gen Loss: 314.7257 Recon Loss: 315.03314 Gen ADV Loss: 5.429485 Dis Loss: 0.038838975 |||| 3.0435662 -6.9806776 -2.0339046\n",
      "Iter: 21 Gen Loss: 317.96082 Recon Loss: 318.27188 Gen ADV Loss: 4.9432235 Dis Loss: 0.024029369 |||| 3.5906506 -9.035784 -2.144115\n",
      "Iter: 22 Gen Loss: 304.63248 Recon Loss: 304.9292 Gen ADV Loss: 5.8384852 Dis Loss: 0.034653805 |||| 4.4061337 -6.481005 -0.14783576\n",
      "Iter: 23 Gen Loss: 294.23398 Recon Loss: 294.51984 Gen ADV Loss: 6.1949506 Dis Loss: 0.027996862 |||| 4.056796 -7.9289265 -0.069793284\n",
      "Iter: 24 Gen Loss: 279.52005 Recon Loss: 279.79083 Gen ADV Loss: 6.455083 Dis Loss: 0.055956017 |||| 3.1284397 -6.5585885 -2.7119653\n",
      "Iter: 25 Gen Loss: 303.06903 Recon Loss: 303.36588 Gen ADV Loss: 3.9198987 Dis Loss: 0.0902034 |||| 4.6740174 -9.537251 1.7257991\n",
      "Iter: 26 Gen Loss: 280.61557 Recon Loss: 280.8883 Gen ADV Loss: 5.4270263 Dis Loss: 0.17529975 |||| 2.6498334 -10.142274 -1.5240552\n",
      "Iter: 27 Gen Loss: 294.09607 Recon Loss: 294.38504 Gen ADV Loss: 2.5706892 Dis Loss: 0.2869107 |||| 4.8630977 -3.641563 2.4750624\n",
      "Iter: 28 Gen Loss: 277.54883 Recon Loss: 277.81583 Gen ADV Loss: 7.87255 Dis Loss: 0.56879216 |||| -0.4282646 -10.739824 -4.8986387\n",
      "Iter: 29 Gen Loss: 282.9944 Recon Loss: 283.2744 Gen ADV Loss: 0.2264773 Dis Loss: 0.94730383 |||| 5.2484217 -0.31661737 5.527228\n",
      "Iter: 30 Gen Loss: 286.33722 Recon Loss: 286.61496 Gen ADV Loss: 5.7512856 Dis Loss: 0.1786691 |||| 1.7355102 -6.556951 0.1460706\n",
      "Iter: 31 Gen Loss: 271.39087 Recon Loss: 271.6553 Gen ADV Loss: 3.97966 Dis Loss: 0.25646463 |||| 0.8669848 -10.433595 -2.1001916\n",
      "Iter: 32 Gen Loss: 283.87045 Recon Loss: 284.14713 Gen ADV Loss: 4.1620336 Dis Loss: 0.4043125 |||| 2.9556134 -2.5824857 2.5459957\n",
      "Iter: 33 Gen Loss: 269.5226 Recon Loss: 269.78503 Gen ADV Loss: 3.9851062 Dis Loss: 0.15713009 |||| 1.612425 -7.9848576 -1.5034395\n",
      "Iter: 34 Gen Loss: 263.19028 Recon Loss: 263.44553 Gen ADV Loss: 4.751134 Dis Loss: 0.052706197 |||| 2.7984252 -7.373797 -2.0896633\n",
      "Iter: 35 Gen Loss: 266.35922 Recon Loss: 266.61804 Gen ADV Loss: 4.26426 Dis Loss: 0.09614007 |||| 2.9584513 -4.654219 0.7138505\n",
      "Iter: 36 Gen Loss: 271.0493 Recon Loss: 271.31207 Gen ADV Loss: 4.9238796 Dis Loss: 0.059237715 |||| 2.9652631 -7.864929 -3.1297023\n",
      "Iter: 37 Gen Loss: 255.26904 Recon Loss: 255.51553 Gen ADV Loss: 5.3259864 Dis Loss: 0.033281427 |||| 3.7072482 -7.900087 -2.2833443\n",
      "Iter: 38 Gen Loss: 263.09067 Recon Loss: 263.3452 Gen ADV Loss: 4.9938464 Dis Loss: 0.0408147 |||| 4.446019 -6.2332006 -1.0243973\n",
      "Iter: 39 Gen Loss: 287.9207 Recon Loss: 288.2002 Gen ADV Loss: 4.8491807 Dis Loss: 0.011612606 |||| 4.4657283 -9.542994 -3.1989734\n",
      "Iter: 40 Gen Loss: 246.52365 Recon Loss: 246.75987 Gen ADV Loss: 6.5988665 Dis Loss: 0.010869537 |||| 4.622084 -9.510402 -3.2367835\n",
      "Iter: 41 Gen Loss: 268.18607 Recon Loss: 268.44363 Gen ADV Loss: 6.83478 Dis Loss: 0.014020074 |||| 4.8464265 -7.0888906 -2.4584174\n",
      "Iter: 42 Gen Loss: 254.60307 Recon Loss: 254.84744 Gen ADV Loss: 6.3678484 Dis Loss: 0.00925144 |||| 4.898318 -8.03699 -2.7570956\n",
      "Iter: 43 Gen Loss: 250.90585 Recon Loss: 251.14612 Gen ADV Loss: 6.692396 Dis Loss: 0.0063972967 |||| 4.9928474 -10.61218 -5.1974072\n",
      "Iter: 44 Gen Loss: 255.57707 Recon Loss: 255.82094 Gen ADV Loss: 7.6833687 Dis Loss: 0.007839655 |||| 5.021164 -9.448902 -3.6667068\n",
      "Iter: 45 Gen Loss: 251.98116 Recon Loss: 252.22241 Gen ADV Loss: 6.63505 Dis Loss: 0.0038644942 |||| 5.648993 -9.564162 -4.1540213\n",
      "Iter: 46 Gen Loss: 243.17702 Recon Loss: 243.4093 Gen ADV Loss: 6.757574 Dis Loss: 0.0052980087 |||| 5.390923 -10.132643 -3.312144\n",
      "Iter: 47 Gen Loss: 249.4576 Recon Loss: 249.69565 Gen ADV Loss: 7.2072234 Dis Loss: 0.0068271738 |||| 5.300827 -10.008252 -3.2533834\n",
      "Iter: 48 Gen Loss: 245.9515 Recon Loss: 246.18639 Gen ADV Loss: 6.7990932 Dis Loss: 0.004986238 |||| 5.458271 -10.066632 -4.615286\n",
      "Iter: 49 Gen Loss: 253.88417 Recon Loss: 254.12654 Gen ADV Loss: 7.170624 Dis Loss: 0.006912854 |||| 5.9615545 -8.030885 -2.4534056\n",
      "Iter: 50 Gen Loss: 244.09393 Recon Loss: 244.32748 Gen ADV Loss: 6.1168504 Dis Loss: 0.0035352553 |||| 5.8943653 -10.153084 -4.0339866\n",
      "Iter: 51 Gen Loss: 256.56406 Recon Loss: 256.80875 Gen ADV Loss: 7.41219 Dis Loss: 0.0038258384 |||| 5.7566686 -10.918697 -5.0467725\n",
      "Iter: 52 Gen Loss: 233.91148 Recon Loss: 234.13258 Gen ADV Loss: 8.250547 Dis Loss: 0.0032106976 |||| 5.63584 -10.963383 -5.5030017\n",
      "Iter: 53 Gen Loss: 252.99951 Recon Loss: 253.24017 Gen ADV Loss: 7.7597656 Dis Loss: 0.004631023 |||| 6.0330825 -9.656025 -4.0097494\n",
      "Iter: 54 Gen Loss: 251.56468 Recon Loss: 251.80469 Gen ADV Loss: 6.945542 Dis Loss: 0.0037462662 |||| 5.838637 -10.056892 -4.212935\n",
      "Iter: 55 Gen Loss: 245.33786 Recon Loss: 245.57153 Gen ADV Loss: 7.005129 Dis Loss: 0.006076602 |||| 5.9241138 -8.237188 -3.3098795\n",
      "Iter: 56 Gen Loss: 232.27844 Recon Loss: 232.49965 Gen ADV Loss: 6.357816 Dis Loss: 0.0034458856 |||| 5.738255 -10.102284 -5.034871\n",
      "Iter: 57 Gen Loss: 241.95854 Recon Loss: 242.18796 Gen ADV Loss: 7.7573085 Dis Loss: 0.0031695005 |||| 6.355949 -9.117605 -4.2777505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 58 Gen Loss: 231.3376 Recon Loss: 231.55717 Gen ADV Loss: 6.8885193 Dis Loss: 0.0028698952 |||| 6.088009 -10.145315 -4.7126365\n",
      "Iter: 59 Gen Loss: 237.15027 Recon Loss: 237.37483 Gen ADV Loss: 7.6493263 Dis Loss: 0.0020329456 |||| 6.4268866 -10.3845415 -4.920836\n",
      "Iter: 60 Gen Loss: 240.80635 Recon Loss: 241.03479 Gen ADV Loss: 7.3560305 Dis Loss: 0.0023718965 |||| 6.4293103 -9.210043 -4.714858\n",
      "Iter: 61 Gen Loss: 239.50038 Recon Loss: 239.72723 Gen ADV Loss: 7.5782056 Dis Loss: 0.002927513 |||| 6.8648524 -8.275056 -3.9380078\n",
      "Iter: 62 Gen Loss: 236.87624 Recon Loss: 237.1016 Gen ADV Loss: 6.4088287 Dis Loss: 0.0021456766 |||| 6.3557835 -10.178634 -4.639629\n",
      "Iter: 63 Gen Loss: 232.23035 Recon Loss: 232.44972 Gen ADV Loss: 7.690402 Dis Loss: 0.0030913712 |||| 6.1828504 -11.164337 -4.610378\n",
      "Iter: 64 Gen Loss: 243.0254 Recon Loss: 243.2547 Gen ADV Loss: 8.503928 Dis Loss: 0.003801695 |||| 6.457927 -7.021912 -3.867027\n",
      "Iter: 65 Gen Loss: 235.09296 Recon Loss: 235.31677 Gen ADV Loss: 6.0306044 Dis Loss: 0.007682692 |||| 6.791887 -8.507768 -1.7237222\n",
      "Iter: 66 Gen Loss: 239.7051 Recon Loss: 239.9336 Gen ADV Loss: 5.8744287 Dis Loss: 0.00307042 |||| 6.107042 -10.973397 -2.9970784\n",
      "Iter: 67 Gen Loss: 241.39336 Recon Loss: 241.62094 Gen ADV Loss: 8.439244 Dis Loss: 0.0020557093 |||| 6.2319493 -12.47683 -4.726876\n",
      "Iter: 68 Gen Loss: 232.4703 Recon Loss: 232.68867 Gen ADV Loss: 8.648181 Dis Loss: 0.0035622043 |||| 6.274019 -11.1563425 -3.0184734\n",
      "Iter: 69 Gen Loss: 238.04933 Recon Loss: 238.27469 Gen ADV Loss: 7.2088356 Dis Loss: 0.0025972242 |||| 5.875917 -10.763402 -6.401493\n",
      "Iter: 70 Gen Loss: 230.92552 Recon Loss: 231.14243 Gen ADV Loss: 8.456836 Dis Loss: 0.002102683 |||| 6.390276 -10.362855 -5.573267\n",
      "Iter: 71 Gen Loss: 217.39996 Recon Loss: 217.60394 Gen ADV Loss: 7.8159227 Dis Loss: 0.001989057 |||| 6.3166466 -10.890885 -5.235773\n",
      "Iter: 72 Gen Loss: 233.26405 Recon Loss: 233.48383 Gen ADV Loss: 7.8625355 Dis Loss: 0.0024022036 |||| 6.13217 -10.068304 -4.90773\n",
      "Iter: 73 Gen Loss: 232.17377 Recon Loss: 232.39214 Gen ADV Loss: 8.101757 Dis Loss: 0.0016972305 |||| 6.5892677 -9.685081 -4.4946375\n",
      "Iter: 74 Gen Loss: 225.2721 Recon Loss: 225.48398 Gen ADV Loss: 7.6470704 Dis Loss: 0.0029637748 |||| 7.1480274 -8.494331 -3.475069\n",
      "Iter: 75 Gen Loss: 238.03719 Recon Loss: 238.26207 Gen ADV Loss: 7.404642 Dis Loss: 0.0016602224 |||| 6.511488 -11.620631 -6.4486365\n",
      "Iter: 76 Gen Loss: 220.07643 Recon Loss: 220.28189 Gen ADV Loss: 8.784701 Dis Loss: 0.001644921 |||| 6.490477 -11.516517 -6.95715\n",
      "Iter: 77 Gen Loss: 222.6719 Recon Loss: 222.87978 Gen ADV Loss: 8.916776 Dis Loss: 0.0015778363 |||| 6.47901 -10.668355 -5.670244\n",
      "Iter: 78 Gen Loss: 228.94408 Recon Loss: 229.15865 Gen ADV Loss: 8.454956 Dis Loss: 0.0019074479 |||| 6.319151 -11.070814 -5.091407\n",
      "Iter: 79 Gen Loss: 226.63528 Recon Loss: 226.8476 Gen ADV Loss: 8.330916 Dis Loss: 0.0013537261 |||| 6.8623133 -10.153721 -5.0488415\n",
      "Iter: 80 Gen Loss: 233.561 Recon Loss: 233.78064 Gen ADV Loss: 7.910682 Dis Loss: 0.0011610507 |||| 6.8466377 -10.484858 -6.7479763\n",
      "Iter: 81 Gen Loss: 213.97914 Recon Loss: 214.17854 Gen ADV Loss: 8.504982 Dis Loss: 0.00109342 |||| 6.9019694 -10.705746 -6.841149\n",
      "Iter: 82 Gen Loss: 234.18077 Recon Loss: 234.40019 Gen ADV Loss: 8.631743 Dis Loss: 0.0010206308 |||| 7.366219 -9.941388 -6.0124884\n",
      "Iter: 83 Gen Loss: 223.12296 Recon Loss: 223.33183 Gen ADV Loss: 8.093544 Dis Loss: 0.00093201274 |||| 7.3297443 -9.365752 -5.4819336\n",
      "Iter: 84 Gen Loss: 224.2219 Recon Loss: 224.43225 Gen ADV Loss: 7.665199 Dis Loss: 0.0011578371 |||| 6.8079753 -11.09547 -6.4546566\n",
      "Iter: 85 Gen Loss: 231.43695 Recon Loss: 231.65321 Gen ADV Loss: 8.940464 Dis Loss: 0.0007582894 |||| 7.3338857 -11.365568 -6.921654\n",
      "Iter: 86 Gen Loss: 230.98979 Recon Loss: 231.20587 Gen ADV Loss: 8.609009 Dis Loss: 0.002501233 |||| 6.7260075 -9.170314 -3.2123928\n",
      "Iter: 87 Gen Loss: 220.21698 Recon Loss: 220.42328 Gen ADV Loss: 7.5213428 Dis Loss: 0.0012371335 |||| 7.0361543 -10.761762 -4.732159\n",
      "Iter: 88 Gen Loss: 221.63602 Recon Loss: 221.84291 Gen ADV Loss: 8.328072 Dis Loss: 0.0013914495 |||| 6.63132 -12.007967 -6.1665487\n",
      "Iter: 89 Gen Loss: 233.38503 Recon Loss: 233.60274 Gen ADV Loss: 9.20297 Dis Loss: 0.0012880735 |||| 7.143915 -10.346841 -5.7273865\n",
      "Iter: 90 Gen Loss: 213.53831 Recon Loss: 213.73782 Gen ADV Loss: 7.4997888 Dis Loss: 0.0011180711 |||| 7.1257634 -9.949165 -4.7644935\n",
      "Iter: 91 Gen Loss: 220.79678 Recon Loss: 221.00311 Gen ADV Loss: 7.9057484 Dis Loss: 0.0016830136 |||| 6.643956 -11.076415 -6.8135524\n",
      "Iter: 92 Gen Loss: 213.02843 Recon Loss: 213.226 Gen ADV Loss: 8.850115 Dis Loss: 0.00084836636 |||| 7.1233287 -11.159648 -6.6739645\n",
      "Iter: 93 Gen Loss: 227.0457 Recon Loss: 227.25703 Gen ADV Loss: 9.0790415 Dis Loss: 0.00080035476 |||| 7.458638 -11.208499 -6.476845\n",
      "Iter: 94 Gen Loss: 229.4097 Recon Loss: 229.62392 Gen ADV Loss: 8.520417 Dis Loss: 0.0008681298 |||| 7.271046 -10.63832 -6.295364\n",
      "Iter: 95 Gen Loss: 217.2831 Recon Loss: 217.48541 Gen ADV Loss: 8.221445 Dis Loss: 0.001025781 |||| 7.476805 -9.13204 -5.0934486\n",
      "Iter: 96 Gen Loss: 215.36807 Recon Loss: 215.56918 Gen ADV Loss: 7.449646 Dis Loss: 0.00090894767 |||| 7.1218176 -10.756014 -6.3119903\n",
      "Iter: 97 Gen Loss: 227.76021 Recon Loss: 227.97247 Gen ADV Loss: 8.654866 Dis Loss: 0.0010812554 |||| 6.85905 -12.153172 -6.5641217\n",
      "Iter: 98 Gen Loss: 212.37413 Recon Loss: 212.56998 Gen ADV Loss: 9.608568 Dis Loss: 0.000811194 |||| 7.414975 -11.087856 -6.718431\n",
      "Iter: 99 Gen Loss: 215.00519 Recon Loss: 215.20442 Gen ADV Loss: 8.806406 Dis Loss: 0.0008648479 |||| 7.3438444 -10.391329 -6.1751018\n",
      "========================================================================\n",
      "8.658064 -0.8718368\n",
      "5.7209005 -0.6112389\n",
      "7.189737 -0.56767225\n",
      "6.3386383 -0.50138015\n",
      "5.9488416 -0.5290205\n",
      "5.1201425 -0.5279886\n",
      "8.433533 0.0\n",
      "7.8690453 0.0\n",
      "12.965915 0.0\n",
      "17.162523 0.0\n",
      "1.3459349 -2.2437022\n",
      "0.8730904 -0.97775054\n",
      "210.09764 0.0009399713\n",
      "=========================================================================\n",
      "Iter: 100 Gen Loss: 216.7929 Recon Loss: 216.9946 Gen ADV Loss: 8.079989 Dis Loss: 0.0023736886 |||| 7.7858067 -8.109149 -3.8721175\n",
      "Iter: 101 Gen Loss: 210.70497 Recon Loss: 210.90167 Gen ADV Loss: 6.929373 Dis Loss: 0.0007007448 |||| 7.426944 -11.111691 -6.213483\n",
      "Iter: 102 Gen Loss: 218.36644 Recon Loss: 218.56927 Gen ADV Loss: 8.418925 Dis Loss: 0.0008246696 |||| 7.1914515 -11.382598 -5.3968296\n",
      "Iter: 103 Gen Loss: 225.18916 Recon Loss: 225.39825 Gen ADV Loss: 8.95268 Dis Loss: 0.00075602287 |||| 7.655901 -10.505449 -5.944501\n",
      "Iter: 104 Gen Loss: 209.77034 Recon Loss: 209.96486 Gen ADV Loss: 8.064206 Dis Loss: 0.0008377095 |||| 7.494676 -9.937477 -5.740099\n",
      "Iter: 105 Gen Loss: 216.00856 Recon Loss: 216.20923 Gen ADV Loss: 8.081937 Dis Loss: 0.0011342893 |||| 7.1321754 -10.422465 -6.313347\n",
      "Iter: 106 Gen Loss: 218.02303 Recon Loss: 218.22566 Gen ADV Loss: 8.071292 Dis Loss: 0.0014016213 |||| 7.063717 -11.185703 -6.2141314\n",
      "Iter: 107 Gen Loss: 204.81807 Recon Loss: 205.00668 Gen ADV Loss: 8.840276 Dis Loss: 0.00061259104 |||| 7.5665684 -11.433698 -6.7996135\n",
      "Iter: 108 Gen Loss: 222.4902 Recon Loss: 222.69646 Gen ADV Loss: 8.849269 Dis Loss: 0.0010350313 |||| 7.2491975 -9.695244 -5.4935784\n",
      "Iter: 109 Gen Loss: 207.49414 Recon Loss: 207.68646 Gen ADV Loss: 7.6997447 Dis Loss: 0.0008247029 |||| 7.646073 -9.725624 -4.989334\n",
      "Iter: 110 Gen Loss: 213.40938 Recon Loss: 213.60716 Gen ADV Loss: 8.114674 Dis Loss: 0.0008679342 |||| 7.4820123 -10.6991205 -5.738695\n",
      "Iter: 111 Gen Loss: 210.94708 Recon Loss: 211.14198 Gen ADV Loss: 8.478367 Dis Loss: 0.00084684073 |||| 7.427739 -10.996199 -6.170008\n",
      "Iter: 112 Gen Loss: 228.61862 Recon Loss: 228.8308 Gen ADV Loss: 8.816528 Dis Loss: 0.0004926369 |||| 7.683462 -12.0469265 -7.05247\n",
      "Iter: 113 Gen Loss: 210.51712 Recon Loss: 210.71075 Gen ADV Loss: 9.209502 Dis Loss: 0.00064807874 |||| 7.604852 -11.0271635 -6.234121\n",
      "Iter: 114 Gen Loss: 208.93134 Recon Loss: 209.12413 Gen ADV Loss: 8.441565 Dis Loss: 0.0007466889 |||| 7.4805326 -10.192148 -5.6001663\n",
      "Iter: 115 Gen Loss: 208.43045 Recon Loss: 208.6229 Gen ADV Loss: 8.214446 Dis Loss: 0.0016276587 |||| 7.674999 -8.890745 -3.8155074\n",
      "Iter: 116 Gen Loss: 221.45052 Recon Loss: 221.65689 Gen ADV Loss: 7.2839966 Dis Loss: 0.0007168228 |||| 7.441175 -11.495906 -6.791203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 117 Gen Loss: 205.87094 Recon Loss: 206.05988 Gen ADV Loss: 9.114497 Dis Loss: 0.0005890623 |||| 7.464624 -11.842564 -7.080006\n",
      "Iter: 118 Gen Loss: 218.68967 Recon Loss: 218.89088 Gen ADV Loss: 9.591427 Dis Loss: 0.0006527532 |||| 7.4559984 -11.674257 -7.0352616\n",
      "Iter: 119 Gen Loss: 216.65614 Recon Loss: 216.85555 Gen ADV Loss: 9.332302 Dis Loss: 0.0004897434 |||| 7.9609284 -11.014358 -5.7336264\n",
      "Iter: 120 Gen Loss: 214.40607 Recon Loss: 214.60394 Gen ADV Loss: 8.558464 Dis Loss: 0.0010558887 |||| 7.876215 -10.536978 -4.4567747\n",
      "Iter: 121 Gen Loss: 214.2661 Recon Loss: 214.46521 Gen ADV Loss: 7.1514525 Dis Loss: 0.00072229316 |||| 8.139944 -9.262059 -4.4953103\n",
      "Iter: 122 Gen Loss: 215.53552 Recon Loss: 215.73509 Gen ADV Loss: 7.9217467 Dis Loss: 0.00046779957 |||| 8.065851 -11.218848 -5.845026\n",
      "Iter: 123 Gen Loss: 208.5002 Recon Loss: 208.69171 Gen ADV Loss: 8.911175 Dis Loss: 0.0006399682 |||| 7.5455766 -12.122566 -7.111809\n",
      "Iter: 124 Gen Loss: 209.4657 Recon Loss: 209.6577 Gen ADV Loss: 9.358382 Dis Loss: 0.00081561954 |||| 7.2599125 -11.818744 -6.3722334\n",
      "Iter: 125 Gen Loss: 221.38673 Recon Loss: 221.59041 Gen ADV Loss: 9.533436 Dis Loss: 0.0012438827 |||| 7.819471 -9.268351 -3.9942799\n",
      "Iter: 126 Gen Loss: 239.46893 Recon Loss: 239.69269 Gen ADV Loss: 7.5086184 Dis Loss: 0.0014162045 |||| 6.831689 -11.987204 -6.5756297\n",
      "Iter: 127 Gen Loss: 212.41045 Recon Loss: 212.60513 Gen ADV Loss: 9.447978 Dis Loss: 0.0009040815 |||| 7.504813 -12.0489025 -6.770439\n",
      "Iter: 128 Gen Loss: 239.99419 Recon Loss: 240.21622 Gen ADV Loss: 9.668008 Dis Loss: 0.00048238685 |||| 8.00074 -12.629648 -6.791506\n",
      "Iter: 129 Gen Loss: 207.5971 Recon Loss: 207.78726 Gen ADV Loss: 9.08643 Dis Loss: 0.0005953085 |||| 7.8766065 -9.972688 -5.898814\n",
      "Iter: 130 Gen Loss: 222.53746 Recon Loss: 222.74307 Gen ADV Loss: 8.511994 Dis Loss: 0.0005900033 |||| 7.635757 -11.584585 -7.455669\n",
      "Iter: 131 Gen Loss: 211.80196 Recon Loss: 211.99586 Gen ADV Loss: 9.437594 Dis Loss: 0.00047930665 |||| 7.7290263 -12.429013 -6.3007727\n",
      "Iter: 132 Gen Loss: 226.62839 Recon Loss: 226.83655 Gen ADV Loss: 9.969199 Dis Loss: 0.00038452068 |||| 8.119153 -11.833751 -7.027632\n",
      "Iter: 133 Gen Loss: 209.49394 Recon Loss: 209.68588 Gen ADV Loss: 9.000633 Dis Loss: 0.00083419844 |||| 7.5177264 -12.483155 -6.648102\n",
      "Iter: 134 Gen Loss: 209.28009 Recon Loss: 209.47151 Gen ADV Loss: 9.257936 Dis Loss: 0.0005496917 |||| 7.6380687 -12.195686 -7.4420214\n",
      "Iter: 135 Gen Loss: 213.00598 Recon Loss: 213.2008 Gen ADV Loss: 9.523007 Dis Loss: 0.0005673552 |||| 7.952572 -10.765974 -6.4827113\n",
      "Iter: 136 Gen Loss: 212.60869 Recon Loss: 212.80362 Gen ADV Loss: 8.949421 Dis Loss: 0.00061854627 |||| 7.6935163 -11.620656 -6.609186\n",
      "Iter: 137 Gen Loss: 217.38843 Recon Loss: 217.58829 Gen ADV Loss: 8.778155 Dis Loss: 0.0005463143 |||| 7.869776 -10.8469 -6.094239\n",
      "Iter: 138 Gen Loss: 212.74023 Recon Loss: 212.93556 Gen ADV Loss: 8.58585 Dis Loss: 0.0006544633 |||| 7.87044 -10.552022 -5.8223248\n",
      "Iter: 139 Gen Loss: 197.86316 Recon Loss: 198.0436 Gen ADV Loss: 8.558283 Dis Loss: 0.00044230855 |||| 8.025878 -11.729388 -6.566505\n",
      "Iter: 140 Gen Loss: 197.12888 Recon Loss: 197.30804 Gen ADV Loss: 9.049502 Dis Loss: 0.00036977892 |||| 8.215037 -11.397588 -6.9572678\n",
      "Iter: 141 Gen Loss: 212.86896 Recon Loss: 213.06387 Gen ADV Loss: 8.996396 Dis Loss: 0.00038859708 |||| 8.2410555 -10.259258 -6.654598\n",
      "Iter: 142 Gen Loss: 218.65291 Recon Loss: 218.8536 Gen ADV Loss: 8.986992 Dis Loss: 0.0007091233 |||| 8.365512 -11.781802 -4.7979403\n",
      "Iter: 143 Gen Loss: 203.54744 Recon Loss: 203.7335 Gen ADV Loss: 8.467429 Dis Loss: 0.0005439214 |||| 8.159462 -11.721654 -6.2097116\n",
      "Iter: 144 Gen Loss: 208.49902 Recon Loss: 208.68979 Gen ADV Loss: 8.674709 Dis Loss: 0.0004954785 |||| 8.213178 -10.546169 -6.0589895\n",
      "Iter: 145 Gen Loss: 214.36969 Recon Loss: 214.5663 Gen ADV Loss: 8.673764 Dis Loss: 0.00045956863 |||| 8.315238 -10.897522 -6.572441\n",
      "Iter: 146 Gen Loss: 208.22528 Recon Loss: 208.41608 Gen ADV Loss: 8.283438 Dis Loss: 0.00066637783 |||| 8.255689 -10.026711 -5.259599\n",
      "Iter: 147 Gen Loss: 219.29659 Recon Loss: 219.49886 Gen ADV Loss: 7.8480535 Dis Loss: 0.00049489294 |||| 7.7689643 -11.577455 -6.635585\n",
      "Iter: 148 Gen Loss: 224.247 Recon Loss: 224.45253 Gen ADV Loss: 9.51247 Dis Loss: 0.00033397466 |||| 8.440542 -11.299745 -6.952345\n",
      "Iter: 149 Gen Loss: 204.36795 Recon Loss: 204.5538 Gen ADV Loss: 9.228193 Dis Loss: 0.0005460474 |||| 7.7394905 -11.704859 -6.394611\n",
      "Iter: 150 Gen Loss: 197.53006 Recon Loss: 197.70909 Gen ADV Loss: 9.169655 Dis Loss: 0.0005476018 |||| 7.6980047 -12.034616 -6.9096007\n",
      "Iter: 151 Gen Loss: 208.4128 Recon Loss: 208.60252 Gen ADV Loss: 9.320747 Dis Loss: 0.00042500836 |||| 8.119683 -12.504611 -5.766531\n",
      "Iter: 152 Gen Loss: 219.39003 Recon Loss: 219.59068 Gen ADV Loss: 9.328126 Dis Loss: 0.00046949432 |||| 7.91842 -11.878401 -7.562966\n",
      "Iter: 153 Gen Loss: 201.3142 Recon Loss: 201.49615 Gen ADV Loss: 9.874727 Dis Loss: 0.00045817287 |||| 8.399345 -10.925197 -6.8448277\n",
      "Iter: 154 Gen Loss: 216.8637 Recon Loss: 217.06195 Gen ADV Loss: 9.1130295 Dis Loss: 0.0005611317 |||| 8.248659 -10.37938 -5.3550897\n",
      "Iter: 155 Gen Loss: 212.69539 Recon Loss: 212.89055 Gen ADV Loss: 8.013142 Dis Loss: 0.0005916721 |||| 8.152208 -11.456566 -6.841704\n",
      "Iter: 156 Gen Loss: 202.77312 Recon Loss: 202.95685 Gen ADV Loss: 9.461618 Dis Loss: 0.00040940597 |||| 8.387432 -11.402732 -6.2763577\n",
      "Iter: 157 Gen Loss: 200.2192 Recon Loss: 200.40085 Gen ADV Loss: 8.941723 Dis Loss: 0.00043724198 |||| 8.281456 -10.785219 -6.5903068\n",
      "Iter: 158 Gen Loss: 204.64854 Recon Loss: 204.83478 Gen ADV Loss: 8.754651 Dis Loss: 0.00042391598 |||| 8.030055 -11.363017 -5.972468\n",
      "Iter: 159 Gen Loss: 202.76605 Recon Loss: 202.94962 Gen ADV Loss: 9.482721 Dis Loss: 0.00038772984 |||| 8.477019 -10.748001 -5.748755\n",
      "Iter: 160 Gen Loss: 201.42712 Recon Loss: 201.61034 Gen ADV Loss: 8.484658 Dis Loss: 0.00053016125 |||| 8.177183 -11.259899 -5.3548207\n",
      "Iter: 161 Gen Loss: 204.17726 Recon Loss: 204.36298 Gen ADV Loss: 8.694691 Dis Loss: 0.0004590207 |||| 8.1407585 -11.348924 -6.6947837\n",
      "Iter: 162 Gen Loss: 203.0085 Recon Loss: 203.19289 Gen ADV Loss: 8.788409 Dis Loss: 0.0004793437 |||| 8.020428 -11.129568 -6.323452\n",
      "Iter: 163 Gen Loss: 203.97285 Recon Loss: 204.1581 Gen ADV Loss: 8.873358 Dis Loss: 0.0003792603 |||| 8.324272 -11.592351 -5.0441976\n",
      "Iter: 164 Gen Loss: 212.36055 Recon Loss: 212.55351 Gen ADV Loss: 9.534264 Dis Loss: 0.00033051154 |||| 8.307281 -12.310159 -6.801177\n",
      "Iter: 165 Gen Loss: 213.13895 Recon Loss: 213.33263 Gen ADV Loss: 9.541016 Dis Loss: 0.00039384636 |||| 8.286673 -11.537344 -6.301341\n",
      "Iter: 166 Gen Loss: 206.55438 Recon Loss: 206.74161 Gen ADV Loss: 9.333163 Dis Loss: 0.0005207913 |||| 8.777146 -10.640844 -4.874678\n",
      "Iter: 167 Gen Loss: 211.1869 Recon Loss: 211.38037 Gen ADV Loss: 7.728081 Dis Loss: 0.0005176135 |||| 8.344168 -9.925685 -5.7769556\n",
      "Iter: 168 Gen Loss: 199.40443 Recon Loss: 199.58522 Gen ADV Loss: 8.56106 Dis Loss: 0.00031969586 |||| 8.699659 -11.175293 -6.5359454\n",
      "Iter: 169 Gen Loss: 208.05353 Recon Loss: 208.24246 Gen ADV Loss: 9.025784 Dis Loss: 0.0005128565 |||| 7.989546 -11.8139 -6.7610483\n",
      "Iter: 170 Gen Loss: 201.7691 Recon Loss: 201.95132 Gen ADV Loss: 9.407075 Dis Loss: 0.00062503444 |||| 7.93148 -11.277226 -6.7609577\n",
      "Iter: 171 Gen Loss: 200.37 Recon Loss: 200.55107 Gen ADV Loss: 9.105716 Dis Loss: 0.00039739115 |||| 8.016048 -10.9583645 -7.173417\n",
      "Iter: 172 Gen Loss: 196.49875 Recon Loss: 196.67587 Gen ADV Loss: 9.137502 Dis Loss: 0.00040503193 |||| 8.648053 -10.9156065 -4.7718773\n",
      "Iter: 173 Gen Loss: 193.1008 Recon Loss: 193.275 Gen ADV Loss: 8.619921 Dis Loss: 0.00055421976 |||| 8.240752 -11.023271 -5.278927\n",
      "Iter: 174 Gen Loss: 218.58263 Recon Loss: 218.78275 Gen ADV Loss: 8.166029 Dis Loss: 0.0004943267 |||| 8.363789 -10.532956 -6.2071714\n",
      "Iter: 175 Gen Loss: 207.00128 Recon Loss: 207.18935 Gen ADV Loss: 8.607942 Dis Loss: 0.00028738374 |||| 8.70752 -11.975789 -6.9690833\n",
      "Iter: 176 Gen Loss: 194.90253 Recon Loss: 195.07825 Gen ADV Loss: 8.799959 Dis Loss: 0.00040799903 |||| 8.245738 -10.920426 -6.2255588\n",
      "Iter: 177 Gen Loss: 198.2171 Recon Loss: 198.39569 Gen ADV Loss: 9.214865 Dis Loss: 0.0002964711 |||| 8.478142 -11.484932 -6.5735564\n",
      "Iter: 178 Gen Loss: 204.03465 Recon Loss: 204.2185 Gen ADV Loss: 9.731699 Dis Loss: 0.00034467073 |||| 8.351055 -11.976612 -7.616357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 179 Gen Loss: 204.77086 Recon Loss: 204.9554 Gen ADV Loss: 9.760978 Dis Loss: 0.00029775003 |||| 8.62724 -12.325468 -7.390116\n",
      "Iter: 180 Gen Loss: 198.96033 Recon Loss: 199.13956 Gen ADV Loss: 9.227502 Dis Loss: 0.000449586 |||| 8.357331 -10.541562 -6.4331055\n",
      "Iter: 181 Gen Loss: 192.52048 Recon Loss: 192.69424 Gen ADV Loss: 8.174565 Dis Loss: 0.00048338718 |||| 8.368251 -10.988181 -6.763397\n",
      "Iter: 182 Gen Loss: 196.49504 Recon Loss: 196.67212 Gen ADV Loss: 8.812735 Dis Loss: 0.00045095955 |||| 8.658133 -10.115421 -5.9774785\n",
      "Iter: 183 Gen Loss: 222.9277 Recon Loss: 223.13148 Gen ADV Loss: 8.524627 Dis Loss: 0.0004620977 |||| 8.079342 -11.8745985 -7.2649617\n",
      "Iter: 184 Gen Loss: 195.474 Recon Loss: 195.64902 Gen ADV Loss: 9.772094 Dis Loss: 0.0003362908 |||| 8.262605 -12.452665 -7.01757\n",
      "Iter: 185 Gen Loss: 223.60587 Recon Loss: 223.80864 Gen ADV Loss: 10.112611 Dis Loss: 0.0003174986 |||| 8.826227 -11.472285 -6.7170305\n",
      "Iter: 186 Gen Loss: 203.34349 Recon Loss: 203.52724 Gen ADV Loss: 8.850703 Dis Loss: 0.00040921546 |||| 8.158852 -11.373687 -6.586174\n",
      "Iter: 187 Gen Loss: 200.51118 Recon Loss: 200.69226 Gen ADV Loss: 8.637023 Dis Loss: 0.00036345812 |||| 8.3679495 -11.680392 -6.7489967\n",
      "Iter: 188 Gen Loss: 222.91595 Recon Loss: 223.11864 Gen ADV Loss: 9.405194 Dis Loss: 0.00039924344 |||| 8.811746 -10.780193 -5.4517245\n",
      "Iter: 189 Gen Loss: 199.99802 Recon Loss: 200.17795 Gen ADV Loss: 9.17321 Dis Loss: 0.0003804534 |||| 8.4472275 -11.151488 -6.175193\n",
      "Iter: 190 Gen Loss: 212.65256 Recon Loss: 212.84547 Gen ADV Loss: 8.830078 Dis Loss: 0.00030907965 |||| 8.425265 -11.718932 -6.8585706\n",
      "Iter: 191 Gen Loss: 208.44675 Recon Loss: 208.63458 Gen ADV Loss: 9.679393 Dis Loss: 0.0003254203 |||| 8.693498 -11.157064 -6.582665\n",
      "Iter: 192 Gen Loss: 200.44308 Recon Loss: 200.62369 Gen ADV Loss: 8.836561 Dis Loss: 0.0005596542 |||| 8.209145 -11.957117 -6.419764\n",
      "Iter: 193 Gen Loss: 205.73636 Recon Loss: 205.92186 Gen ADV Loss: 9.1874485 Dis Loss: 0.00036119216 |||| 8.763205 -11.342327 -5.749131\n",
      "Iter: 194 Gen Loss: 212.65195 Recon Loss: 212.84476 Gen ADV Loss: 8.766989 Dis Loss: 0.0004071937 |||| 8.674724 -11.328006 -6.9974456\n",
      "Iter: 195 Gen Loss: 204.66884 Recon Loss: 204.8536 Gen ADV Loss: 8.759769 Dis Loss: 0.00037004985 |||| 8.350204 -11.59576 -5.3748856\n",
      "Iter: 196 Gen Loss: 205.74818 Recon Loss: 205.93333 Gen ADV Loss: 9.436769 Dis Loss: 0.00031985925 |||| 8.701951 -11.389436 -6.947514\n",
      "Iter: 197 Gen Loss: 195.54803 Recon Loss: 195.723 Gen ADV Loss: 9.336036 Dis Loss: 0.00026320384 |||| 8.499235 -12.863913 -8.028917\n",
      "Iter: 198 Gen Loss: 198.93118 Recon Loss: 199.10905 Gen ADV Loss: 9.815202 Dis Loss: 0.00028569862 |||| 8.676901 -12.756598 -6.8904443\n",
      "Iter: 199 Gen Loss: 211.64261 Recon Loss: 211.83347 Gen ADV Loss: 9.512995 Dis Loss: 0.00036668524 |||| 8.384522 -12.163827 -5.4308853\n",
      "========================================================================\n",
      "7.3855796 -0.8292224\n",
      "7.1577797 -0.6296971\n",
      "6.823538 -0.6224872\n",
      "8.065197 -0.62381357\n",
      "8.143613 -0.6018509\n",
      "6.4475822 -0.5446117\n",
      "10.277942 0.0\n",
      "8.94549 0.0\n",
      "17.667637 0.0\n",
      "31.076307 0.0\n",
      "1.7705412 -1.9970717\n",
      "0.9436687 -0.9638201\n",
      "191.7286 0.00040306253\n",
      "=========================================================================\n",
      "Iter: 200 Gen Loss: 199.8554 Recon Loss: 200.03403 Gen ADV Loss: 9.837055 Dis Loss: 0.00028558978 |||| 8.552898 -12.55596 -7.8288927\n",
      "Iter: 201 Gen Loss: 194.17096 Recon Loss: 194.34384 Gen ADV Loss: 9.880894 Dis Loss: 0.0003060635 |||| 8.789664 -12.558468 -5.853009\n",
      "Iter: 202 Gen Loss: 198.83842 Recon Loss: 199.01657 Gen ADV Loss: 9.283469 Dis Loss: 0.00036618975 |||| 8.343156 -11.206985 -6.463694\n",
      "Iter: 203 Gen Loss: 196.90923 Recon Loss: 197.08543 Gen ADV Loss: 9.221703 Dis Loss: 0.00045469415 |||| 8.392546 -10.476847 -6.459099\n",
      "Iter: 204 Gen Loss: 204.67056 Recon Loss: 204.8551 Gen ADV Loss: 8.601012 Dis Loss: 0.00029803338 |||| 8.878274 -10.817825 -6.9048796\n",
      "Iter: 205 Gen Loss: 204.63052 Recon Loss: 204.81488 Gen ADV Loss: 8.748539 Dis Loss: 0.00040732673 |||| 8.7231 -10.563434 -4.948432\n",
      "Iter: 206 Gen Loss: 197.71786 Recon Loss: 197.89561 Gen ADV Loss: 8.367406 Dis Loss: 0.00030074973 |||| 8.791321 -10.622125 -6.044203\n",
      "Iter: 207 Gen Loss: 194.27744 Recon Loss: 194.45111 Gen ADV Loss: 8.93787 Dis Loss: 0.000270451 |||| 8.808795 -11.362309 -6.872173\n",
      "Iter: 208 Gen Loss: 191.63359 Recon Loss: 191.80408 Gen ADV Loss: 9.454823 Dis Loss: 0.00027471478 |||| 8.546495 -11.605517 -7.3787155\n",
      "Iter: 209 Gen Loss: 195.53189 Recon Loss: 195.70609 Gen ADV Loss: 9.609091 Dis Loss: 0.00027695388 |||| 8.582895 -12.665036 -5.549194\n",
      "Iter: 210 Gen Loss: 194.08904 Recon Loss: 194.26123 Gen ADV Loss: 10.119965 Dis Loss: 0.00032530873 |||| 8.369306 -11.76077 -7.6070123\n",
      "Iter: 211 Gen Loss: 201.00822 Recon Loss: 201.18791 Gen ADV Loss: 9.495319 Dis Loss: 0.0003951842 |||| 8.46309 -11.458714 -6.400669\n",
      "Iter: 212 Gen Loss: 204.04704 Recon Loss: 204.23027 Gen ADV Loss: 8.934366 Dis Loss: 0.00054627954 |||| 9.217206 -10.341796 -4.7420907\n",
      "Iter: 213 Gen Loss: 201.27888 Recon Loss: 201.46008 Gen ADV Loss: 8.192783 Dis Loss: 0.0003259783 |||| 8.636077 -11.374481 -6.54853\n",
      "Iter: 214 Gen Loss: 202.25381 Recon Loss: 202.43497 Gen ADV Loss: 9.169649 Dis Loss: 0.00022549793 |||| 9.106122 -11.10293 -7.2461343\n",
      "Iter: 215 Gen Loss: 199.44467 Recon Loss: 199.62303 Gen ADV Loss: 9.147208 Dis Loss: 0.00029381132 |||| 8.612998 -12.184503 -5.977525\n",
      "Iter: 216 Gen Loss: 212.97046 Recon Loss: 213.16222 Gen ADV Loss: 9.257249 Dis Loss: 0.00064959703 |||| 8.106091 -10.889963 -6.5854945\n",
      "Iter: 217 Gen Loss: 204.16742 Recon Loss: 204.35033 Gen ADV Loss: 9.237558 Dis Loss: 0.00033532237 |||| 8.723999 -11.007691 -6.6982937\n",
      "Iter: 218 Gen Loss: 192.61359 Recon Loss: 192.78525 Gen ADV Loss: 8.847157 Dis Loss: 0.00031742183 |||| 8.73539 -11.182047 -6.264408\n",
      "Iter: 219 Gen Loss: 197.45258 Recon Loss: 197.62894 Gen ADV Loss: 8.957772 Dis Loss: 0.00033238635 |||| 8.38576 -13.279522 -7.554845\n",
      "Iter: 220 Gen Loss: 196.284 Recon Loss: 196.45836 Gen ADV Loss: 9.749639 Dis Loss: 0.0003591523 |||| 8.408633 -12.512713 -7.5281343\n",
      "Iter: 221 Gen Loss: 207.0876 Recon Loss: 207.27242 Gen ADV Loss: 10.049331 Dis Loss: 0.00026782276 |||| 8.511463 -12.365143 -8.072971\n",
      "Iter: 222 Gen Loss: 207.73718 Recon Loss: 207.92284 Gen ADV Loss: 9.813488 Dis Loss: 0.00034098554 |||| 8.766886 -10.813328 -5.4895873\n",
      "Iter: 223 Gen Loss: 209.79338 Recon Loss: 209.98184 Gen ADV Loss: 9.006303 Dis Loss: 0.00026093214 |||| 8.851961 -12.1771345 -6.700494\n",
      "Iter: 224 Gen Loss: 194.75499 Recon Loss: 194.92815 Gen ADV Loss: 9.257126 Dis Loss: 0.0002972186 |||| 8.600385 -11.915072 -5.71178\n",
      "Iter: 225 Gen Loss: 210.10403 Recon Loss: 210.2921 Gen ADV Loss: 9.650507 Dis Loss: 0.00022221354 |||| 8.886049 -12.142341 -7.731761\n",
      "Iter: 226 Gen Loss: 195.74734 Recon Loss: 195.92062 Gen ADV Loss: 10.012591 Dis Loss: 0.00026825396 |||| 8.589261 -12.123226 -7.3722205\n",
      "Iter: 227 Gen Loss: 202.2105 Recon Loss: 202.39017 Gen ADV Loss: 10.066475 Dis Loss: 0.00023334022 |||| 9.306102 -11.721647 -7.296189\n",
      "Iter: 228 Gen Loss: 203.48169 Recon Loss: 203.66388 Gen ADV Loss: 8.788676 Dis Loss: 0.00035824705 |||| 8.704555 -11.5853 -5.9235363\n",
      "Iter: 229 Gen Loss: 191.4623 Recon Loss: 191.63237 Gen ADV Loss: 8.813175 Dis Loss: 0.00024247996 |||| 9.00508 -11.639888 -6.959694\n",
      "Iter: 230 Gen Loss: 188.73755 Recon Loss: 188.90413 Gen ADV Loss: 9.554726 Dis Loss: 0.00032189238 |||| 8.7786875 -11.727818 -6.1861753\n",
      "Iter: 231 Gen Loss: 198.11629 Recon Loss: 198.29291 Gen ADV Loss: 8.844224 Dis Loss: 0.00040371536 |||| 8.531 -12.082637 -5.8759847\n",
      "Iter: 232 Gen Loss: 192.44662 Recon Loss: 192.61723 Gen ADV Loss: 9.122274 Dis Loss: 0.00038345053 |||| 8.552181 -11.170449 -6.690847\n",
      "Iter: 233 Gen Loss: 198.14891 Recon Loss: 198.32472 Gen ADV Loss: 9.581139 Dis Loss: 0.0002502974 |||| 8.956209 -11.819365 -6.8195415\n",
      "Iter: 234 Gen Loss: 191.4213 Recon Loss: 191.59079 Gen ADV Loss: 9.13142 Dis Loss: 0.00028859056 |||| 8.909249 -11.596666 -6.5650177\n",
      "Iter: 235 Gen Loss: 200.32613 Recon Loss: 200.50432 Gen ADV Loss: 9.3194065 Dis Loss: 0.000303819 |||| 8.608017 -11.691073 -6.7072988\n",
      "Iter: 236 Gen Loss: 210.82579 Recon Loss: 211.01433 Gen ADV Loss: 9.463404 Dis Loss: 0.00029515265 |||| 8.556692 -11.997625 -7.1497126\n",
      "Iter: 237 Gen Loss: 203.30461 Recon Loss: 203.48495 Gen ADV Loss: 10.059842 Dis Loss: 0.0002605831 |||| 8.664737 -12.497274 -6.7990675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 238 Gen Loss: 202.05396 Recon Loss: 202.23294 Gen ADV Loss: 10.097119 Dis Loss: 0.00031124154 |||| 9.050695 -12.026803 -6.5879426\n",
      "Iter: 239 Gen Loss: 210.11177 Recon Loss: 210.29987 Gen ADV Loss: 9.033227 Dis Loss: 0.00043339265 |||| 8.76172 -10.564774 -5.8380547\n",
      "Iter: 240 Gen Loss: 193.23882 Recon Loss: 193.41019 Gen ADV Loss: 8.8154335 Dis Loss: 0.00028570692 |||| 9.137256 -10.846731 -5.6453667\n",
      "Iter: 241 Gen Loss: 190.37859 Recon Loss: 190.54724 Gen ADV Loss: 8.64794 Dis Loss: 0.00028278714 |||| 8.941408 -11.922947 -6.517758\n",
      "Iter: 242 Gen Loss: 192.01932 Recon Loss: 192.18886 Gen ADV Loss: 9.341419 Dis Loss: 0.00032751993 |||| 8.718692 -12.286104 -6.8033476\n",
      "Iter: 243 Gen Loss: 193.42343 Recon Loss: 193.59427 Gen ADV Loss: 9.414975 Dis Loss: 0.00026637825 |||| 8.459139 -12.85082 -8.128083\n",
      "Iter: 244 Gen Loss: 183.94975 Recon Loss: 184.11029 Gen ADV Loss: 10.18269 Dis Loss: 0.00037757383 |||| 8.5318165 -12.477951 -7.308596\n",
      "Iter: 245 Gen Loss: 198.53053 Recon Loss: 198.70607 Gen ADV Loss: 9.731589 Dis Loss: 0.00029354307 |||| 9.2218275 -12.15806 -6.537194\n",
      "Iter: 246 Gen Loss: 192.96089 Recon Loss: 193.13182 Gen ADV Loss: 8.765646 Dis Loss: 0.00037901965 |||| 9.103738 -11.663614 -5.0041986\n",
      "Iter: 247 Gen Loss: 190.87514 Recon Loss: 191.04385 Gen ADV Loss: 8.834711 Dis Loss: 0.00026569242 |||| 8.824371 -12.720572 -7.282114\n",
      "Iter: 248 Gen Loss: 193.42982 Recon Loss: 193.60056 Gen ADV Loss: 9.340007 Dis Loss: 0.0002864589 |||| 8.562478 -12.75353 -6.5850286\n",
      "Iter: 249 Gen Loss: 196.4624 Recon Loss: 196.63513 Gen ADV Loss: 10.314528 Dis Loss: 0.00030982433 |||| 8.741789 -12.57996 -7.5949407\n",
      "Iter: 250 Gen Loss: 206.50163 Recon Loss: 206.68436 Gen ADV Loss: 10.325077 Dis Loss: 0.00029167987 |||| 8.565838 -15.137845 -7.267865\n",
      "Iter: 251 Gen Loss: 202.04642 Recon Loss: 202.2252 Gen ADV Loss: 9.790144 Dis Loss: 0.0003254476 |||| 8.390853 -12.943184 -6.6806545\n",
      "Iter: 252 Gen Loss: 186.49968 Recon Loss: 186.66263 Gen ADV Loss: 10.010562 Dis Loss: 0.00030108204 |||| 8.668158 -12.12866 -5.2823796\n",
      "Iter: 253 Gen Loss: 205.51645 Recon Loss: 205.69864 Gen ADV Loss: 9.749369 Dis Loss: 0.00024161488 |||| 9.419443 -11.804013 -6.3461957\n",
      "Iter: 254 Gen Loss: 184.90225 Recon Loss: 185.06448 Gen ADV Loss: 9.059159 Dis Loss: 0.00025960564 |||| 8.981253 -12.443466 -6.307175\n",
      "Iter: 255 Gen Loss: 193.77272 Recon Loss: 193.94354 Gen ADV Loss: 9.333268 Dis Loss: 0.00034118778 |||| 9.298786 -11.255256 -6.187584\n",
      "Iter: 256 Gen Loss: 201.01971 Recon Loss: 201.19824 Gen ADV Loss: 8.85145 Dis Loss: 0.00025147153 |||| 8.706953 -11.806184 -7.0012927\n",
      "Iter: 257 Gen Loss: 185.55226 Recon Loss: 185.71417 Gen ADV Loss: 9.927876 Dis Loss: 0.00023506183 |||| 8.992516 -12.8034115 -7.552439\n",
      "Iter: 258 Gen Loss: 210.24821 Recon Loss: 210.43446 Gen ADV Loss: 10.289448 Dis Loss: 0.00022972716 |||| 9.2610855 -13.133163 -6.153111\n",
      "Iter: 259 Gen Loss: 199.51068 Recon Loss: 199.68698 Gen ADV Loss: 9.487779 Dis Loss: 0.0003674338 |||| 8.895505 -11.103274 -5.3061347\n",
      "Iter: 260 Gen Loss: 199.72604 Recon Loss: 199.9031 Gen ADV Loss: 8.852334 Dis Loss: 0.00022132203 |||| 9.049483 -12.866516 -7.3084507\n",
      "Iter: 261 Gen Loss: 186.38002 Recon Loss: 186.54297 Gen ADV Loss: 9.570533 Dis Loss: 0.00022179761 |||| 9.030271 -12.884633 -7.054723\n",
      "Iter: 262 Gen Loss: 191.96036 Recon Loss: 192.12842 Gen ADV Loss: 9.985316 Dis Loss: 0.00024551435 |||| 8.929469 -12.877212 -6.823752\n",
      "Iter: 263 Gen Loss: 203.78496 Recon Loss: 203.965 Gen ADV Loss: 9.789297 Dis Loss: 0.00036430362 |||| 8.59686 -12.815082 -7.2071924\n",
      "Iter: 264 Gen Loss: 192.5417 Recon Loss: 192.71017 Gen ADV Loss: 10.064392 Dis Loss: 0.00023316697 |||| 9.171073 -12.218407 -6.3540087\n",
      "Iter: 265 Gen Loss: 194.54709 Recon Loss: 194.71826 Gen ADV Loss: 9.350042 Dis Loss: 0.00027746387 |||| 8.784981 -11.838205 -6.768428\n",
      "Iter: 266 Gen Loss: 191.26102 Recon Loss: 191.42877 Gen ADV Loss: 9.424616 Dis Loss: 0.00024877832 |||| 8.639508 -12.466827 -7.864385\n",
      "Iter: 267 Gen Loss: 191.79132 Recon Loss: 191.95905 Gen ADV Loss: 9.950996 Dis Loss: 0.00024382595 |||| 8.709864 -12.901244 -7.787802\n",
      "Iter: 268 Gen Loss: 206.81985 Recon Loss: 207.00192 Gen ADV Loss: 10.604858 Dis Loss: 0.00017359895 |||| 9.154633 -13.023959 -8.18459\n",
      "Iter: 269 Gen Loss: 198.9849 Recon Loss: 199.15952 Gen ADV Loss: 10.165003 Dis Loss: 0.00028405234 |||| 9.0879545 -10.929175 -6.10409\n",
      "Iter: 270 Gen Loss: 203.98985 Recon Loss: 204.17035 Gen ADV Loss: 9.236415 Dis Loss: 0.00028208588 |||| 8.589831 -11.476806 -6.9186606\n",
      "Iter: 271 Gen Loss: 193.01245 Recon Loss: 193.18152 Gen ADV Loss: 9.648446 Dis Loss: 0.00023890033 |||| 9.293915 -11.771835 -6.496687\n",
      "Iter: 272 Gen Loss: 192.61247 Recon Loss: 192.78145 Gen ADV Loss: 9.303786 Dis Loss: 0.00023353723 |||| 8.990597 -11.581181 -6.5255175\n",
      "Iter: 273 Gen Loss: 188.47424 Recon Loss: 188.63864 Gen ADV Loss: 9.70581 Dis Loss: 0.00016735223 |||| 9.330191 -12.246872 -8.450298\n",
      "Iter: 274 Gen Loss: 196.66885 Recon Loss: 196.84094 Gen ADV Loss: 10.188034 Dis Loss: 0.00018523149 |||| 9.063645 -12.750528 -7.4549804\n",
      "Iter: 275 Gen Loss: 191.07791 Recon Loss: 191.24422 Gen ADV Loss: 10.325165 Dis Loss: 0.000199815 |||| 9.276602 -13.30189 -6.7273273\n",
      "Iter: 276 Gen Loss: 185.10754 Recon Loss: 185.26788 Gen ADV Loss: 10.287621 Dis Loss: 0.0002494497 |||| 8.86893 -12.464444 -7.47633\n",
      "Iter: 277 Gen Loss: 192.32344 Recon Loss: 192.49219 Gen ADV Loss: 9.06081 Dis Loss: 0.00028806162 |||| 8.73125 -11.107499 -6.8374677\n",
      "Iter: 278 Gen Loss: 194.1143 Recon Loss: 194.28436 Gen ADV Loss: 9.472991 Dis Loss: 0.0002985954 |||| 9.257525 -12.056359 -6.2611594\n",
      "Iter: 279 Gen Loss: 200.36504 Recon Loss: 200.54169 Gen ADV Loss: 9.1213255 Dis Loss: 0.00038388808 |||| 8.996242 -12.2936945 -4.1243534\n",
      "Iter: 280 Gen Loss: 200.91032 Recon Loss: 201.08696 Gen ADV Loss: 9.631763 Dis Loss: 0.0002460751 |||| 8.677995 -13.467686 -8.038865\n",
      "Iter: 281 Gen Loss: 180.43098 Recon Loss: 180.58614 Gen ADV Loss: 10.592485 Dis Loss: 0.00024254934 |||| 8.748893 -14.010521 -6.569845\n",
      "Iter: 282 Gen Loss: 186.04987 Recon Loss: 186.21066 Gen ADV Loss: 10.523166 Dis Loss: 0.00026627615 |||| 9.024069 -12.261163 -5.2827196\n",
      "Iter: 283 Gen Loss: 186.77528 Recon Loss: 186.93755 Gen ADV Loss: 9.755995 Dis Loss: 0.0002605254 |||| 9.228497 -11.494405 -6.501609\n",
      "Iter: 284 Gen Loss: 193.59206 Recon Loss: 193.76166 Gen ADV Loss: 9.185856 Dis Loss: 0.00031110988 |||| 9.054574 -11.273632 -6.062885\n",
      "Iter: 285 Gen Loss: 191.26051 Recon Loss: 191.42807 Gen ADV Loss: 8.85538 Dis Loss: 0.00042503726 |||| 8.578619 -12.330268 -6.8981767\n",
      "Iter: 286 Gen Loss: 187.36212 Recon Loss: 187.52446 Gen ADV Loss: 10.096258 Dis Loss: 0.00023536925 |||| 8.905527 -13.591033 -6.7608213\n",
      "Iter: 287 Gen Loss: 192.67505 Recon Loss: 192.84293 Gen ADV Loss: 9.866568 Dis Loss: 0.0004447475 |||| 9.290914 -11.480872 -4.301688\n",
      "Iter: 288 Gen Loss: 183.60492 Recon Loss: 183.76488 Gen ADV Loss: 8.674461 Dis Loss: 0.00032467535 |||| 8.856381 -11.720132 -6.1232367\n",
      "Iter: 289 Gen Loss: 187.57231 Recon Loss: 187.7359 Gen ADV Loss: 8.953884 Dis Loss: 0.0002561957 |||| 9.282214 -12.252858 -6.2653146\n",
      "Iter: 290 Gen Loss: 187.51476 Recon Loss: 187.6781 Gen ADV Loss: 9.155473 Dis Loss: 0.0002195386 |||| 9.123099 -13.545584 -7.363452\n",
      "Iter: 291 Gen Loss: 186.2532 Recon Loss: 186.41467 Gen ADV Loss: 9.738182 Dis Loss: 0.00027295455 |||| 8.798739 -12.517713 -6.3098364\n",
      "Iter: 292 Gen Loss: 186.06442 Recon Loss: 186.22533 Gen ADV Loss: 10.033967 Dis Loss: 0.0003076035 |||| 8.493928 -13.093525 -6.3603086\n",
      "Iter: 293 Gen Loss: 195.35681 Recon Loss: 195.52704 Gen ADV Loss: 9.996402 Dis Loss: 0.00027224747 |||| 9.2034 -12.738515 -6.2580233\n",
      "Iter: 294 Gen Loss: 194.84462 Recon Loss: 195.01468 Gen ADV Loss: 9.621491 Dis Loss: 0.00021569368 |||| 9.159363 -12.783846 -6.8534675\n",
      "Iter: 295 Gen Loss: 191.49284 Recon Loss: 191.6592 Gen ADV Loss: 9.919015 Dis Loss: 0.00022440513 |||| 9.252531 -11.903395 -7.103809\n",
      "Iter: 296 Gen Loss: 199.39989 Recon Loss: 199.5746 Gen ADV Loss: 9.420141 Dis Loss: 0.00031914585 |||| 9.350654 -11.434868 -5.2340684\n",
      "Iter: 297 Gen Loss: 190.37573 Recon Loss: 190.54204 Gen ADV Loss: 8.770969 Dis Loss: 0.0002062266 |||| 9.149918 -12.62986 -7.262901\n",
      "Iter: 298 Gen Loss: 193.18771 Recon Loss: 193.35599 Gen ADV Loss: 9.608498 Dis Loss: 0.00021210208 |||| 9.106783 -12.426968 -7.374662\n",
      "Iter: 299 Gen Loss: 197.90886 Recon Loss: 198.08124 Gen ADV Loss: 10.181964 Dis Loss: 0.0001902737 |||| 9.300098 -12.017874 -7.515441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "9.784009 -1.059301\n",
      "6.2278194 -0.6167653\n",
      "6.748866 -0.6034673\n",
      "8.722768 -0.58051723\n",
      "7.4046717 -0.5749561\n",
      "6.524213 -0.5398342\n",
      "8.435158 0.0\n",
      "9.697851 0.0\n",
      "16.489521 0.0\n",
      "30.756477 0.0\n",
      "1.8171616 -2.280157\n",
      "0.9485547 -0.97929895\n",
      "187.46342 0.0002491117\n",
      "=========================================================================\n",
      "Iter: 300 Gen Loss: 192.4739 Recon Loss: 192.64133 Gen ADV Loss: 9.659074 Dis Loss: 0.00021299854 |||| 9.008666 -12.699586 -7.5616965\n",
      "Iter: 301 Gen Loss: 196.8534 Recon Loss: 197.02448 Gen ADV Loss: 10.335821 Dis Loss: 0.00018613387 |||| 9.0479555 -13.690834 -6.657926\n",
      "Iter: 302 Gen Loss: 200.28091 Recon Loss: 200.45506 Gen ADV Loss: 10.663112 Dis Loss: 0.00020618929 |||| 9.01076 -13.364225 -7.785128\n",
      "Iter: 303 Gen Loss: 193.42523 Recon Loss: 193.5928 Gen ADV Loss: 10.326822 Dis Loss: 0.00020388742 |||| 9.193125 -12.406329 -6.635972\n",
      "Iter: 304 Gen Loss: 190.95335 Recon Loss: 191.11873 Gen ADV Loss: 10.001477 Dis Loss: 0.00024331422 |||| 9.036164 -11.806317 -7.5488796\n",
      "Iter: 305 Gen Loss: 180.08167 Recon Loss: 180.23659 Gen ADV Loss: 9.538656 Dis Loss: 0.00020644153 |||| 9.096427 -13.102637 -7.466681\n",
      "Iter: 306 Gen Loss: 188.98076 Recon Loss: 189.14406 Gen ADV Loss: 10.054416 Dis Loss: 0.00020357588 |||| 9.045679 -12.904651 -6.6127157\n",
      "Iter: 307 Gen Loss: 192.64713 Recon Loss: 192.81357 Gen ADV Loss: 10.508572 Dis Loss: 0.00015399871 |||| 9.472273 -13.175976 -8.653003\n",
      "Iter: 308 Gen Loss: 183.45547 Recon Loss: 183.61275 Gen ADV Loss: 10.472069 Dis Loss: 0.00023722714 |||| 8.886575 -12.113359 -7.321361\n",
      "Iter: 309 Gen Loss: 187.84515 Recon Loss: 188.00745 Gen ADV Loss: 9.792974 Dis Loss: 0.0004979169 |||| 9.12388 -11.609198 -4.826625\n",
      "Iter: 310 Gen Loss: 189.3066 Recon Loss: 189.47139 Gen ADV Loss: 8.726706 Dis Loss: 0.00032278043 |||| 9.096879 -11.173929 -6.142134\n",
      "Iter: 311 Gen Loss: 187.33221 Recon Loss: 187.49477 Gen ADV Loss: 8.949187 Dis Loss: 0.0001827504 |||| 9.2643795 -12.559807 -6.9883065\n",
      "Iter: 312 Gen Loss: 189.28157 Recon Loss: 189.44476 Gen ADV Loss: 10.219693 Dis Loss: 0.00025370304 |||| 8.997222 -12.304567 -7.3662395\n",
      "Iter: 313 Gen Loss: 185.22148 Recon Loss: 185.38065 Gen ADV Loss: 10.149866 Dis Loss: 0.00033196242 |||| 8.484425 -12.694454 -7.092559\n",
      "Iter: 314 Gen Loss: 185.70337 Recon Loss: 185.86295 Gen ADV Loss: 10.18545 Dis Loss: 0.00026075935 |||| 9.067068 -12.134284 -6.7011847\n",
      "Iter: 315 Gen Loss: 180.3391 Recon Loss: 180.49403 Gen ADV Loss: 9.392823 Dis Loss: 0.00046194743 |||| 8.253638 -13.057632 -8.187831\n",
      "Iter: 316 Gen Loss: 200.87389 Recon Loss: 201.0484 Gen ADV Loss: 10.340174 Dis Loss: 0.00016920002 |||| 9.461499 -13.061178 -7.369736\n",
      "Iter: 317 Gen Loss: 187.4907 Recon Loss: 187.65173 Gen ADV Loss: 10.389939 Dis Loss: 0.00026518782 |||| 9.495228 -12.022946 -6.1951103\n",
      "Iter: 318 Gen Loss: 213.57886 Recon Loss: 213.76724 Gen ADV Loss: 9.056856 Dis Loss: 0.0002713844 |||| 8.636862 -13.11195 -7.1867514\n",
      "Iter: 319 Gen Loss: 185.71616 Recon Loss: 185.87567 Gen ADV Loss: 10.029083 Dis Loss: 0.00021925126 |||| 9.179698 -12.893378 -5.187594\n",
      "Iter: 320 Gen Loss: 179.62943 Recon Loss: 179.78235 Gen ADV Loss: 10.494822 Dis Loss: 0.0002998468 |||| 8.376004 -13.775086 -7.8345714\n",
      "Iter: 321 Gen Loss: 207.60194 Recon Loss: 207.78273 Gen ADV Loss: 10.591216 Dis Loss: 0.00021445105 |||| 9.217144 -13.185573 -6.539524\n",
      "Iter: 322 Gen Loss: 199.29932 Recon Loss: 199.47223 Gen ADV Loss: 10.114074 Dis Loss: 0.00021659494 |||| 9.148367 -11.931252 -6.556658\n",
      "Iter: 323 Gen Loss: 190.01059 Recon Loss: 190.17426 Gen ADV Loss: 10.013464 Dis Loss: 0.00017564977 |||| 9.506855 -13.349726 -6.997175\n",
      "Iter: 324 Gen Loss: 195.18169 Recon Loss: 195.3506 Gen ADV Loss: 9.913624 Dis Loss: 0.0002696198 |||| 8.475777 -14.206025 -7.946945\n",
      "Iter: 325 Gen Loss: 201.66943 Recon Loss: 201.84422 Gen ADV Loss: 10.491856 Dis Loss: 0.00017553821 |||| 9.111781 -13.17149 -7.7171426\n",
      "Iter: 326 Gen Loss: 204.2447 Recon Loss: 204.42194 Gen ADV Loss: 10.5800905 Dis Loss: 0.00015250081 |||| 9.721489 -13.092221 -7.780417\n",
      "Iter: 327 Gen Loss: 183.3197 Recon Loss: 183.47624 Gen ADV Loss: 10.282696 Dis Loss: 0.00021937332 |||| 9.1877775 -13.260724 -6.803131\n",
      "Iter: 328 Gen Loss: 185.035 Recon Loss: 185.19362 Gen ADV Loss: 9.914171 Dis Loss: 0.0002490898 |||| 9.3333435 -12.170365 -6.4043036\n",
      "Iter: 329 Gen Loss: 191.50601 Recon Loss: 191.672 Gen ADV Loss: 8.95009 Dis Loss: 0.00023700747 |||| 9.22107 -11.836345 -6.65149\n",
      "Iter: 330 Gen Loss: 181.64204 Recon Loss: 181.79718 Gen ADV Loss: 9.895282 Dis Loss: 0.00017332651 |||| 9.364117 -13.321504 -7.506958\n",
      "Iter: 331 Gen Loss: 194.851 Recon Loss: 195.01913 Gen ADV Loss: 10.107693 Dis Loss: 0.00023249855 |||| 8.963535 -12.721437 -6.639972\n",
      "Iter: 332 Gen Loss: 175.97615 Recon Loss: 176.12524 Gen ADV Loss: 10.191026 Dis Loss: 0.0002123325 |||| 9.174619 -12.362523 -6.0054693\n",
      "Iter: 333 Gen Loss: 191.60046 Recon Loss: 191.76529 Gen ADV Loss: 10.054661 Dis Loss: 0.000210772 |||| 9.447221 -13.383644 -6.36817\n",
      "Iter: 334 Gen Loss: 193.077 Recon Loss: 193.24329 Gen ADV Loss: 10.035722 Dis Loss: 0.00014086123 |||| 9.597048 -13.9718075 -8.537907\n",
      "Iter: 335 Gen Loss: 189.49808 Recon Loss: 189.66046 Gen ADV Loss: 10.346863 Dis Loss: 0.00015465515 |||| 9.594255 -12.7034025 -7.9265957\n",
      "Iter: 336 Gen Loss: 188.46819 Recon Loss: 188.62949 Gen ADV Loss: 10.341362 Dis Loss: 0.00019320849 |||| 9.0477 -13.335943 -7.255542\n",
      "Iter: 337 Gen Loss: 189.59044 Recon Loss: 189.75273 Gen ADV Loss: 10.413872 Dis Loss: 0.00025814315 |||| 9.734611 -11.020566 -5.5344367\n",
      "Iter: 338 Gen Loss: 185.51585 Recon Loss: 185.67528 Gen ADV Loss: 9.176996 Dis Loss: 0.00031860202 |||| 9.304479 -11.662437 -5.490581\n",
      "Iter: 339 Gen Loss: 187.75581 Recon Loss: 187.91728 Gen ADV Loss: 9.32718 Dis Loss: 0.00019160578 |||| 9.259788 -14.796221 -7.9500923\n",
      "Iter: 340 Gen Loss: 184.51764 Recon Loss: 184.67499 Gen ADV Loss: 10.18692 Dis Loss: 0.00029562262 |||| 9.125138 -13.194764 -7.7784805\n",
      "Iter: 341 Gen Loss: 198.12315 Recon Loss: 198.29369 Gen ADV Loss: 10.540097 Dis Loss: 0.00017072176 |||| 9.478861 -12.887708 -7.258949\n",
      "Iter: 342 Gen Loss: 188.4134 Recon Loss: 188.57404 Gen ADV Loss: 10.680802 Dis Loss: 0.00016980627 |||| 9.53788 -13.328715 -6.485631\n",
      "Iter: 343 Gen Loss: 193.51125 Recon Loss: 193.67688 Gen ADV Loss: 10.739634 Dis Loss: 0.00015052453 |||| 9.521721 -13.782283 -8.474452\n",
      "Iter: 344 Gen Loss: 185.74559 Recon Loss: 185.90369 Gen ADV Loss: 10.493421 Dis Loss: 0.00022603119 |||| 9.561051 -12.449337 -6.6387978\n",
      "Iter: 345 Gen Loss: 188.58263 Recon Loss: 188.74469 Gen ADV Loss: 9.3260145 Dis Loss: 0.00039067317 |||| 9.995965 -11.758774 -5.7156277\n",
      "Iter: 346 Gen Loss: 201.66623 Recon Loss: 201.84192 Gen ADV Loss: 8.735037 Dis Loss: 0.00024691698 |||| 8.92122 -13.465779 -7.58257\n",
      "Iter: 347 Gen Loss: 186.11526 Recon Loss: 186.27399 Gen ADV Loss: 10.092414 Dis Loss: 0.00020691064 |||| 9.265977 -13.591564 -7.2298293\n",
      "Iter: 348 Gen Loss: 182.13676 Recon Loss: 182.29108 Gen ADV Loss: 10.483618 Dis Loss: 0.00018539652 |||| 9.54874 -13.838893 -6.7114553\n",
      "Iter: 349 Gen Loss: 183.08405 Recon Loss: 183.23988 Gen ADV Loss: 9.872224 Dis Loss: 0.00041386642 |||| 9.376654 -12.929436 -4.0299206\n",
      "Iter: 350 Gen Loss: 189.86023 Recon Loss: 190.02357 Gen ADV Loss: 9.076234 Dis Loss: 0.00023498946 |||| 9.126091 -12.708086 -7.051494\n",
      "Iter: 351 Gen Loss: 178.55386 Recon Loss: 178.70523 Gen ADV Loss: 9.69721 Dis Loss: 0.00019183513 |||| 9.308705 -12.265787 -7.0388784\n",
      "Iter: 352 Gen Loss: 184.14334 Recon Loss: 184.29982 Gen ADV Loss: 10.140546 Dis Loss: 0.0002480985 |||| 8.758095 -12.988545 -8.590683\n",
      "Iter: 353 Gen Loss: 181.49707 Recon Loss: 181.65039 Gen ADV Loss: 10.621583 Dis Loss: 0.00020824862 |||| 9.063286 -12.957711 -7.822803\n",
      "Iter: 354 Gen Loss: 183.98756 Recon Loss: 184.14334 Gen ADV Loss: 10.617997 Dis Loss: 0.0003615048 |||| 8.47458 -13.757901 -7.9643135\n",
      "Iter: 355 Gen Loss: 188.06523 Recon Loss: 188.22505 Gen ADV Loss: 10.586182 Dis Loss: 0.00018321324 |||| 9.347546 -13.5088005 -6.276142\n",
      "Iter: 356 Gen Loss: 187.41713 Recon Loss: 187.57648 Gen ADV Loss: 10.394636 Dis Loss: 0.00031905068 |||| 9.628587 -13.012188 -4.549987\n",
      "Iter: 357 Gen Loss: 201.03203 Recon Loss: 201.20674 Gen ADV Loss: 8.611285 Dis Loss: 0.0002211125 |||| 9.197747 -12.2441435 -6.969808\n",
      "Iter: 358 Gen Loss: 186.42003 Recon Loss: 186.57874 Gen ADV Loss: 9.961286 Dis Loss: 0.00018577799 |||| 9.360385 -13.304405 -7.6917725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 359 Gen Loss: 194.3341 Recon Loss: 194.50046 Gen ADV Loss: 10.243111 Dis Loss: 0.00019848443 |||| 9.365219 -12.382145 -6.503922\n",
      "Iter: 360 Gen Loss: 192.82413 Recon Loss: 192.9894 Gen ADV Loss: 9.742996 Dis Loss: 0.00052438665 |||| 9.678918 -11.677323 -3.0977023\n",
      "Iter: 361 Gen Loss: 186.00143 Recon Loss: 186.16052 Gen ADV Loss: 9.039185 Dis Loss: 0.0003504126 |||| 9.727932 -13.086866 -5.160398\n",
      "Iter: 362 Gen Loss: 205.5977 Recon Loss: 205.77623 Gen ADV Loss: 9.203168 Dis Loss: 0.0002097936 |||| 9.098812 -14.098116 -6.524252\n",
      "Iter: 363 Gen Loss: 187.32292 Recon Loss: 187.4819 Gen ADV Loss: 10.428088 Dis Loss: 0.00022683466 |||| 9.222419 -14.73659 -5.823194\n",
      "Iter: 364 Gen Loss: 185.31064 Recon Loss: 185.46733 Gen ADV Loss: 10.647473 Dis Loss: 0.00021442739 |||| 8.993739 -14.372916 -6.9045553\n",
      "Iter: 365 Gen Loss: 185.69586 Recon Loss: 185.85295 Gen ADV Loss: 10.576189 Dis Loss: 0.0002157008 |||| 9.341769 -14.358713 -5.062651\n",
      "Iter: 366 Gen Loss: 184.97276 Recon Loss: 185.12933 Gen ADV Loss: 10.3092785 Dis Loss: 0.00021088174 |||| 9.471426 -12.427953 -5.4942007\n",
      "Iter: 367 Gen Loss: 179.54393 Recon Loss: 179.69516 Gen ADV Loss: 10.173924 Dis Loss: 0.00023896538 |||| 8.860588 -15.378627 -7.042486\n",
      "Iter: 368 Gen Loss: 179.50323 Recon Loss: 179.65408 Gen ADV Loss: 10.506997 Dis Loss: 0.00022238483 |||| 8.865149 -14.388442 -7.579578\n",
      "Iter: 369 Gen Loss: 183.1581 Recon Loss: 183.31285 Gen ADV Loss: 10.197782 Dis Loss: 0.0003098506 |||| 8.709823 -13.421162 -6.411653\n",
      "Iter: 370 Gen Loss: 199.49663 Recon Loss: 199.668 Gen ADV Loss: 9.852858 Dis Loss: 0.0002261084 |||| 9.553462 -12.770016 -5.7667975\n",
      "Iter: 371 Gen Loss: 184.20494 Recon Loss: 184.3612 Gen ADV Loss: 9.6233225 Dis Loss: 0.00020788248 |||| 9.065298 -13.531546 -7.270822\n",
      "Iter: 372 Gen Loss: 185.39328 Recon Loss: 185.54977 Gen ADV Loss: 10.538422 Dis Loss: 0.00018171151 |||| 9.185697 -13.576902 -8.192448\n",
      "Iter: 373 Gen Loss: 175.70444 Recon Loss: 175.85072 Gen ADV Loss: 11.037139 Dis Loss: 0.00020477982 |||| 8.963724 -14.464978 -8.022365\n",
      "Iter: 374 Gen Loss: 181.78761 Recon Loss: 181.94028 Gen ADV Loss: 10.713337 Dis Loss: 0.00020637248 |||| 9.081188 -15.82455 -8.143072\n",
      "Iter: 375 Gen Loss: 181.5263 Recon Loss: 181.67885 Gen ADV Loss: 10.533857 Dis Loss: 0.00020796403 |||| 9.062351 -13.073001 -8.366656\n",
      "Iter: 376 Gen Loss: 180.25417 Recon Loss: 180.40543 Gen ADV Loss: 10.496468 Dis Loss: 0.00019889983 |||| 9.100559 -12.673833 -7.564135\n",
      "Iter: 377 Gen Loss: 193.42569 Recon Loss: 193.59012 Gen ADV Loss: 10.469314 Dis Loss: 0.0001808644 |||| 9.750725 -13.24849 -6.704835\n",
      "Iter: 378 Gen Loss: 182.18452 Recon Loss: 182.3383 Gen ADV Loss: 9.848291 Dis Loss: 0.0001864181 |||| 9.530589 -12.238402 -6.763009\n",
      "Iter: 379 Gen Loss: 190.33305 Recon Loss: 190.49477 Gen ADV Loss: 9.992426 Dis Loss: 0.00015587843 |||| 9.420363 -13.434718 -7.6338544\n",
      "Iter: 380 Gen Loss: 184.88309 Recon Loss: 185.03864 Gen ADV Loss: 10.676143 Dis Loss: 0.00023089364 |||| 9.068945 -14.052591 -7.416579\n",
      "Iter: 381 Gen Loss: 190.71461 Recon Loss: 190.8762 Gen ADV Loss: 10.447087 Dis Loss: 0.00022000809 |||| 9.678852 -12.898368 -7.1015606\n",
      "Iter: 382 Gen Loss: 194.31949 Recon Loss: 194.48503 Gen ADV Loss: 10.055742 Dis Loss: 0.00024018544 |||| 9.605909 -12.862926 -6.555967\n",
      "Iter: 383 Gen Loss: 184.46843 Recon Loss: 184.62497 Gen ADV Loss: 9.1486845 Dis Loss: 0.00021669673 |||| 8.917819 -14.237872 -7.5495253\n",
      "Iter: 384 Gen Loss: 184.4698 Recon Loss: 184.62503 Gen ADV Loss: 10.450832 Dis Loss: 0.00017273129 |||| 9.530171 -14.694015 -7.7054744\n",
      "Iter: 385 Gen Loss: 184.9688 Recon Loss: 185.12431 Gen ADV Loss: 10.613123 Dis Loss: 0.00022717597 |||| 9.020529 -13.524055 -7.6769032\n",
      "Iter: 386 Gen Loss: 185.54874 Recon Loss: 185.70508 Gen ADV Loss: 10.313965 Dis Loss: 0.00024215909 |||| 8.841031 -12.821374 -6.4719853\n",
      "Iter: 387 Gen Loss: 183.36871 Recon Loss: 183.52263 Gen ADV Loss: 10.488107 Dis Loss: 0.00018264806 |||| 9.192375 -13.314927 -7.329483\n",
      "Iter: 388 Gen Loss: 177.61801 Recon Loss: 177.7658 Gen ADV Loss: 10.852186 Dis Loss: 0.00014081459 |||| 9.792264 -12.955701 -8.06755\n",
      "Iter: 389 Gen Loss: 185.52531 Recon Loss: 185.68134 Gen ADV Loss: 10.502674 Dis Loss: 0.00015976603 |||| 9.64711 -12.916025 -6.6426897\n",
      "Iter: 390 Gen Loss: 195.73029 Recon Loss: 195.89687 Gen ADV Loss: 10.1096525 Dis Loss: 0.00017986195 |||| 9.175711 -12.903506 -8.665773\n",
      "Iter: 391 Gen Loss: 188.76643 Recon Loss: 188.92555 Gen ADV Loss: 10.578442 Dis Loss: 0.00017376275 |||| 9.250379 -14.3965 -8.611871\n",
      "Iter: 392 Gen Loss: 183.98862 Recon Loss: 184.14252 Gen ADV Loss: 10.980203 Dis Loss: 0.00020188624 |||| 9.325778 -13.8273325 -7.1599326\n",
      "Iter: 393 Gen Loss: 180.79294 Recon Loss: 180.94415 Gen ADV Loss: 10.426783 Dis Loss: 0.00016480344 |||| 9.491158 -13.06126 -7.864508\n",
      "Iter: 394 Gen Loss: 188.99747 Recon Loss: 189.15738 Gen ADV Loss: 9.881713 Dis Loss: 0.00015448773 |||| 9.537566 -13.3059635 -7.6990786\n",
      "Iter: 395 Gen Loss: 195.05156 Recon Loss: 195.217 Gen ADV Loss: 10.372049 Dis Loss: 0.00022507663 |||| 9.694907 -12.945696 -6.404208\n",
      "Iter: 396 Gen Loss: 174.31926 Recon Loss: 174.4642 Gen ADV Loss: 10.102016 Dis Loss: 0.0002971053 |||| 8.933008 -13.744799 -6.183477\n",
      "Iter: 397 Gen Loss: 182.74295 Recon Loss: 182.89572 Gen ADV Loss: 10.635153 Dis Loss: 0.00015409684 |||| 9.743836 -13.689608 -8.398502\n",
      "Iter: 398 Gen Loss: 177.49252 Recon Loss: 177.63994 Gen ADV Loss: 10.696093 Dis Loss: 0.00023572307 |||| 9.656635 -12.229211 -6.4466405\n",
      "Iter: 399 Gen Loss: 189.63037 Recon Loss: 189.79129 Gen ADV Loss: 9.277971 Dis Loss: 0.00022121295 |||| 9.435964 -11.922896 -6.055375\n",
      "========================================================================\n",
      "7.664593 -0.7944894\n",
      "6.836052 -0.67548954\n",
      "6.6339602 -0.6655303\n",
      "7.496964 -0.57949144\n",
      "6.5351224 -0.5198686\n",
      "5.0078998 -0.55096954\n",
      "7.6602125 0.0\n",
      "9.856635 0.0\n",
      "13.068982 0.0\n",
      "24.21239 0.0\n",
      "1.787895 -1.6728511\n",
      "0.9455379 -0.93192756\n",
      "174.62238 0.00022871472\n",
      "=========================================================================\n",
      "Iter: 400 Gen Loss: 186.30617 Recon Loss: 186.46312 Gen ADV Loss: 9.86987 Dis Loss: 0.0001634077 |||| 9.585225 -13.511663 -6.407016\n",
      "Iter: 401 Gen Loss: 188.24739 Recon Loss: 188.40584 Gen ADV Loss: 10.262888 Dis Loss: 0.00026470603 |||| 8.747227 -14.203261 -6.8984933\n",
      "Iter: 402 Gen Loss: 176.15979 Recon Loss: 176.30504 Gen ADV Loss: 11.3365345 Dis Loss: 0.00017068422 |||| 9.358201 -14.164604 -8.132826\n",
      "Iter: 403 Gen Loss: 180.11418 Recon Loss: 180.26396 Gen ADV Loss: 10.759713 Dis Loss: 0.0001878147 |||| 9.545976 -12.160165 -6.300486\n",
      "Iter: 404 Gen Loss: 179.78958 Recon Loss: 179.93976 Gen ADV Loss: 9.982606 Dis Loss: 0.00020646495 |||| 9.704126 -12.666298 -6.4074273\n",
      "Iter: 405 Gen Loss: 186.7128 Recon Loss: 186.8704 Gen ADV Loss: 9.44832 Dis Loss: 0.00021187503 |||| 9.195684 -12.96493 -6.298821\n",
      "Iter: 406 Gen Loss: 185.5841 Recon Loss: 185.73943 Gen ADV Loss: 10.5275345 Dis Loss: 0.00014832296 |||| 9.984077 -13.294002 -6.8781815\n",
      "Iter: 407 Gen Loss: 182.95332 Recon Loss: 183.10657 Gen ADV Loss: 9.955859 Dis Loss: 0.0003854794 |||| 9.577152 -11.926404 -4.2347336\n",
      "Iter: 408 Gen Loss: 175.82108 Recon Loss: 175.96732 Gen ADV Loss: 9.778504 Dis Loss: 0.00031836273 |||| 9.847809 -12.294245 -4.5514965\n",
      "Iter: 409 Gen Loss: 182.36086 Recon Loss: 182.51482 Gen ADV Loss: 8.573779 Dis Loss: 0.00031493415 |||| 9.698327 -12.060589 -5.2427135\n",
      "Iter: 410 Gen Loss: 190.29968 Recon Loss: 190.46075 Gen ADV Loss: 9.379797 Dis Loss: 0.0001800163 |||| 9.523345 -12.575709 -6.447135\n",
      "Iter: 411 Gen Loss: 191.30463 Recon Loss: 191.4657 Gen ADV Loss: 10.353372 Dis Loss: 0.00016319749 |||| 9.396224 -14.944846 -7.7023487\n",
      "Iter: 412 Gen Loss: 179.1697 Recon Loss: 179.318 Gen ADV Loss: 10.979662 Dis Loss: 0.00017760762 |||| 9.129123 -14.351285 -8.610962\n",
      "Iter: 413 Gen Loss: 182.57707 Recon Loss: 182.72862 Gen ADV Loss: 11.085099 Dis Loss: 0.0001501201 |||| 9.791538 -14.376606 -8.196126\n",
      "Iter: 414 Gen Loss: 181.0724 Recon Loss: 181.22278 Gen ADV Loss: 10.697202 Dis Loss: 0.00018292546 |||| 9.185413 -14.1610985 -7.62355\n",
      "Iter: 415 Gen Loss: 182.40927 Recon Loss: 182.56136 Gen ADV Loss: 10.291898 Dis Loss: 0.00018436348 |||| 9.059951 -14.101306 -7.7858424\n",
      "Iter: 416 Gen Loss: 179.84756 Recon Loss: 179.99628 Gen ADV Loss: 11.030447 Dis Loss: 0.0001620533 |||| 9.308224 -14.123425 -8.403763\n",
      "Iter: 417 Gen Loss: 192.43896 Recon Loss: 192.60045 Gen ADV Loss: 10.839577 Dis Loss: 0.00022518588 |||| 9.857 -11.938751 -5.800886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 418 Gen Loss: 185.05873 Recon Loss: 185.21434 Gen ADV Loss: 9.276488 Dis Loss: 0.0003090292 |||| 9.768215 -11.784393 -4.2814484\n",
      "Iter: 419 Gen Loss: 177.00464 Recon Loss: 177.152 Gen ADV Loss: 9.448848 Dis Loss: 0.0001519536 |||| 9.748448 -13.543277 -6.9912715\n",
      "Iter: 420 Gen Loss: 172.91492 Recon Loss: 173.0569 Gen ADV Loss: 10.682095 Dis Loss: 0.00016121831 |||| 9.453815 -15.050491 -7.276159\n",
      "Iter: 421 Gen Loss: 193.32266 Recon Loss: 193.48486 Gen ADV Loss: 10.854156 Dis Loss: 0.0001598841 |||| 9.780194 -15.457848 -6.985312\n",
      "Iter: 422 Gen Loss: 176.35652 Recon Loss: 176.50217 Gen ADV Loss: 10.390332 Dis Loss: 0.00025411192 |||| 9.063479 -14.167206 -7.4793935\n",
      "Iter: 423 Gen Loss: 186.58125 Recon Loss: 186.7372 Gen ADV Loss: 10.279945 Dis Loss: 0.00015034902 |||| 9.831656 -13.605919 -6.1936383\n",
      "Iter: 424 Gen Loss: 183.33444 Recon Loss: 183.48685 Gen ADV Loss: 10.531719 Dis Loss: 0.0002005723 |||| 9.008825 -13.625583 -7.370764\n",
      "Iter: 425 Gen Loss: 181.81442 Recon Loss: 181.96483 Gen ADV Loss: 10.93522 Dis Loss: 0.00017161656 |||| 9.703435 -13.742365 -7.487992\n",
      "Iter: 426 Gen Loss: 181.4053 Recon Loss: 181.55609 Gen ADV Loss: 10.120844 Dis Loss: 0.00025001197 |||| 9.301558 -14.43696 -6.3410563\n",
      "Iter: 427 Gen Loss: 185.67523 Recon Loss: 185.83104 Gen ADV Loss: 9.312353 Dis Loss: 0.00026287 |||| 9.49232 -13.610859 -5.5440087\n",
      "Iter: 428 Gen Loss: 191.61237 Recon Loss: 191.77412 Gen ADV Loss: 9.254379 Dis Loss: 0.00036322296 |||| 8.636884 -13.87381 -6.739268\n",
      "Iter: 429 Gen Loss: 209.06474 Recon Loss: 209.2425 Gen ADV Loss: 10.676848 Dis Loss: 0.00013316434 |||| 10.20254 -13.200687 -7.7828016\n",
      "Iter: 430 Gen Loss: 182.21344 Recon Loss: 182.36412 Gen ADV Loss: 10.833082 Dis Loss: 0.0003086662 |||| 8.642117 -14.12272 -4.2002063\n",
      "Iter: 431 Gen Loss: 181.98314 Recon Loss: 182.13309 Gen ADV Loss: 11.294481 Dis Loss: 0.00016459558 |||| 9.587874 -13.537131 -7.2337575\n",
      "Iter: 432 Gen Loss: 179.7557 Recon Loss: 179.90417 Gen ADV Loss: 10.491879 Dis Loss: 0.00017336928 |||| 9.394856 -13.37759 -7.52133\n",
      "Iter: 433 Gen Loss: 181.14336 Recon Loss: 181.29301 Gen ADV Loss: 10.649469 Dis Loss: 0.00019890114 |||| 9.326995 -13.935427 -8.38558\n",
      "Iter: 434 Gen Loss: 186.05579 Recon Loss: 186.21014 Gen ADV Loss: 10.838863 Dis Loss: 0.00016847474 |||| 10.048758 -12.51041 -6.294549\n",
      "Iter: 435 Gen Loss: 178.02934 Recon Loss: 178.1759 Gen ADV Loss: 10.558824 Dis Loss: 0.00022174101 |||| 9.006758 -14.177747 -7.816586\n",
      "Iter: 436 Gen Loss: 182.9244 Recon Loss: 183.07578 Gen ADV Loss: 10.619138 Dis Loss: 0.00017591193 |||| 9.361162 -14.675981 -6.668731\n",
      "Iter: 437 Gen Loss: 176.85097 Recon Loss: 176.99628 Gen ADV Loss: 10.566397 Dis Loss: 0.00029428117 |||| 9.501376 -12.430701 -4.8518677\n",
      "Iter: 438 Gen Loss: 179.00574 Recon Loss: 179.15346 Gen ADV Loss: 10.226925 Dis Loss: 0.0002063998 |||| 9.977402 -13.27679 -6.244757\n",
      "Iter: 439 Gen Loss: 179.26906 Recon Loss: 179.41762 Gen ADV Loss: 9.598732 Dis Loss: 0.00020452018 |||| 9.194971 -15.51141 -7.593819\n",
      "Iter: 440 Gen Loss: 172.9643 Recon Loss: 173.10533 Gen ADV Loss: 10.776973 Dis Loss: 0.00017212753 |||| 9.429619 -14.015248 -8.473187\n",
      "Iter: 441 Gen Loss: 173.24124 Recon Loss: 173.38268 Gen ADV Loss: 10.61287 Dis Loss: 0.00016170285 |||| 9.52944 -14.090723 -7.9550047\n",
      "Iter: 442 Gen Loss: 178.25677 Recon Loss: 178.40303 Gen ADV Loss: 10.784682 Dis Loss: 0.00016737333 |||| 9.771841 -13.251387 -5.651848\n",
      "Iter: 443 Gen Loss: 178.11057 Recon Loss: 178.25713 Gen ADV Loss: 10.301582 Dis Loss: 0.00016081307 |||| 9.744705 -13.282616 -7.3093376\n",
      "Iter: 444 Gen Loss: 177.8341 Recon Loss: 177.98041 Gen ADV Loss: 10.254997 Dis Loss: 0.00014946313 |||| 9.561976 -13.950148 -7.965767\n",
      "Iter: 445 Gen Loss: 179.10657 Recon Loss: 179.25388 Gen ADV Loss: 10.478119 Dis Loss: 0.0002040297 |||| 9.726431 -12.243081 -6.273448\n",
      "Iter: 446 Gen Loss: 184.53201 Recon Loss: 184.68483 Gen ADV Loss: 10.361646 Dis Loss: 0.00018516599 |||| 9.377771 -13.433875 -6.342406\n",
      "Iter: 447 Gen Loss: 185.33653 Recon Loss: 185.49005 Gen ADV Loss: 10.409771 Dis Loss: 0.00014242288 |||| 9.85985 -12.747771 -6.833153\n",
      "Iter: 448 Gen Loss: 185.07164 Recon Loss: 185.22472 Gen ADV Loss: 10.5447445 Dis Loss: 0.00019973647 |||| 10.01439 -12.738377 -6.361043\n",
      "Iter: 449 Gen Loss: 180.12837 Recon Loss: 180.27774 Gen ADV Loss: 9.318689 Dis Loss: 0.00032679923 |||| 9.5428505 -13.805682 -5.1440625\n",
      "Iter: 450 Gen Loss: 180.7536 Recon Loss: 180.9035 Gen ADV Loss: 9.347437 Dis Loss: 0.00016990214 |||| 9.6957 -13.506516 -6.6097975\n",
      "Iter: 451 Gen Loss: 178.22157 Recon Loss: 178.3682 Gen ADV Loss: 10.02131 Dis Loss: 0.00016943605 |||| 9.881962 -13.389767 -6.1601167\n",
      "Iter: 452 Gen Loss: 179.8568 Recon Loss: 180.00455 Gen ADV Loss: 10.508058 Dis Loss: 0.00016067695 |||| 9.412261 -13.693642 -8.676147\n",
      "Iter: 453 Gen Loss: 177.80788 Recon Loss: 177.95355 Gen ADV Loss: 10.512321 Dis Loss: 0.00027381655 |||| 9.840948 -12.576354 -4.7389145\n",
      "Iter: 454 Gen Loss: 172.43327 Recon Loss: 172.57385 Gen ADV Loss: 10.177707 Dis Loss: 0.00021054584 |||| 9.515247 -12.967041 -5.1660438\n",
      "Iter: 455 Gen Loss: 183.92955 Recon Loss: 184.08148 Gen ADV Loss: 10.30393 Dis Loss: 0.00016601998 |||| 9.419928 -14.241354 -7.105048\n",
      "Iter: 456 Gen Loss: 173.04523 Recon Loss: 173.18546 Gen ADV Loss: 11.061118 Dis Loss: 0.00020148297 |||| 9.129701 -14.174702 -6.7688174\n",
      "Iter: 457 Gen Loss: 195.2053 Recon Loss: 195.36775 Gen ADV Loss: 10.950574 Dis Loss: 0.00018910541 |||| 9.263549 -14.739593 -7.4213815\n",
      "Iter: 458 Gen Loss: 179.11191 Recon Loss: 179.2583 Gen ADV Loss: 10.887108 Dis Loss: 0.00015817398 |||| 9.662367 -16.031437 -7.1504173\n",
      "Iter: 459 Gen Loss: 182.41788 Recon Loss: 182.5673 Gen ADV Loss: 11.123279 Dis Loss: 0.00016853845 |||| 9.53259 -14.905517 -7.9438653\n",
      "Iter: 460 Gen Loss: 185.31078 Recon Loss: 185.46344 Gen ADV Loss: 10.738242 Dis Loss: 0.0003627828 |||| 9.536404 -12.453493 -5.080662\n",
      "Iter: 461 Gen Loss: 173.0029 Recon Loss: 173.14459 Gen ADV Loss: 9.348041 Dis Loss: 0.00022473147 |||| 9.3903675 -13.113293 -6.470614\n",
      "Iter: 462 Gen Loss: 184.11519 Recon Loss: 184.26738 Gen ADV Loss: 9.9502325 Dis Loss: 0.00015631165 |||| 9.445854 -14.103282 -8.096222\n",
      "Iter: 463 Gen Loss: 185.9149 Recon Loss: 186.06796 Gen ADV Loss: 10.839891 Dis Loss: 0.0001516628 |||| 9.941715 -13.113819 -7.3505363\n",
      "Iter: 464 Gen Loss: 177.48578 Recon Loss: 177.63086 Gen ADV Loss: 10.350181 Dis Loss: 0.00042349607 |||| 9.579525 -13.43798 -2.918176\n",
      "Iter: 465 Gen Loss: 175.67476 Recon Loss: 175.81847 Gen ADV Loss: 9.865861 Dis Loss: 0.00019086737 |||| 9.492839 -14.341343 -7.0710354\n",
      "Iter: 466 Gen Loss: 178.48071 Recon Loss: 178.62701 Gen ADV Loss: 10.040494 Dis Loss: 0.00022428046 |||| 9.721657 -14.835907 -6.8002005\n",
      "Iter: 467 Gen Loss: 184.11241 Recon Loss: 184.26459 Gen ADV Loss: 9.803045 Dis Loss: 0.00019012598 |||| 9.403533 -15.995892 -7.3719397\n",
      "Iter: 468 Gen Loss: 175.75322 Recon Loss: 175.89622 Gen ADV Loss: 10.563003 Dis Loss: 0.00017341443 |||| 9.443963 -14.55142 -6.9074545\n",
      "Iter: 469 Gen Loss: 183.75388 Recon Loss: 183.90518 Gen ADV Loss: 10.217346 Dis Loss: 0.00024061429 |||| 9.208434 -15.146646 -6.117116\n",
      "Iter: 470 Gen Loss: 178.2961 Recon Loss: 178.44153 Gen ADV Loss: 10.594273 Dis Loss: 0.00017307361 |||| 9.872623 -14.836317 -7.1866775\n",
      "Iter: 471 Gen Loss: 175.31349 Recon Loss: 175.45613 Gen ADV Loss: 10.390316 Dis Loss: 0.00017806693 |||| 9.489071 -14.40846 -6.851519\n",
      "Iter: 472 Gen Loss: 178.80788 Recon Loss: 178.95404 Gen ADV Loss: 10.3210335 Dis Loss: 0.0001929389 |||| 9.291283 -14.08212 -7.5809917\n",
      "Iter: 473 Gen Loss: 175.56613 Recon Loss: 175.70871 Gen ADV Loss: 10.602361 Dis Loss: 0.00015857715 |||| 9.539864 -13.914265 -7.9391394\n",
      "Iter: 474 Gen Loss: 183.35625 Recon Loss: 183.50653 Gen ADV Loss: 10.670233 Dis Loss: 0.0001579505 |||| 9.565927 -14.499713 -7.163542\n",
      "Iter: 475 Gen Loss: 181.97183 Recon Loss: 182.12064 Gen ADV Loss: 10.727185 Dis Loss: 0.00015705283 |||| 9.79857 -14.26352 -7.14303\n",
      "Iter: 476 Gen Loss: 178.29181 Recon Loss: 178.4375 Gen ADV Loss: 10.11449 Dis Loss: 0.00025487755 |||| 9.21329 -13.962439 -5.8688703\n",
      "Iter: 477 Gen Loss: 178.50438 Recon Loss: 178.65024 Gen ADV Loss: 10.107822 Dis Loss: 0.00016982142 |||| 9.405926 -13.18933 -7.8294654\n",
      "Iter: 478 Gen Loss: 172.07243 Recon Loss: 172.21158 Gen ADV Loss: 10.313911 Dis Loss: 0.00022588216 |||| 9.5009165 -14.86137 -4.4930177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 479 Gen Loss: 174.67216 Recon Loss: 174.81332 Gen ADV Loss: 10.893555 Dis Loss: 0.00014811067 |||| 9.734339 -14.116719 -8.0066185\n",
      "Iter: 480 Gen Loss: 175.17404 Recon Loss: 175.31601 Gen ADV Loss: 10.52224 Dis Loss: 0.00022631737 |||| 9.281627 -13.8681755 -6.538431\n",
      "Iter: 481 Gen Loss: 177.26175 Recon Loss: 177.40633 Gen ADV Loss: 9.927478 Dis Loss: 0.00016301539 |||| 9.731143 -13.003595 -7.4994755\n",
      "Iter: 482 Gen Loss: 177.2722 Recon Loss: 177.41635 Gen ADV Loss: 10.34699 Dis Loss: 0.00026207615 |||| 10.05583 -14.659372 -5.3095155\n",
      "Iter: 483 Gen Loss: 178.76408 Recon Loss: 178.9105 Gen ADV Loss: 9.544509 Dis Loss: 0.00027467514 |||| 9.609424 -13.43059 -4.5595202\n",
      "Iter: 484 Gen Loss: 183.90146 Recon Loss: 184.05328 Gen ADV Loss: 9.252683 Dis Loss: 0.0002256167 |||| 9.500111 -13.114625 -6.6896086\n",
      "Iter: 485 Gen Loss: 190.29938 Recon Loss: 190.4568 Gen ADV Loss: 9.995367 Dis Loss: 0.00016696981 |||| 9.155835 -14.479421 -8.57054\n",
      "Iter: 486 Gen Loss: 176.53854 Recon Loss: 176.68082 Gen ADV Loss: 11.353233 Dis Loss: 0.00016406001 |||| 9.586368 -15.385448 -7.4170966\n",
      "Iter: 487 Gen Loss: 175.45706 Recon Loss: 175.59834 Gen ADV Loss: 11.233162 Dis Loss: 0.0001774792 |||| 9.535771 -15.072891 -5.372066\n",
      "Iter: 488 Gen Loss: 175.64061 Recon Loss: 175.78217 Gen ADV Loss: 11.091819 Dis Loss: 0.00014979689 |||| 9.879201 -13.467215 -7.6242766\n",
      "Iter: 489 Gen Loss: 187.98825 Recon Loss: 188.14285 Gen ADV Loss: 10.382529 Dis Loss: 0.00025953472 |||| 8.713591 -13.775132 -7.622675\n",
      "Iter: 490 Gen Loss: 166.84566 Recon Loss: 166.97818 Gen ADV Loss: 11.242413 Dis Loss: 0.00024745957 |||| 8.849375 -14.293488 -8.526216\n",
      "Iter: 491 Gen Loss: 187.10706 Recon Loss: 187.25955 Gen ADV Loss: 11.51449 Dis Loss: 0.00015852321 |||| 9.61778 -14.289374 -7.550966\n",
      "Iter: 492 Gen Loss: 177.62195 Recon Loss: 177.76538 Gen ADV Loss: 11.041839 Dis Loss: 0.00015259723 |||| 9.635443 -12.88448 -7.382359\n",
      "Iter: 493 Gen Loss: 182.06511 Recon Loss: 182.21321 Gen ADV Loss: 10.7593565 Dis Loss: 0.00015468421 |||| 9.983221 -12.566225 -7.86052\n",
      "Iter: 494 Gen Loss: 185.35793 Recon Loss: 185.50975 Gen ADV Loss: 10.281831 Dis Loss: 0.00025368665 |||| 9.886908 -15.548104 -4.489395\n",
      "Iter: 495 Gen Loss: 184.976 Recon Loss: 185.12788 Gen ADV Loss: 9.817694 Dis Loss: 0.00017311929 |||| 9.534781 -14.42466 -7.3728085\n",
      "Iter: 496 Gen Loss: 174.2943 Recon Loss: 174.43431 Gen ADV Loss: 10.954219 Dis Loss: 0.00015572738 |||| 9.564145 -13.191919 -8.584889\n",
      "Iter: 497 Gen Loss: 176.31825 Recon Loss: 176.46033 Gen ADV Loss: 10.861585 Dis Loss: 0.00015100144 |||| 9.689487 -14.017247 -7.92088\n",
      "Iter: 498 Gen Loss: 182.33784 Recon Loss: 182.48605 Gen ADV Loss: 10.742066 Dis Loss: 0.00019022831 |||| 9.558468 -13.334961 -6.5357323\n",
      "Iter: 499 Gen Loss: 177.80109 Recon Loss: 177.94498 Gen ADV Loss: 10.47093 Dis Loss: 0.00024180564 |||| 9.948653 -13.276989 -5.223421\n",
      "========================================================================\n",
      "9.588801 -1.1408535\n",
      "7.634111 -0.6258823\n",
      "7.133478 -0.8025338\n",
      "8.261403 -0.6076233\n",
      "6.626537 -0.55375165\n",
      "5.9591002 -0.5779365\n",
      "7.5856924 0.0\n",
      "9.093033 0.0\n",
      "14.034759 0.0\n",
      "26.09412 0.0\n",
      "1.8505936 -2.063626\n",
      "0.95180184 -0.96825767\n",
      "169.5808 0.00019200597\n",
      "=========================================================================\n",
      "Iter: 500 Gen Loss: 169.61029 Recon Loss: 169.74643 Gen ADV Loss: 9.979056 Dis Loss: 0.00023294028 |||| 9.42454 -13.344158 -7.273602\n",
      "Iter: 501 Gen Loss: 174.84007 Recon Loss: 174.98181 Gen ADV Loss: 9.555073 Dis Loss: 0.00018259061 |||| 9.760769 -13.25955 -6.8676577\n",
      "Iter: 502 Gen Loss: 176.04237 Recon Loss: 176.18503 Gen ADV Loss: 9.8078165 Dis Loss: 0.00018924191 |||| 9.760291 -11.896654 -6.316484\n",
      "Iter: 503 Gen Loss: 185.54881 Recon Loss: 185.70117 Gen ADV Loss: 9.588022 Dis Loss: 0.00020616912 |||| 9.053141 -14.975261 -7.5466094\n",
      "Iter: 504 Gen Loss: 176.46042 Recon Loss: 176.60251 Gen ADV Loss: 10.733303 Dis Loss: 0.00015607863 |||| 9.503963 -14.500867 -7.53479\n",
      "Iter: 505 Gen Loss: 174.99095 Recon Loss: 175.13095 Gen ADV Loss: 11.297751 Dis Loss: 0.00016943604 |||| 9.9828205 -15.688647 -7.5448318\n",
      "Iter: 506 Gen Loss: 181.42638 Recon Loss: 181.57384 Gen ADV Loss: 10.238056 Dis Loss: 0.00019898216 |||| 9.713056 -12.999896 -6.1833987\n",
      "Iter: 507 Gen Loss: 172.90237 Recon Loss: 173.04109 Gen ADV Loss: 10.403555 Dis Loss: 0.00038373898 |||| 10.045764 -14.347119 -3.9261034\n",
      "Iter: 508 Gen Loss: 185.73657 Recon Loss: 185.88853 Gen ADV Loss: 9.973026 Dis Loss: 0.0001670521 |||| 9.455466 -14.105845 -7.452266\n",
      "Iter: 509 Gen Loss: 178.15666 Recon Loss: 178.30005 Gen ADV Loss: 10.951905 Dis Loss: 0.00013444401 |||| 9.871851 -15.519513 -8.216567\n",
      "Iter: 510 Gen Loss: 170.85046 Recon Loss: 170.98636 Gen ADV Loss: 11.121895 Dis Loss: 0.00018154111 |||| 9.311946 -14.932107 -7.871806\n",
      "Iter: 511 Gen Loss: 168.64095 Recon Loss: 168.77489 Gen ADV Loss: 10.824747 Dis Loss: 0.00020906836 |||| 9.467868 -14.6029825 -6.04214\n",
      "Iter: 512 Gen Loss: 172.44551 Recon Loss: 172.5838 Gen ADV Loss: 10.210191 Dis Loss: 0.0003661059 |||| 9.1321945 -12.341112 -5.692643\n",
      "Iter: 513 Gen Loss: 185.85379 Recon Loss: 186.00558 Gen ADV Loss: 10.060752 Dis Loss: 0.00014660074 |||| 10.061019 -13.870217 -6.417572\n",
      "Iter: 514 Gen Loss: 177.89346 Recon Loss: 178.03699 Gen ADV Loss: 10.331038 Dis Loss: 0.00026514454 |||| 8.932074 -13.983827 -7.2296968\n",
      "Iter: 515 Gen Loss: 175.91139 Recon Loss: 176.05243 Gen ADV Loss: 10.795462 Dis Loss: 0.00015267178 |||| 9.50346 -14.261122 -8.37153\n",
      "Iter: 516 Gen Loss: 180.6766 Recon Loss: 180.82193 Gen ADV Loss: 11.198002 Dis Loss: 0.00014945134 |||| 9.620499 -14.527124 -8.1332855\n",
      "Iter: 517 Gen Loss: 184.50636 Recon Loss: 184.6558 Gen ADV Loss: 10.8629055 Dis Loss: 0.0002450202 |||| 10.030888 -13.541909 -5.0940185\n",
      "Iter: 518 Gen Loss: 181.54204 Recon Loss: 181.68954 Gen ADV Loss: 9.808542 Dis Loss: 0.00018763596 |||| 10.193232 -13.080789 -6.4449177\n",
      "Iter: 519 Gen Loss: 181.6668 Recon Loss: 181.81425 Gen ADV Loss: 9.976933 Dis Loss: 0.00022072821 |||| 9.475502 -15.538504 -7.5108547\n",
      "Iter: 520 Gen Loss: 175.10535 Recon Loss: 175.24539 Gen ADV Loss: 10.798828 Dis Loss: 0.000125226 |||| 9.992129 -13.636604 -7.6715345\n",
      "Iter: 521 Gen Loss: 178.7799 Recon Loss: 178.92343 Gen ADV Loss: 10.94822 Dis Loss: 0.00015082325 |||| 9.833019 -13.561945 -6.287858\n",
      "Iter: 522 Gen Loss: 184.42361 Recon Loss: 184.57254 Gen ADV Loss: 11.140891 Dis Loss: 0.0001243465 |||| 10.185992 -14.8933325 -7.243901\n",
      "Iter: 523 Gen Loss: 172.17674 Recon Loss: 172.3137 Gen ADV Loss: 10.850112 Dis Loss: 0.00019957256 |||| 9.678787 -13.808716 -5.681276\n",
      "Iter: 524 Gen Loss: 176.45235 Recon Loss: 176.59387 Gen ADV Loss: 10.504463 Dis Loss: 0.00017236696 |||| 9.709414 -13.41434 -7.3025274\n",
      "Iter: 525 Gen Loss: 174.29305 Recon Loss: 174.4325 Gen ADV Loss: 10.348807 Dis Loss: 0.00013342005 |||| 9.830054 -13.540836 -7.400417\n",
      "Iter: 526 Gen Loss: 183.53925 Recon Loss: 183.68732 Gen ADV Loss: 10.957524 Dis Loss: 0.00014511858 |||| 9.616648 -14.314408 -8.199332\n",
      "Iter: 527 Gen Loss: 172.94998 Recon Loss: 173.08719 Gen ADV Loss: 11.168923 Dis Loss: 0.00014923437 |||| 9.684127 -14.825838 -6.1208377\n",
      "Iter: 528 Gen Loss: 177.9219 Recon Loss: 178.06464 Gen ADV Loss: 10.5764675 Dis Loss: 0.00019039062 |||| 9.990634 -13.488709 -5.801201\n",
      "Iter: 529 Gen Loss: 170.58589 Recon Loss: 170.72157 Gen ADV Loss: 10.25556 Dis Loss: 0.00025367964 |||| 9.391878 -13.082629 -4.677506\n",
      "Iter: 530 Gen Loss: 174.74432 Recon Loss: 174.88435 Gen ADV Loss: 10.028772 Dis Loss: 0.0003486341 |||| 9.67284 -13.752603 -6.668858\n",
      "Iter: 531 Gen Loss: 172.25269 Recon Loss: 172.38972 Gen ADV Loss: 10.481662 Dis Loss: 0.00016550481 |||| 9.92578 -14.800411 -5.3067904\n",
      "Iter: 532 Gen Loss: 179.39124 Recon Loss: 179.53511 Gen ADV Loss: 10.763606 Dis Loss: 0.00013789888 |||| 9.810824 -14.476733 -7.7390256\n",
      "Iter: 533 Gen Loss: 169.35977 Recon Loss: 169.49342 Gen ADV Loss: 10.909593 Dis Loss: 0.00025445278 |||| 9.306336 -14.960785 -5.620478\n",
      "Iter: 534 Gen Loss: 172.76723 Recon Loss: 172.90463 Gen ADV Loss: 10.486924 Dis Loss: 0.00016957044 |||| 9.704129 -14.382321 -6.8125367\n",
      "Iter: 535 Gen Loss: 182.50714 Recon Loss: 182.65431 Gen ADV Loss: 10.429419 Dis Loss: 0.00014493088 |||| 9.655289 -14.677922 -8.294173\n",
      "Iter: 536 Gen Loss: 177.77786 Recon Loss: 177.91977 Gen ADV Loss: 10.9443655 Dis Loss: 0.0001349597 |||| 9.983489 -13.61379 -7.5842943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 537 Gen Loss: 175.70212 Recon Loss: 175.84206 Gen ADV Loss: 10.778077 Dis Loss: 0.00016560082 |||| 9.672759 -14.040907 -7.141843\n",
      "Iter: 538 Gen Loss: 179.17093 Recon Loss: 179.31453 Gen ADV Loss: 10.54321 Dis Loss: 0.00016306553 |||| 9.817633 -14.093778 -5.7081437\n",
      "Iter: 539 Gen Loss: 171.84824 Recon Loss: 171.98401 Gen ADV Loss: 11.015198 Dis Loss: 0.00014401051 |||| 9.71672 -13.606534 -6.736842\n",
      "Iter: 540 Gen Loss: 175.62862 Recon Loss: 175.76851 Gen ADV Loss: 10.621252 Dis Loss: 0.00017391128 |||| 9.342135 -14.364509 -8.81857\n",
      "Iter: 541 Gen Loss: 170.19225 Recon Loss: 170.32605 Gen ADV Loss: 11.207649 Dis Loss: 0.00020988777 |||| 9.010706 -14.964449 -8.330775\n",
      "Iter: 542 Gen Loss: 196.24178 Recon Loss: 196.40128 Gen ADV Loss: 11.550398 Dis Loss: 0.00012861055 |||| 9.986129 -14.36648 -7.708737\n",
      "Iter: 543 Gen Loss: 166.99644 Recon Loss: 167.12685 Gen ADV Loss: 11.357632 Dis Loss: 0.00015406511 |||| 9.577628 -14.021503 -8.441807\n",
      "Iter: 544 Gen Loss: 177.57413 Recon Loss: 177.71579 Gen ADV Loss: 10.655283 Dis Loss: 0.00017378503 |||| 9.564806 -13.692905 -6.596098\n",
      "Iter: 545 Gen Loss: 182.29813 Recon Loss: 182.44414 Gen ADV Loss: 10.973758 Dis Loss: 0.00013674621 |||| 9.991866 -13.292989 -7.8748093\n",
      "Iter: 546 Gen Loss: 168.95767 Recon Loss: 169.09082 Gen ADV Loss: 10.416208 Dis Loss: 0.0001500328 |||| 9.739154 -13.568416 -7.3833084\n",
      "Iter: 547 Gen Loss: 174.70341 Recon Loss: 174.8422 Gen ADV Loss: 10.513271 Dis Loss: 0.00016610659 |||| 10.183574 -13.060464 -6.0699797\n",
      "Iter: 548 Gen Loss: 181.75623 Recon Loss: 181.90198 Gen ADV Loss: 10.549756 Dis Loss: 0.00013539527 |||| 9.965011 -14.20245 -7.7486143\n",
      "Iter: 549 Gen Loss: 176.86 Recon Loss: 177.001 Gen ADV Loss: 10.378143 Dis Loss: 0.00029731408 |||| 10.205516 -13.103199 -4.385247\n",
      "Iter: 550 Gen Loss: 173.902 Recon Loss: 174.04039 Gen ADV Loss: 10.010387 Dis Loss: 0.00015583326 |||| 9.595613 -14.277432 -6.5967064\n",
      "Iter: 551 Gen Loss: 171.46463 Recon Loss: 171.59969 Gen ADV Loss: 10.893728 Dis Loss: 0.00015842429 |||| 9.557784 -14.685473 -7.396214\n",
      "Iter: 552 Gen Loss: 176.57318 Recon Loss: 176.71295 Gen ADV Loss: 11.24415 Dis Loss: 0.00018534236 |||| 9.803114 -14.442038 -5.794858\n",
      "Iter: 553 Gen Loss: 181.26404 Recon Loss: 181.40878 Gen ADV Loss: 10.910283 Dis Loss: 0.00016483862 |||| 9.625737 -13.517306 -6.786851\n",
      "Iter: 554 Gen Loss: 171.37161 Recon Loss: 171.50671 Gen ADV Loss: 10.574723 Dis Loss: 0.00014479316 |||| 9.869542 -14.00757 -7.3800316\n",
      "Iter: 555 Gen Loss: 172.958 Recon Loss: 173.09444 Gen ADV Loss: 10.774866 Dis Loss: 0.00015392835 |||| 9.508477 -14.47168 -7.1486735\n",
      "Iter: 556 Gen Loss: 168.8285 Recon Loss: 168.96045 Gen ADV Loss: 11.089843 Dis Loss: 0.00017291243 |||| 10.043801 -14.167963 -5.472492\n",
      "Iter: 557 Gen Loss: 181.86177 Recon Loss: 182.00739 Gen ADV Loss: 10.437862 Dis Loss: 0.00025434376 |||| 10.240209 -11.86209 -5.9613404\n",
      "Iter: 558 Gen Loss: 176.40959 Recon Loss: 176.55104 Gen ADV Loss: 9.100578 Dis Loss: 0.00021886919 |||| 9.329942 -12.761126 -6.4114184\n",
      "Iter: 559 Gen Loss: 170.26366 Recon Loss: 170.39792 Gen ADV Loss: 10.106115 Dis Loss: 0.00018865924 |||| 9.18428 -13.740934 -7.1479416\n",
      "Iter: 560 Gen Loss: 170.96294 Recon Loss: 171.09705 Gen ADV Loss: 10.906212 Dis Loss: 0.0001327294 |||| 9.8097 -14.459839 -8.502509\n",
      "Iter: 561 Gen Loss: 172.90443 Recon Loss: 173.04016 Gen ADV Loss: 11.203451 Dis Loss: 0.00016021659 |||| 9.928012 -13.421323 -7.2084875\n",
      "Iter: 562 Gen Loss: 176.536 Recon Loss: 176.67564 Gen ADV Loss: 10.850033 Dis Loss: 0.00015832126 |||| 9.880811 -14.139712 -7.0908704\n",
      "Iter: 563 Gen Loss: 180.78925 Recon Loss: 180.93353 Gen ADV Loss: 10.469394 Dis Loss: 0.00023908948 |||| 10.286677 -13.098213 -5.3237367\n",
      "Iter: 564 Gen Loss: 175.02322 Recon Loss: 175.16234 Gen ADV Loss: 9.810056 Dis Loss: 0.00023189065 |||| 10.071789 -13.266469 -5.3705196\n",
      "Iter: 565 Gen Loss: 175.20465 Recon Loss: 175.34425 Gen ADV Loss: 9.459878 Dis Loss: 0.000164531 |||| 9.996788 -14.391601 -6.574684\n",
      "Iter: 566 Gen Loss: 169.45753 Recon Loss: 169.59029 Gen ADV Loss: 10.512712 Dis Loss: 0.00027615312 |||| 8.852837 -15.161497 -7.696435\n",
      "Iter: 567 Gen Loss: 176.72833 Recon Loss: 176.86758 Gen ADV Loss: 11.234788 Dis Loss: 0.00015009017 |||| 9.699811 -14.222466 -7.829517\n",
      "Iter: 568 Gen Loss: 179.50807 Recon Loss: 179.64993 Gen ADV Loss: 11.3534155 Dis Loss: 0.00019376841 |||| 10.106131 -14.636368 -5.1216226\n",
      "Iter: 569 Gen Loss: 170.2776 Recon Loss: 170.41153 Gen ADV Loss: 10.026142 Dis Loss: 0.00041422097 |||| 10.089217 -12.764843 -3.6267757\n",
      "Iter: 570 Gen Loss: 175.91609 Recon Loss: 176.05685 Gen ADV Loss: 8.807995 Dis Loss: 0.00020573489 |||| 9.368711 -13.869647 -7.3742456\n",
      "Iter: 571 Gen Loss: 176.73537 Recon Loss: 176.87477 Gen ADV Loss: 10.971195 Dis Loss: 0.00012970573 |||| 9.958665 -14.713758 -7.9004583\n",
      "Iter: 572 Gen Loss: 170.27689 Recon Loss: 170.40965 Gen ADV Loss: 11.089835 Dis Loss: 0.0001827951 |||| 10.016405 -14.007228 -7.1565337\n",
      "Iter: 573 Gen Loss: 175.79938 Recon Loss: 175.93738 Gen ADV Loss: 11.35225 Dis Loss: 0.00013782045 |||| 10.0463295 -15.285836 -7.4294505\n",
      "Iter: 574 Gen Loss: 174.5609 Recon Loss: 174.69824 Gen ADV Loss: 10.760584 Dis Loss: 0.00017831095 |||| 9.600971 -15.078434 -6.371152\n",
      "Iter: 575 Gen Loss: 174.45653 Recon Loss: 174.59344 Gen ADV Loss: 11.011029 Dis Loss: 0.00013919867 |||| 9.678639 -14.575728 -7.975686\n",
      "Iter: 576 Gen Loss: 168.62938 Recon Loss: 168.76056 Gen ADV Loss: 10.857403 Dis Loss: 0.00020218221 |||| 9.275358 -13.767384 -7.231254\n",
      "Iter: 577 Gen Loss: 168.45335 Recon Loss: 168.58441 Gen ADV Loss: 10.770647 Dis Loss: 0.00017824027 |||| 9.5912285 -14.256387 -7.2648864\n",
      "Iter: 578 Gen Loss: 167.56313 Recon Loss: 167.69351 Gen ADV Loss: 10.506512 Dis Loss: 0.00026508133 |||| 8.776248 -14.562065 -8.434679\n",
      "Iter: 579 Gen Loss: 187.57597 Recon Loss: 187.72615 Gen ADV Loss: 10.711601 Dis Loss: 0.00016658298 |||| 10.038774 -13.086874 -6.7475944\n",
      "Iter: 580 Gen Loss: 175.59868 Recon Loss: 175.737 Gen ADV Loss: 10.558867 Dis Loss: 0.00016612299 |||| 9.710951 -13.673184 -7.0859528\n",
      "Iter: 581 Gen Loss: 173.44833 Recon Loss: 173.58398 Gen ADV Loss: 11.038095 Dis Loss: 0.000278926 |||| 9.138106 -15.165751 -8.439552\n",
      "Iter: 582 Gen Loss: 177.65901 Recon Loss: 177.79843 Gen ADV Loss: 11.458331 Dis Loss: 0.0001302898 |||| 9.82367 -14.861173 -7.393845\n",
      "Iter: 583 Gen Loss: 173.0042 Recon Loss: 173.13916 Gen ADV Loss: 11.204876 Dis Loss: 0.00015594167 |||| 10.168434 -13.905403 -6.3468227\n",
      "Iter: 584 Gen Loss: 177.1072 Recon Loss: 177.2467 Gen ADV Loss: 10.731968 Dis Loss: 0.00018417563 |||| 9.762096 -13.703603 -5.7018847\n",
      "Iter: 585 Gen Loss: 170.51825 Recon Loss: 170.65129 Gen ADV Loss: 10.571359 Dis Loss: 0.00014110429 |||| 10.03024 -13.813079 -6.7973957\n",
      "Iter: 586 Gen Loss: 172.03369 Recon Loss: 172.16846 Gen ADV Loss: 10.334202 Dis Loss: 0.0002484449 |||| 9.444871 -13.272952 -6.692182\n",
      "Iter: 587 Gen Loss: 170.64555 Recon Loss: 170.77885 Gen ADV Loss: 10.36211 Dis Loss: 0.00037793684 |||| 9.773736 -12.213493 -4.01172\n",
      "Iter: 588 Gen Loss: 176.10828 Recon Loss: 176.24774 Gen ADV Loss: 9.612431 Dis Loss: 0.00022317117 |||| 10.006928 -12.661478 -5.7888384\n",
      "Iter: 589 Gen Loss: 173.55481 Recon Loss: 173.69174 Gen ADV Loss: 9.586266 Dis Loss: 0.00061882875 |||| 9.69199 -12.019758 -4.79708\n",
      "Iter: 590 Gen Loss: 188.39641 Recon Loss: 188.54938 Gen ADV Loss: 8.31246 Dis Loss: 0.00023054804 |||| 9.090149 -12.260714 -7.1289115\n",
      "Iter: 591 Gen Loss: 179.11043 Recon Loss: 179.2522 Gen ADV Loss: 10.172998 Dis Loss: 0.00024331904 |||| 9.499358 -13.52779 -6.200633\n",
      "Iter: 592 Gen Loss: 171.29222 Recon Loss: 171.42696 Gen ADV Loss: 9.397555 Dis Loss: 0.0013455908 |||| 8.7355 -12.808403 -3.297372\n",
      "Iter: 593 Gen Loss: 173.35059 Recon Loss: 173.48618 Gen ADV Loss: 10.554564 Dis Loss: 0.00020040767 |||| 9.620428 -14.550717 -6.5299354\n",
      "Iter: 594 Gen Loss: 171.34607 Recon Loss: 171.47998 Gen ADV Loss: 10.190993 Dis Loss: 0.00020980465 |||| 9.737319 -13.512562 -7.0751452\n",
      "Iter: 595 Gen Loss: 167.10497 Recon Loss: 167.23442 Gen ADV Loss: 10.373091 Dis Loss: 0.00026231664 |||| 9.609363 -14.600239 -6.3410754\n",
      "Iter: 596 Gen Loss: 171.83218 Recon Loss: 171.9665 Gen ADV Loss: 10.201177 Dis Loss: 0.00028903695 |||| 10.10933 -12.850164 -5.1092763\n",
      "Iter: 597 Gen Loss: 174.0602 Recon Loss: 174.19783 Gen ADV Loss: 9.099766 Dis Loss: 0.015632164 |||| 10.043346 -10.419788 1.8500593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 598 Gen Loss: 164.3929 Recon Loss: 164.5254 Gen ADV Loss: 4.4936047 Dis Loss: 0.20964386 |||| 5.6854353 -12.552493 3.470031\n",
      "Iter: 599 Gen Loss: 170.31674 Recon Loss: 170.45749 Gen ADV Loss: 2.184711 Dis Loss: 1.3462566 |||| 2.9806824 -14.253572 7.342652\n",
      "========================================================================\n",
      "7.549771 -0.85242176\n",
      "7.522462 -0.68875104\n",
      "8.80488 -0.8331987\n",
      "8.115602 -0.601556\n",
      "7.050896 -0.5550847\n",
      "4.985008 -0.4958857\n",
      "7.5061145 0.0\n",
      "9.134273 0.0\n",
      "14.68381 0.0\n",
      "26.18509 0.0\n",
      "1.910019 -1.8344922\n",
      "0.95708704 -0.95026374\n",
      "162.14052 6.487053\n",
      "=========================================================================\n",
      "Iter: 600 Gen Loss: 175.15582 Recon Loss: 175.29735 Gen ADV Loss: 6.1903687 Dis Loss: 2.1294847 |||| -3.8036542 -10.7592535 2.1130528\n",
      "Iter: 601 Gen Loss: 166.76347 Recon Loss: 166.90233 Gen ADV Loss: 0.37530282 Dis Loss: 0.9243068 |||| 2.2283788 -1.9374815 3.9088845\n",
      "Iter: 602 Gen Loss: 178.06667 Recon Loss: 178.21648 Gen ADV Loss: 0.66506606 Dis Loss: 0.61673284 |||| 1.1563697 -2.39225 2.5541627\n",
      "Iter: 603 Gen Loss: 170.24881 Recon Loss: 170.38976 Gen ADV Loss: 1.72613 Dis Loss: 0.68430173 |||| -0.49254242 -5.2101035 0.88941336\n",
      "Iter: 604 Gen Loss: 184.25023 Recon Loss: 184.40562 Gen ADV Loss: 1.215952 Dis Loss: 0.5668091 |||| 0.47587615 -3.1048195 1.5157577\n",
      "Iter: 605 Gen Loss: 172.33734 Recon Loss: 172.48114 Gen ADV Loss: 0.873687 Dis Loss: 0.5708427 |||| 0.98028564 -2.101376 2.0054872\n",
      "Iter: 606 Gen Loss: 174.74991 Recon Loss: 174.89606 Gen ADV Loss: 0.8958622 Dis Loss: 0.5428101 |||| 0.8196477 -2.0981803 1.1022868\n",
      "Iter: 607 Gen Loss: 166.05025 Recon Loss: 166.1873 Gen ADV Loss: 1.2545422 Dis Loss: 0.5281988 |||| 0.43540367 -3.1261575 0.9567727\n",
      "Iter: 608 Gen Loss: 163.53885 Recon Loss: 163.67342 Gen ADV Loss: 1.1534718 Dis Loss: 0.56211275 |||| 0.43747258 -2.5444946 1.1958867\n",
      "Iter: 609 Gen Loss: 171.1624 Recon Loss: 171.30472 Gen ADV Loss: 0.98359954 Dis Loss: 0.5421759 |||| 0.6703327 -2.3352053 1.5973747\n",
      "Iter: 610 Gen Loss: 177.91025 Recon Loss: 178.05917 Gen ADV Loss: 1.0815628 Dis Loss: 0.48815635 |||| 0.79914635 -2.457285 0.98947376\n",
      "Iter: 611 Gen Loss: 167.38097 Recon Loss: 167.51926 Gen ADV Loss: 1.1440091 Dis Loss: 0.48590788 |||| 0.73508674 -2.6865647 0.89467\n",
      "Iter: 612 Gen Loss: 176.76614 Recon Loss: 176.9138 Gen ADV Loss: 1.1387814 Dis Loss: 0.4683177 |||| 1.0295199 -2.2744296 1.0579541\n",
      "Iter: 613 Gen Loss: 159.68285 Recon Loss: 159.81308 Gen ADV Loss: 1.4043149 Dis Loss: 0.44728696 |||| 0.5419158 -4.1356735 0.46005133\n",
      "Iter: 614 Gen Loss: 190.10245 Recon Loss: 190.26315 Gen ADV Loss: 1.3625057 Dis Loss: 0.40732342 |||| 1.1916306 -2.8640013 1.3000436\n",
      "Iter: 615 Gen Loss: 178.80823 Recon Loss: 178.95776 Gen ADV Loss: 1.1743493 Dis Loss: 0.38963217 |||| 1.4547334 -3.2299979 0.7727346\n",
      "Iter: 616 Gen Loss: 174.3952 Recon Loss: 174.53995 Gen ADV Loss: 1.506781 Dis Loss: 0.381227 |||| 1.1795331 -3.3883345 0.6384527\n",
      "Iter: 617 Gen Loss: 174.1538 Recon Loss: 174.29802 Gen ADV Loss: 1.7185599 Dis Loss: 0.33568314 |||| 1.2742299 -4.135331 0.3541408\n",
      "Iter: 618 Gen Loss: 179.6041 Recon Loss: 179.75378 Gen ADV Loss: 1.6715091 Dis Loss: 0.352289 |||| 1.5774963 -3.34749 0.9441146\n",
      "Iter: 619 Gen Loss: 174.02104 Recon Loss: 174.1651 Gen ADV Loss: 1.7132516 Dis Loss: 0.2837509 |||| 1.5991749 -3.223294 1.1238128\n",
      "Iter: 620 Gen Loss: 172.74385 Recon Loss: 172.88649 Gen ADV Loss: 1.7807807 Dis Loss: 0.24435541 |||| 1.8927093 -4.17302 0.86635464\n",
      "Iter: 621 Gen Loss: 177.03212 Recon Loss: 177.17874 Gen ADV Loss: 2.0528905 Dis Loss: 0.25379425 |||| 2.0553777 -3.1345515 0.8962558\n",
      "Iter: 622 Gen Loss: 169.2219 Recon Loss: 169.36096 Gen ADV Loss: 1.7805274 Dis Loss: 0.34870654 |||| 1.3998578 -4.634371 1.4984677\n",
      "Iter: 623 Gen Loss: 170.15207 Recon Loss: 170.29248 Gen ADV Loss: 1.3358463 Dis Loss: 0.3341604 |||| 2.4730453 -4.616502 2.2158391\n",
      "Iter: 624 Gen Loss: 166.68431 Recon Loss: 166.81995 Gen ADV Loss: 2.591434 Dis Loss: 0.32430854 |||| 1.1666023 -8.001837 0.18882431\n",
      "Iter: 625 Gen Loss: 165.80284 Recon Loss: 165.93793 Gen ADV Loss: 2.2058222 Dis Loss: 0.25589162 |||| 2.2056997 -5.593641 0.6270661\n",
      "Iter: 626 Gen Loss: 175.31934 Recon Loss: 175.46393 Gen ADV Loss: 2.1970398 Dis Loss: 0.23988704 |||| 2.1505013 -4.9574113 1.1360669\n",
      "Iter: 627 Gen Loss: 174.36232 Recon Loss: 174.50562 Gen ADV Loss: 2.438819 Dis Loss: 0.17820911 |||| 2.769982 -5.525284 0.7218698\n",
      "Iter: 628 Gen Loss: 172.32973 Recon Loss: 172.47087 Gen ADV Loss: 2.5498178 Dis Loss: 0.19266994 |||| 2.7978327 -4.984267 2.024543\n",
      "Iter: 629 Gen Loss: 172.95387 Recon Loss: 173.09407 Gen ADV Loss: 4.062064 Dis Loss: 0.2243665 |||| 1.6812241 -7.5691967 -0.94612294\n",
      "Iter: 630 Gen Loss: 174.46924 Recon Loss: 174.61374 Gen ADV Loss: 1.2296095 Dis Loss: 0.41092795 |||| 4.103682 -3.8437984 3.9213312\n",
      "Iter: 631 Gen Loss: 172.51514 Recon Loss: 172.65558 Gen ADV Loss: 3.3095245 Dis Loss: 0.3562827 |||| 0.7342521 -6.8294272 -0.80631864\n",
      "Iter: 632 Gen Loss: 171.16893 Recon Loss: 171.30963 Gen ADV Loss: 1.6438434 Dis Loss: 0.2848514 |||| 2.8618233 -5.4059687 2.3095763\n",
      "Iter: 633 Gen Loss: 161.05244 Recon Loss: 161.18152 Gen ADV Loss: 3.0810785 Dis Loss: 0.30296957 |||| 1.7171723 -6.5781274 1.6364597\n",
      "Iter: 634 Gen Loss: 166.94191 Recon Loss: 167.07779 Gen ADV Loss: 2.131343 Dis Loss: 0.18862699 |||| 2.7617931 -6.5282607 1.4737723\n",
      "Iter: 635 Gen Loss: 180.1482 Recon Loss: 180.29594 Gen ADV Loss: 3.4133978 Dis Loss: 0.10467403 |||| 2.9775171 -5.515594 0.853654\n",
      "Iter: 636 Gen Loss: 168.06851 Recon Loss: 168.20447 Gen ADV Loss: 3.0383673 Dis Loss: 0.13651668 |||| 2.8286424 -5.694694 1.1729335\n",
      "Iter: 637 Gen Loss: 164.27518 Recon Loss: 164.40851 Gen ADV Loss: 1.8420498 Dis Loss: 0.307139 |||| 3.501504 -4.367423 3.1626134\n",
      "Iter: 638 Gen Loss: 171.02696 Recon Loss: 171.16487 Gen ADV Loss: 3.9697013 Dis Loss: 0.25538433 |||| 1.4471637 -6.8945727 -1.4584056\n",
      "Iter: 639 Gen Loss: 188.16325 Recon Loss: 188.32033 Gen ADV Loss: 1.9377255 Dis Loss: 0.2381694 |||| 3.600598 -5.098117 1.9624699\n",
      "Iter: 640 Gen Loss: 170.55495 Recon Loss: 170.6928 Gen ADV Loss: 3.495489 Dis Loss: 0.14825387 |||| 2.3044655 -5.819746 -0.3119357\n",
      "Iter: 641 Gen Loss: 171.48654 Recon Loss: 171.6262 Gen ADV Loss: 2.5859144 Dis Loss: 0.14012621 |||| 3.881936 -5.1948075 1.1449673\n",
      "Iter: 642 Gen Loss: 183.95444 Recon Loss: 184.10498 Gen ADV Loss: 4.1186943 Dis Loss: 0.17540728 |||| 2.4585383 -7.8390713 0.9037492\n",
      "Iter: 643 Gen Loss: 166.88132 Recon Loss: 167.01546 Gen ADV Loss: 3.345664 Dis Loss: 0.08066733 |||| 4.1347632 -7.8360133 0.006218455\n",
      "Iter: 644 Gen Loss: 161.49057 Recon Loss: 161.61867 Gen ADV Loss: 3.9460387 Dis Loss: 0.086557984 |||| 3.796417 -7.207717 0.76756954\n",
      "Iter: 645 Gen Loss: 170.73752 Recon Loss: 170.87537 Gen ADV Loss: 3.3991456 Dis Loss: 0.08892084 |||| 4.3550453 -8.67444 2.049347\n",
      "Iter: 646 Gen Loss: 165.54971 Recon Loss: 165.68193 Gen ADV Loss: 3.7655115 Dis Loss: 0.102851145 |||| 3.6437356 -7.890412 0.004249894\n",
      "Iter: 647 Gen Loss: 171.77448 Recon Loss: 171.9119 Gen ADV Loss: 4.7617803 Dis Loss: 0.075042754 |||| 4.006288 -8.156663 -1.3777502\n",
      "Iter: 648 Gen Loss: 169.97714 Recon Loss: 170.1149 Gen ADV Loss: 2.6007006 Dis Loss: 0.104689114 |||| 6.2341084 -6.53838 1.4871389\n",
      "Iter: 649 Gen Loss: 179.3665 Recon Loss: 179.5116 Gen ADV Loss: 4.5935225 Dis Loss: 0.11384823 |||| 3.497698 -9.901026 0.004220717\n",
      "Iter: 650 Gen Loss: 162.45464 Recon Loss: 162.58426 Gen ADV Loss: 3.0895689 Dis Loss: 0.05644859 |||| 4.8910265 -7.3213634 -0.18414699\n",
      "Iter: 651 Gen Loss: 170.27466 Recon Loss: 170.41014 Gen ADV Loss: 4.9932485 Dis Loss: 0.048537962 |||| 4.1005306 -8.762845 -0.7010939\n",
      "Iter: 652 Gen Loss: 161.48221 Recon Loss: 161.61028 Gen ADV Loss: 3.5492468 Dis Loss: 0.039614692 |||| 4.7499185 -8.482615 -0.82711834\n",
      "Iter: 653 Gen Loss: 172.05139 Recon Loss: 172.18927 Gen ADV Loss: 4.264466 Dis Loss: 0.029152619 |||| 4.9391646 -8.743058 -0.8231649\n",
      "Iter: 654 Gen Loss: 161.72127 Recon Loss: 161.84775 Gen ADV Loss: 5.3017235 Dis Loss: 0.051336046 |||| 4.456387 -8.540973 0.44456366\n",
      "Iter: 655 Gen Loss: 167.76686 Recon Loss: 167.90042 Gen ADV Loss: 4.1849103 Dis Loss: 0.030754365 |||| 6.4826727 -8.402638 -0.14403641\n",
      "Iter: 656 Gen Loss: 170.2268 Recon Loss: 170.36057 Gen ADV Loss: 6.409624 Dis Loss: 0.01338193 |||| 5.448039 -10.187574 -1.968573\n",
      "Iter: 657 Gen Loss: 161.2207 Recon Loss: 161.34528 Gen ADV Loss: 6.545251 Dis Loss: 0.028562117 |||| 4.280986 -9.948202 -2.5075905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 658 Gen Loss: 164.19844 Recon Loss: 164.3286 Gen ADV Loss: 3.8928852 Dis Loss: 0.04280514 |||| 6.9691925 -8.397968 1.5849499\n",
      "Iter: 659 Gen Loss: 160.62881 Recon Loss: 160.75354 Gen ADV Loss: 5.688728 Dis Loss: 0.04799755 |||| 4.5969963 -10.769473 -1.7765195\n",
      "Iter: 660 Gen Loss: 171.46017 Recon Loss: 171.5965 Gen ADV Loss: 4.9071217 Dis Loss: 0.045108028 |||| 5.1416235 -11.134275 -0.1868738\n",
      "Iter: 661 Gen Loss: 169.48073 Recon Loss: 169.6156 Gen ADV Loss: 4.3301916 Dis Loss: 0.020879928 |||| 6.564881 -8.42785 -0.9221909\n",
      "Iter: 662 Gen Loss: 168.39473 Recon Loss: 168.52585 Gen ADV Loss: 6.934535 Dis Loss: 0.008865431 |||| 6.40349 -12.558677 -1.748385\n",
      "Iter: 663 Gen Loss: 160.5435 Recon Loss: 160.66699 Gen ADV Loss: 6.651164 Dis Loss: 0.015172418 |||| 5.4136133 -11.42158 -2.9251115\n",
      "Iter: 664 Gen Loss: 168.08957 Recon Loss: 168.22171 Gen ADV Loss: 5.4773517 Dis Loss: 0.011436992 |||| 6.5647907 -10.111692 -1.417589\n",
      "Iter: 665 Gen Loss: 174.18481 Recon Loss: 174.32275 Gen ADV Loss: 5.76528 Dis Loss: 0.009770199 |||| 6.7290773 -9.945034 -1.7809503\n",
      "Iter: 666 Gen Loss: 174.60275 Recon Loss: 174.74026 Gen ADV Loss: 6.6129165 Dis Loss: 0.0076440694 |||| 7.3670373 -11.385574 -3.7174041\n",
      "Iter: 667 Gen Loss: 170.74893 Recon Loss: 170.88293 Gen ADV Loss: 6.1790833 Dis Loss: 0.0055084764 |||| 7.3308234 -10.106407 -2.0758333\n",
      "Iter: 668 Gen Loss: 160.95604 Recon Loss: 161.08029 Gen ADV Loss: 6.0472717 Dis Loss: 0.0051740143 |||| 7.0023975 -12.306334 -2.828943\n",
      "Iter: 669 Gen Loss: 173.39783 Recon Loss: 173.53415 Gen ADV Loss: 6.3473735 Dis Loss: 0.0071540205 |||| 8.541945 -9.902612 -1.6732883\n",
      "Iter: 670 Gen Loss: 158.166 Recon Loss: 158.28667 Gen ADV Loss: 6.726622 Dis Loss: 0.009632546 |||| 6.7513914 -10.183789 -1.5507718\n",
      "Iter: 671 Gen Loss: 173.14688 Recon Loss: 173.28293 Gen ADV Loss: 6.2846246 Dis Loss: 0.013246221 |||| 6.0795593 -10.154755 -1.2679144\n",
      "Iter: 672 Gen Loss: 161.36461 Recon Loss: 161.48846 Gen ADV Loss: 6.664348 Dis Loss: 0.016098974 |||| 5.962925 -10.923431 -1.4568874\n",
      "Iter: 673 Gen Loss: 156.96217 Recon Loss: 157.08247 Gen ADV Loss: 5.7637877 Dis Loss: 0.010506548 |||| 8.026535 -8.917678 -0.2772775\n",
      "Iter: 674 Gen Loss: 168.34563 Recon Loss: 168.47736 Gen ADV Loss: 5.6664968 Dis Loss: 0.009982725 |||| 9.004103 -8.889914 -0.80805105\n",
      "Iter: 675 Gen Loss: 161.92175 Recon Loss: 162.04614 Gen ADV Loss: 6.527108 Dis Loss: 0.0073626344 |||| 6.076391 -11.222706 -3.103566\n",
      "Iter: 676 Gen Loss: 167.61678 Recon Loss: 167.74622 Gen ADV Loss: 7.125625 Dis Loss: 0.004817964 |||| 7.2923217 -10.208819 -2.3884847\n",
      "Iter: 677 Gen Loss: 168.06071 Recon Loss: 168.1908 Gen ADV Loss: 6.91534 Dis Loss: 0.005876434 |||| 6.7688932 -11.48785 -3.3023186\n",
      "Iter: 678 Gen Loss: 169.43655 Recon Loss: 169.56792 Gen ADV Loss: 6.977843 Dis Loss: 0.0037711214 |||| 7.4208264 -11.54546 -2.2361937\n",
      "Iter: 679 Gen Loss: 156.6368 Recon Loss: 156.75478 Gen ADV Loss: 7.4945326 Dis Loss: 0.0040921187 |||| 7.632333 -10.050522 -2.971499\n",
      "Iter: 680 Gen Loss: 164.9122 Recon Loss: 165.0392 Gen ADV Loss: 6.7108364 Dis Loss: 0.003697646 |||| 7.890561 -10.65266 -2.821313\n",
      "Iter: 681 Gen Loss: 161.69403 Recon Loss: 161.81758 Gen ADV Loss: 6.9186907 Dis Loss: 0.005362484 |||| 6.829148 -11.292242 -3.9482775\n",
      "Iter: 682 Gen Loss: 166.71419 Recon Loss: 166.84262 Gen ADV Loss: 7.0103283 Dis Loss: 0.0024582292 |||| 7.8166127 -10.277808 -3.5299263\n",
      "Iter: 683 Gen Loss: 157.15437 Recon Loss: 157.27322 Gen ADV Loss: 6.9268775 Dis Loss: 0.005851 |||| 7.540471 -10.78617 -1.5311141\n",
      "Iter: 684 Gen Loss: 155.78838 Recon Loss: 155.90594 Gen ADV Loss: 6.790917 Dis Loss: 0.0018046345 |||| 8.459212 -10.238729 -3.1030464\n",
      "Iter: 685 Gen Loss: 156.35234 Recon Loss: 156.4699 Gen ADV Loss: 7.3320484 Dis Loss: 0.0046617147 |||| 7.779626 -10.763669 -2.0221133\n",
      "Iter: 686 Gen Loss: 158.68126 Recon Loss: 158.80136 Gen ADV Loss: 7.0635962 Dis Loss: 0.008737657 |||| 7.0620227 -10.113549 -0.21433266\n",
      "Iter: 687 Gen Loss: 161.93828 Recon Loss: 162.0616 Gen ADV Loss: 7.0720387 Dis Loss: 0.0025675532 |||| 7.825077 -11.315173 -3.632766\n",
      "Iter: 688 Gen Loss: 165.47041 Recon Loss: 165.59692 Gen ADV Loss: 7.3820715 Dis Loss: 0.0024167157 |||| 7.495202 -11.841957 -4.0362573\n",
      "Iter: 689 Gen Loss: 165.31447 Recon Loss: 165.44078 Gen ADV Loss: 7.377091 Dis Loss: 0.006721811 |||| 7.1089582 -11.523891 -3.888419\n",
      "Iter: 690 Gen Loss: 176.46552 Recon Loss: 176.60315 Gen ADV Loss: 7.148077 Dis Loss: 0.0027950003 |||| 8.910894 -9.760026 -3.2575343\n",
      "Iter: 691 Gen Loss: 168.6856 Recon Loss: 168.81578 Gen ADV Loss: 6.722662 Dis Loss: 0.004498867 |||| 6.7332134 -11.0264225 -2.551908\n",
      "Iter: 692 Gen Loss: 164.88591 Recon Loss: 165.01233 Gen ADV Loss: 6.6181602 Dis Loss: 0.004001339 |||| 8.786214 -10.845919 -1.9158566\n",
      "Iter: 693 Gen Loss: 171.29964 Recon Loss: 171.43188 Gen ADV Loss: 7.153165 Dis Loss: 0.0020203397 |||| 8.916821 -11.757873 -3.1959338\n",
      "Iter: 694 Gen Loss: 159.42361 Recon Loss: 159.54372 Gen ADV Loss: 7.407298 Dis Loss: 0.003789681 |||| 8.553233 -9.89463 -2.9819171\n",
      "Iter: 695 Gen Loss: 158.01881 Recon Loss: 158.13814 Gen ADV Loss: 6.724611 Dis Loss: 0.0025001413 |||| 7.9042253 -10.885148 -3.7893465\n",
      "Iter: 696 Gen Loss: 171.19646 Recon Loss: 171.32799 Gen ADV Loss: 7.659912 Dis Loss: 0.0024198794 |||| 7.4827785 -12.749742 -3.940802\n",
      "Iter: 697 Gen Loss: 152.28427 Recon Loss: 152.3961 Gen ADV Loss: 8.362773 Dis Loss: 0.0036752871 |||| 7.0351515 -13.208839 -5.222684\n",
      "Iter: 698 Gen Loss: 177.9407 Recon Loss: 178.07889 Gen ADV Loss: 7.6180897 Dis Loss: 0.0029228483 |||| 8.605244 -11.240864 -3.0574036\n",
      "Iter: 699 Gen Loss: 169.57338 Recon Loss: 169.7037 Gen ADV Loss: 7.050422 Dis Loss: 0.0029384708 |||| 8.6666355 -10.655316 -3.367609\n",
      "========================================================================\n",
      "8.744172 -0.91349536\n",
      "7.9746857 -0.6281488\n",
      "8.007616 -0.8354149\n",
      "9.30499 -0.7506971\n",
      "7.523219 -0.5628265\n",
      "5.7640176 -0.4945365\n",
      "8.000673 0.0\n",
      "10.130768 0.0\n",
      "17.827803 0.0\n",
      "32.038963 0.0\n",
      "2.0894158 -1.9526664\n",
      "0.9698293 -0.9605262\n",
      "155.86938 0.0023200244\n",
      "=========================================================================\n",
      "Iter: 700 Gen Loss: 175.90698 Recon Loss: 176.04407 Gen ADV Loss: 6.5809984 Dis Loss: 0.0025790692 |||| 7.601273 -13.673776 -4.075364\n",
      "Iter: 701 Gen Loss: 160.52394 Recon Loss: 160.64449 Gen ADV Loss: 7.741041 Dis Loss: 0.001657122 |||| 8.473844 -10.850263 -3.8443847\n",
      "Iter: 702 Gen Loss: 161.09145 Recon Loss: 161.21275 Gen ADV Loss: 7.4749937 Dis Loss: 0.0025983907 |||| 8.815125 -10.276441 -1.6160927\n",
      "Iter: 703 Gen Loss: 162.04103 Recon Loss: 162.16348 Gen ADV Loss: 7.214537 Dis Loss: 0.0024327019 |||| 8.177945 -11.7582035 -3.12793\n",
      "Iter: 704 Gen Loss: 161.85333 Recon Loss: 161.97556 Gen ADV Loss: 7.2104163 Dis Loss: 0.0025858055 |||| 8.309056 -10.641313 -2.9098744\n",
      "Iter: 705 Gen Loss: 167.27827 Recon Loss: 167.40579 Gen ADV Loss: 7.303761 Dis Loss: 0.004092184 |||| 7.0732007 -13.075708 -3.5175974\n",
      "Iter: 706 Gen Loss: 162.93054 Recon Loss: 163.05309 Gen ADV Loss: 7.8950996 Dis Loss: 0.0035347599 |||| 8.730343 -11.652236 -2.8888476\n",
      "Iter: 707 Gen Loss: 163.6954 Recon Loss: 163.81943 Gen ADV Loss: 7.1328626 Dis Loss: 0.0034951344 |||| 8.975129 -9.544752 -2.9894438\n",
      "Iter: 708 Gen Loss: 169.13148 Recon Loss: 169.26154 Gen ADV Loss: 6.4836617 Dis Loss: 0.0036982403 |||| 7.2842035 -12.8708315 -4.398269\n",
      "Iter: 709 Gen Loss: 161.85182 Recon Loss: 161.97319 Gen ADV Loss: 7.835231 Dis Loss: 0.0025672344 |||| 7.7136083 -15.143664 -2.9719224\n",
      "Iter: 710 Gen Loss: 171.4963 Recon Loss: 171.62646 Gen ADV Loss: 8.655573 Dis Loss: 0.0009729372 |||| 8.865099 -12.276965 -3.495877\n",
      "Iter: 711 Gen Loss: 162.14233 Recon Loss: 162.26366 Gen ADV Loss: 8.115323 Dis Loss: 0.0023312503 |||| 8.521009 -12.082278 -2.8898647\n",
      "Iter: 712 Gen Loss: 159.18047 Recon Loss: 159.29922 Gen ADV Loss: 7.645305 Dis Loss: 0.003251521 |||| 8.389194 -11.928046 -3.452396\n",
      "Iter: 713 Gen Loss: 161.73341 Recon Loss: 161.85516 Gen ADV Loss: 7.140912 Dis Loss: 0.0013341769 |||| 8.656703 -11.560074 -3.8272467\n",
      "Iter: 714 Gen Loss: 166.53152 Recon Loss: 166.65678 Gen ADV Loss: 8.390737 Dis Loss: 0.0019869185 |||| 9.223386 -12.473806 -3.7250664\n",
      "Iter: 715 Gen Loss: 172.05968 Recon Loss: 172.19139 Gen ADV Loss: 7.4303784 Dis Loss: 0.0013662017 |||| 8.839831 -12.999057 -3.6604095\n",
      "Iter: 716 Gen Loss: 162.30588 Recon Loss: 162.4272 Gen ADV Loss: 7.9887915 Dis Loss: 0.0014141238 |||| 8.105023 -13.696137 -3.4547913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 717 Gen Loss: 177.55273 Recon Loss: 177.68907 Gen ADV Loss: 8.219841 Dis Loss: 0.00081978104 |||| 9.472577 -11.7978115 -4.1277385\n",
      "Iter: 718 Gen Loss: 171.52374 Recon Loss: 171.65448 Gen ADV Loss: 7.7420015 Dis Loss: 0.0018256342 |||| 8.501324 -11.562577 -4.0428476\n",
      "Iter: 719 Gen Loss: 156.5263 Recon Loss: 156.64217 Gen ADV Loss: 7.5130177 Dis Loss: 0.0030937009 |||| 7.269173 -10.997869 -3.0138907\n",
      "Iter: 720 Gen Loss: 151.9284 Recon Loss: 152.03932 Gen ADV Loss: 7.8170805 Dis Loss: 0.0027206691 |||| 7.7458196 -11.583913 -5.6202407\n",
      "Iter: 721 Gen Loss: 157.31827 Recon Loss: 157.43442 Gen ADV Loss: 7.94099 Dis Loss: 0.0010907225 |||| 8.751339 -11.890986 -4.4499655\n",
      "Iter: 722 Gen Loss: 158.43712 Recon Loss: 158.55408 Gen ADV Loss: 8.20867 Dis Loss: 0.0011056791 |||| 9.971191 -12.258357 -4.598885\n",
      "Iter: 723 Gen Loss: 161.5803 Recon Loss: 161.70119 Gen ADV Loss: 7.3397474 Dis Loss: 0.005867944 |||| 10.154538 -10.404816 -1.229601\n",
      "Iter: 724 Gen Loss: 165.12016 Recon Loss: 165.24452 Gen ADV Loss: 7.334747 Dis Loss: 0.0035461897 |||| 7.651079 -11.355829 -3.777355\n",
      "Iter: 725 Gen Loss: 156.14494 Recon Loss: 156.26015 Gen ADV Loss: 7.4802623 Dis Loss: 0.0020413822 |||| 8.112262 -13.501422 -5.416405\n",
      "Iter: 726 Gen Loss: 164.27518 Recon Loss: 164.3971 Gen ADV Loss: 8.852769 Dis Loss: 0.004368684 |||| 7.5970716 -12.532295 -4.6644382\n",
      "Iter: 727 Gen Loss: 158.98036 Recon Loss: 159.09799 Gen ADV Loss: 7.830649 Dis Loss: 0.0013612746 |||| 8.3759365 -11.186134 -3.7974696\n",
      "Iter: 728 Gen Loss: 158.64095 Recon Loss: 158.75856 Gen ADV Loss: 7.4686184 Dis Loss: 0.003176821 |||| 8.31979 -11.162245 -2.4453604\n",
      "Iter: 729 Gen Loss: 157.18123 Recon Loss: 157.29715 Gen ADV Loss: 7.6495986 Dis Loss: 0.0017918275 |||| 8.293013 -12.10451 -4.693449\n",
      "Iter: 730 Gen Loss: 153.51353 Recon Loss: 153.62497 Gen ADV Loss: 8.443856 Dis Loss: 0.00080809765 |||| 9.239539 -11.692887 -5.6005673\n",
      "Iter: 731 Gen Loss: 162.46059 Recon Loss: 162.58118 Gen ADV Loss: 8.215667 Dis Loss: 0.0018228956 |||| 9.390122 -10.60099 -3.5341728\n",
      "Iter: 732 Gen Loss: 158.77945 Recon Loss: 158.8975 Gen ADV Loss: 6.999996 Dis Loss: 0.0012911999 |||| 9.292757 -11.027035 -3.6726882\n",
      "Iter: 733 Gen Loss: 164.877 Recon Loss: 165.0002 Gen ADV Loss: 7.9010143 Dis Loss: 0.0010911146 |||| 8.552737 -12.337439 -5.683589\n",
      "Iter: 734 Gen Loss: 161.84683 Recon Loss: 161.96628 Gen ADV Loss: 8.547315 Dis Loss: 0.00078363135 |||| 9.163587 -12.961549 -4.534726\n",
      "Iter: 735 Gen Loss: 155.98813 Recon Loss: 156.10153 Gen ADV Loss: 8.712412 Dis Loss: 0.0011879632 |||| 8.44139 -12.250476 -4.123344\n",
      "Iter: 736 Gen Loss: 153.29153 Recon Loss: 153.40256 Gen ADV Loss: 8.375812 Dis Loss: 0.0010800321 |||| 8.755063 -11.581645 -5.0025163\n",
      "Iter: 737 Gen Loss: 153.55626 Recon Loss: 153.66776 Gen ADV Loss: 8.124638 Dis Loss: 0.0017196292 |||| 8.137087 -13.029789 -3.9900086\n",
      "Iter: 738 Gen Loss: 155.63933 Recon Loss: 155.75287 Gen ADV Loss: 8.088901 Dis Loss: 0.0009727541 |||| 9.025274 -11.517742 -4.0882635\n",
      "Iter: 739 Gen Loss: 152.74388 Recon Loss: 152.85437 Gen ADV Loss: 8.197509 Dis Loss: 0.001061828 |||| 9.303693 -11.241627 -4.185462\n",
      "Iter: 740 Gen Loss: 154.74211 Recon Loss: 154.85509 Gen ADV Loss: 7.6903505 Dis Loss: 0.0021268206 |||| 9.478369 -11.635046 -2.7846863\n",
      "Iter: 741 Gen Loss: 152.5937 Recon Loss: 152.70457 Gen ADV Loss: 7.5823936 Dis Loss: 0.0011851985 |||| 9.271767 -12.343454 -4.607447\n",
      "Iter: 742 Gen Loss: 165.90271 Recon Loss: 166.02675 Gen ADV Loss: 7.711576 Dis Loss: 0.0027962725 |||| 7.248878 -12.807098 -3.5030284\n",
      "Iter: 743 Gen Loss: 160.13116 Recon Loss: 160.24911 Gen ADV Loss: 7.9433813 Dis Loss: 0.0011484929 |||| 8.388443 -12.348616 -3.8174675\n",
      "Iter: 744 Gen Loss: 170.0202 Recon Loss: 170.14777 Gen ADV Loss: 8.224402 Dis Loss: 0.0005618194 |||| 9.680383 -12.679149 -5.1365395\n",
      "Iter: 745 Gen Loss: 159.19589 Recon Loss: 159.31223 Gen ADV Loss: 8.576992 Dis Loss: 0.0025235598 |||| 9.834024 -12.041369 -2.1446238\n",
      "Iter: 746 Gen Loss: 161.83736 Recon Loss: 161.9579 Gen ADV Loss: 6.9373927 Dis Loss: 0.002065164 |||| 10.333121 -10.960424 -3.1986945\n",
      "Iter: 747 Gen Loss: 161.97371 Recon Loss: 162.09358 Gen ADV Loss: 7.6877527 Dis Loss: 0.0015332967 |||| 8.066297 -11.588237 -4.936405\n",
      "Iter: 748 Gen Loss: 162.52095 Recon Loss: 162.63995 Gen ADV Loss: 9.063395 Dis Loss: 0.0007053283 |||| 8.824174 -13.434064 -5.9508624\n",
      "Iter: 749 Gen Loss: 151.56693 Recon Loss: 151.67471 Gen ADV Loss: 9.256853 Dis Loss: 0.0022087628 |||| 7.1722026 -13.407778 -6.070989\n",
      "Iter: 750 Gen Loss: 156.82632 Recon Loss: 156.93985 Gen ADV Loss: 8.737367 Dis Loss: 0.0013142305 |||| 8.687698 -12.816615 -3.7991817\n",
      "Iter: 751 Gen Loss: 153.93387 Recon Loss: 154.04526 Gen ADV Loss: 7.916946 Dis Loss: 0.0010925449 |||| 8.317763 -12.145799 -5.225247\n",
      "Iter: 752 Gen Loss: 155.87964 Recon Loss: 155.9926 Gen ADV Loss: 8.261143 Dis Loss: 0.00097405823 |||| 9.006745 -13.004776 -4.600786\n",
      "Iter: 753 Gen Loss: 155.19884 Recon Loss: 155.3109 Gen ADV Loss: 8.412948 Dis Loss: 0.0007579816 |||| 9.861942 -11.960556 -4.949242\n",
      "Iter: 754 Gen Loss: 149.0319 Recon Loss: 149.13799 Gen ADV Loss: 8.178683 Dis Loss: 0.0012076662 |||| 8.891655 -11.360821 -4.5945244\n",
      "Iter: 755 Gen Loss: 157.77385 Recon Loss: 157.88878 Gen ADV Loss: 8.062324 Dis Loss: 0.0011952333 |||| 9.45353 -12.150995 -4.4993854\n",
      "Iter: 756 Gen Loss: 157.77127 Recon Loss: 157.88582 Gen ADV Loss: 8.376858 Dis Loss: 0.0007205636 |||| 10.2330265 -12.449612 -3.9504848\n",
      "Iter: 757 Gen Loss: 152.14244 Recon Loss: 152.25148 Gen ADV Loss: 8.225314 Dis Loss: 0.0007018321 |||| 10.2580185 -12.067456 -5.4151883\n",
      "Iter: 758 Gen Loss: 162.73097 Recon Loss: 162.85121 Gen ADV Loss: 7.5688386 Dis Loss: 0.00076374965 |||| 9.869483 -10.331921 -4.2631435\n",
      "Iter: 759 Gen Loss: 152.34961 Recon Loss: 152.45851 Gen ADV Loss: 8.475175 Dis Loss: 0.0013846181 |||| 8.278148 -11.450159 -5.0020113\n",
      "Iter: 760 Gen Loss: 161.86967 Recon Loss: 161.98808 Gen ADV Loss: 8.459277 Dis Loss: 0.0004107232 |||| 10.023221 -13.895476 -4.9140058\n",
      "Iter: 761 Gen Loss: 164.50565 Recon Loss: 164.62701 Gen ADV Loss: 8.089925 Dis Loss: 0.0013569294 |||| 9.9302225 -11.167716 -3.5499082\n",
      "Iter: 762 Gen Loss: 160.85345 Recon Loss: 160.9712 Gen ADV Loss: 8.008819 Dis Loss: 0.0007081395 |||| 9.01296 -12.385603 -5.4010167\n",
      "Iter: 763 Gen Loss: 165.6901 Recon Loss: 165.81215 Gen ADV Loss: 8.499328 Dis Loss: 0.00074016897 |||| 10.2409525 -12.49667 -4.407262\n",
      "Iter: 764 Gen Loss: 159.22519 Recon Loss: 159.34055 Gen ADV Loss: 8.67184 Dis Loss: 0.00055930065 |||| 9.630643 -11.286561 -5.2501917\n",
      "Iter: 765 Gen Loss: 162.49876 Recon Loss: 162.61726 Gen ADV Loss: 8.751221 Dis Loss: 0.0011301963 |||| 9.0237875 -13.019988 -4.139197\n",
      "Iter: 766 Gen Loss: 149.84044 Recon Loss: 149.9462 Gen ADV Loss: 8.7828665 Dis Loss: 0.0014523967 |||| 9.299966 -12.71027 -2.8844934\n",
      "Iter: 767 Gen Loss: 159.13364 Recon Loss: 159.25003 Gen ADV Loss: 7.3804297 Dis Loss: 0.0022100667 |||| 7.9023414 -12.052678 -5.3859587\n",
      "Iter: 768 Gen Loss: 165.2548 Recon Loss: 165.3759 Gen ADV Loss: 8.765526 Dis Loss: 0.0004404585 |||| 10.790509 -11.757438 -4.869889\n",
      "Iter: 769 Gen Loss: 162.7881 Recon Loss: 162.90695 Gen ADV Loss: 8.507203 Dis Loss: 0.0005908321 |||| 10.377494 -14.444383 -4.5203233\n",
      "Iter: 770 Gen Loss: 167.23726 Recon Loss: 167.36053 Gen ADV Loss: 8.512539 Dis Loss: 0.00083053607 |||| 9.966854 -13.0409155 -4.117578\n",
      "Iter: 771 Gen Loss: 161.2902 Recon Loss: 161.40706 Gen ADV Loss: 8.905304 Dis Loss: 0.00066209363 |||| 9.732179 -12.333179 -4.575949\n",
      "Iter: 772 Gen Loss: 157.05243 Recon Loss: 157.16573 Gen ADV Loss: 8.160185 Dis Loss: 0.0010716906 |||| 10.034126 -11.454528 -4.115569\n",
      "Iter: 773 Gen Loss: 165.4195 Recon Loss: 165.54181 Gen ADV Loss: 7.4757886 Dis Loss: 0.0014663406 |||| 10.155429 -12.495177 -3.3132243\n",
      "Iter: 774 Gen Loss: 154.56844 Recon Loss: 154.67915 Gen ADV Loss: 8.192249 Dis Loss: 0.0009397053 |||| 10.078953 -11.704808 -3.708789\n",
      "Iter: 775 Gen Loss: 168.24046 Recon Loss: 168.36458 Gen ADV Loss: 8.44386 Dis Loss: 0.0004192534 |||| 9.4662895 -14.229247 -6.11432\n",
      "Iter: 776 Gen Loss: 161.06703 Recon Loss: 161.18356 Gen ADV Loss: 8.768806 Dis Loss: 0.0016591132 |||| 9.256177 -13.27066 -3.779394\n",
      "Iter: 777 Gen Loss: 152.81795 Recon Loss: 152.92603 Gen ADV Loss: 8.915233 Dis Loss: 0.00060073606 |||| 8.737976 -13.287318 -4.531147\n",
      "Iter: 778 Gen Loss: 161.06674 Recon Loss: 161.18253 Gen ADV Loss: 9.388819 Dis Loss: 0.0004072871 |||| 9.419265 -14.756634 -5.9336114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 779 Gen Loss: 157.77888 Recon Loss: 157.89163 Gen ADV Loss: 9.113972 Dis Loss: 0.00051577465 |||| 9.757369 -11.880541 -5.1783047\n",
      "Iter: 780 Gen Loss: 163.53073 Recon Loss: 163.64934 Gen ADV Loss: 8.965915 Dis Loss: 0.00047559236 |||| 10.213089 -12.094158 -4.853883\n",
      "Iter: 781 Gen Loss: 159.9774 Recon Loss: 160.09293 Gen ADV Loss: 8.474227 Dis Loss: 0.00047745998 |||| 9.548973 -11.462203 -5.919619\n",
      "Iter: 782 Gen Loss: 165.89835 Recon Loss: 166.01971 Gen ADV Loss: 8.483645 Dis Loss: 0.000385904 |||| 10.438581 -11.806303 -6.1455674\n",
      "Iter: 783 Gen Loss: 151.14627 Recon Loss: 151.25235 Gen ADV Loss: 8.971995 Dis Loss: 0.00072243216 |||| 8.93578 -12.436778 -4.613691\n",
      "Iter: 784 Gen Loss: 167.40619 Recon Loss: 167.52826 Gen ADV Loss: 9.206635 Dis Loss: 0.0007708481 |||| 9.742966 -12.254427 -4.096668\n",
      "Iter: 785 Gen Loss: 158.82137 Recon Loss: 158.93588 Gen ADV Loss: 8.136839 Dis Loss: 0.0009590533 |||| 9.640795 -11.815439 -4.785258\n",
      "Iter: 786 Gen Loss: 153.40685 Recon Loss: 153.51599 Gen ADV Loss: 8.026531 Dis Loss: 0.00066438474 |||| 8.91004 -13.697498 -5.602109\n",
      "Iter: 787 Gen Loss: 148.01514 Recon Loss: 148.11815 Gen ADV Loss: 8.691646 Dis Loss: 0.0009729697 |||| 8.298858 -12.795308 -5.7004004\n",
      "Iter: 788 Gen Loss: 154.65498 Recon Loss: 154.76402 Gen ADV Loss: 9.291517 Dis Loss: 0.0003481975 |||| 10.4276705 -13.043818 -5.335335\n",
      "Iter: 789 Gen Loss: 157.79211 Recon Loss: 157.90427 Gen ADV Loss: 9.27866 Dis Loss: 0.00031528767 |||| 10.372004 -13.106005 -5.7683635\n",
      "Iter: 790 Gen Loss: 149.42133 Recon Loss: 149.5251 Gen ADV Loss: 9.2283535 Dis Loss: 0.00078748615 |||| 9.101895 -13.040607 -4.9045415\n",
      "Iter: 791 Gen Loss: 154.9413 Recon Loss: 155.05075 Gen ADV Loss: 9.007918 Dis Loss: 0.0005449808 |||| 10.153394 -14.807905 -4.961389\n",
      "Iter: 792 Gen Loss: 156.08723 Recon Loss: 156.19868 Gen ADV Loss: 8.130732 Dis Loss: 0.0008792052 |||| 10.820295 -12.304017 -3.3945813\n",
      "Iter: 793 Gen Loss: 158.6794 Recon Loss: 158.79291 Gen ADV Loss: 8.622856 Dis Loss: 0.00069081684 |||| 9.673112 -13.866792 -5.1041727\n",
      "Iter: 794 Gen Loss: 162.90657 Recon Loss: 163.0238 Gen ADV Loss: 9.053893 Dis Loss: 0.00032371766 |||| 10.480508 -13.394432 -6.9725857\n",
      "Iter: 795 Gen Loss: 159.13852 Recon Loss: 159.2523 Gen ADV Loss: 8.756386 Dis Loss: 0.00054002594 |||| 10.020764 -12.30677 -5.4282594\n",
      "Iter: 796 Gen Loss: 150.64153 Recon Loss: 150.7463 Gen ADV Loss: 9.185561 Dis Loss: 0.00058586977 |||| 9.746715 -12.576103 -5.4143887\n",
      "Iter: 797 Gen Loss: 148.25423 Recon Loss: 148.357 Gen ADV Loss: 8.775561 Dis Loss: 0.00096033834 |||| 8.570651 -13.899614 -6.3270226\n",
      "Iter: 798 Gen Loss: 156.92004 Recon Loss: 157.03088 Gen ADV Loss: 9.297209 Dis Loss: 0.00037946718 |||| 10.462303 -12.810177 -5.7496443\n",
      "Iter: 799 Gen Loss: 151.46295 Recon Loss: 151.56853 Gen ADV Loss: 9.070407 Dis Loss: 0.0011088015 |||| 9.016941 -11.994233 -5.5223746\n",
      "========================================================================\n",
      "7.9133234 -0.92056817\n",
      "7.9112253 -0.7669442\n",
      "8.361628 -0.8430643\n",
      "7.5466943 -0.7166786\n",
      "6.3603787 -0.5648977\n",
      "6.116151 -0.64435905\n",
      "7.089517 0.0\n",
      "9.270411 0.0\n",
      "15.691854 0.0\n",
      "24.655394 0.0\n",
      "2.2910988 -2.035751\n",
      "0.97974247 -0.96646833\n",
      "145.35538 0.00059102394\n",
      "=========================================================================\n",
      "Iter: 800 Gen Loss: 151.51898 Recon Loss: 151.62563 Gen ADV Loss: 7.9968433 Dis Loss: 0.00045533295 |||| 10.231224 -12.178319 -5.7275095\n",
      "Iter: 801 Gen Loss: 152.05798 Recon Loss: 152.1646 Gen ADV Loss: 8.548835 Dis Loss: 0.0005646669 |||| 11.371882 -11.132692 -4.295605\n",
      "Iter: 802 Gen Loss: 161.19861 Recon Loss: 161.31453 Gen ADV Loss: 8.325548 Dis Loss: 0.0005566666 |||| 11.123308 -12.821477 -3.721336\n",
      "Iter: 803 Gen Loss: 154.40912 Recon Loss: 154.5175 Gen ADV Loss: 9.021098 Dis Loss: 0.0004069798 |||| 11.191868 -12.350235 -5.650683\n",
      "Iter: 804 Gen Loss: 156.06706 Recon Loss: 156.17744 Gen ADV Loss: 8.649081 Dis Loss: 0.0003123507 |||| 10.738069 -12.736755 -5.8995223\n",
      "Iter: 805 Gen Loss: 152.89104 Recon Loss: 152.99774 Gen ADV Loss: 9.137669 Dis Loss: 0.0005016429 |||| 10.237156 -12.264514 -5.694005\n",
      "Iter: 806 Gen Loss: 168.46796 Recon Loss: 168.59053 Gen ADV Loss: 8.808589 Dis Loss: 0.011014184 |||| 11.657427 -12.569746 1.4985733\n",
      "Iter: 807 Gen Loss: 150.24825 Recon Loss: 150.35324 Gen ADV Loss: 8.071688 Dis Loss: 0.009856653 |||| 8.3045845 -11.245187 -2.744613\n",
      "Iter: 808 Gen Loss: 163.46724 Recon Loss: 163.5862 Gen ADV Loss: 7.317228 Dis Loss: 0.012056354 |||| 7.0413837 -11.787059 -2.4069643\n",
      "Iter: 809 Gen Loss: 149.19579 Recon Loss: 149.30148 Gen ADV Loss: 6.2645245 Dis Loss: 0.0045850254 |||| 9.145177 -11.571184 -1.7324716\n",
      "Iter: 810 Gen Loss: 155.38474 Recon Loss: 155.49539 Gen ADV Loss: 7.464415 Dis Loss: 0.006346725 |||| 9.782386 -11.536837 -0.27113044\n",
      "Iter: 811 Gen Loss: 157.759 Recon Loss: 157.87048 Gen ADV Loss: 8.976124 Dis Loss: 0.00136799 |||| 9.210051 -11.201741 -2.4137292\n",
      "Iter: 812 Gen Loss: 152.1743 Recon Loss: 152.27997 Gen ADV Loss: 9.130389 Dis Loss: 0.008783518 |||| 7.182367 -13.246512 -2.4606583\n",
      "Iter: 813 Gen Loss: 154.42686 Recon Loss: 154.5374 Gen ADV Loss: 6.492017 Dis Loss: 0.0053082127 |||| 10.265875 -11.008577 -2.0504596\n",
      "Iter: 814 Gen Loss: 160.31746 Recon Loss: 160.43263 Gen ADV Loss: 7.7075243 Dis Loss: 0.003730075 |||| 7.786908 -12.089109 -5.9600577\n",
      "Iter: 815 Gen Loss: 160.09056 Recon Loss: 160.20526 Gen ADV Loss: 7.8851075 Dis Loss: 0.0009593875 |||| 10.102722 -12.30063 -3.493356\n",
      "Iter: 816 Gen Loss: 158.30002 Recon Loss: 158.41292 Gen ADV Loss: 7.873644 Dis Loss: 0.0016680466 |||| 10.123628 -11.219341 -2.9923449\n",
      "Iter: 817 Gen Loss: 153.86447 Recon Loss: 153.97287 Gen ADV Loss: 7.8576484 Dis Loss: 0.00091971416 |||| 9.622198 -11.70738 -4.313717\n",
      "Iter: 818 Gen Loss: 156.79175 Recon Loss: 156.90208 Gen ADV Loss: 8.787043 Dis Loss: 0.0006040466 |||| 10.115433 -12.772672 -3.9880905\n",
      "Iter: 819 Gen Loss: 144.43494 Recon Loss: 144.53287 Gen ADV Loss: 8.786747 Dis Loss: 0.0016595399 |||| 8.735971 -13.359979 -4.2170367\n",
      "Iter: 820 Gen Loss: 149.6986 Recon Loss: 149.80222 Gen ADV Loss: 8.336559 Dis Loss: 0.00055469247 |||| 9.928023 -11.90456 -5.83937\n",
      "Iter: 821 Gen Loss: 150.60036 Recon Loss: 150.70465 Gen ADV Loss: 8.52265 Dis Loss: 0.001002911 |||| 8.635882 -14.275318 -4.513737\n",
      "Iter: 822 Gen Loss: 154.52206 Recon Loss: 154.6305 Gen ADV Loss: 8.29028 Dis Loss: 0.0007943902 |||| 9.327424 -11.980387 -4.251457\n",
      "Iter: 823 Gen Loss: 153.79175 Recon Loss: 153.89874 Gen ADV Loss: 8.934873 Dis Loss: 0.00061118224 |||| 10.899893 -12.311778 -4.36675\n",
      "Iter: 824 Gen Loss: 150.28996 Recon Loss: 150.39395 Gen ADV Loss: 8.425093 Dis Loss: 0.0007030409 |||| 10.34658 -10.841746 -3.7646298\n",
      "Iter: 825 Gen Loss: 156.43694 Recon Loss: 156.5473 Gen ADV Loss: 8.129742 Dis Loss: 0.00052527926 |||| 11.299595 -12.926545 -4.6107793\n",
      "Iter: 826 Gen Loss: 154.03203 Recon Loss: 154.13959 Gen ADV Loss: 8.511347 Dis Loss: 0.00074272876 |||| 9.7149935 -11.87325 -4.3126144\n",
      "Iter: 827 Gen Loss: 153.08444 Recon Loss: 153.19127 Gen ADV Loss: 8.233667 Dis Loss: 0.00056680397 |||| 10.147332 -11.531652 -5.3634844\n",
      "Iter: 828 Gen Loss: 155.19348 Recon Loss: 155.30214 Gen ADV Loss: 8.472137 Dis Loss: 0.00060999964 |||| 9.472032 -13.705912 -4.71742\n",
      "Iter: 829 Gen Loss: 153.89362 Recon Loss: 154.00084 Gen ADV Loss: 8.564553 Dis Loss: 0.0009375013 |||| 9.565342 -12.327654 -4.2818174\n",
      "Iter: 830 Gen Loss: 156.33235 Recon Loss: 156.44205 Gen ADV Loss: 8.521729 Dis Loss: 0.00059533556 |||| 10.005608 -12.94879 -4.712711\n",
      "Iter: 831 Gen Loss: 168.44032 Recon Loss: 168.56255 Gen ADV Loss: 8.051399 Dis Loss: 0.0007863515 |||| 8.86165 -13.002489 -4.8557463\n",
      "Iter: 832 Gen Loss: 165.14555 Recon Loss: 165.26323 Gen ADV Loss: 9.266889 Dis Loss: 0.000433449 |||| 10.774494 -12.735086 -4.0374637\n",
      "Iter: 833 Gen Loss: 151.44926 Recon Loss: 151.55338 Gen ADV Loss: 9.098545 Dis Loss: 0.0007451982 |||| 9.766144 -12.610928 -3.301916\n",
      "Iter: 834 Gen Loss: 155.6705 Recon Loss: 155.77896 Gen ADV Loss: 8.944385 Dis Loss: 0.000534897 |||| 9.5437765 -13.867179 -5.876537\n",
      "Iter: 835 Gen Loss: 154.12051 Recon Loss: 154.22758 Gen ADV Loss: 8.716781 Dis Loss: 0.00034722988 |||| 10.372638 -12.353805 -4.8731885\n",
      "Iter: 836 Gen Loss: 153.95982 Recon Loss: 154.06729 Gen ADV Loss: 8.116046 Dis Loss: 0.0007335214 |||| 9.787943 -13.416619 -4.899689\n",
      "Iter: 837 Gen Loss: 156.22621 Recon Loss: 156.33502 Gen ADV Loss: 9.010236 Dis Loss: 0.0005278816 |||| 10.163507 -12.409501 -5.3441215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 838 Gen Loss: 158.06003 Recon Loss: 158.17075 Gen ADV Loss: 8.906744 Dis Loss: 0.00054125034 |||| 9.904462 -11.561662 -5.5779305\n",
      "Iter: 839 Gen Loss: 165.5006 Recon Loss: 165.61954 Gen ADV Loss: 8.057606 Dis Loss: 0.00060584897 |||| 10.815905 -12.569544 -4.4537044\n",
      "Iter: 840 Gen Loss: 157.7773 Recon Loss: 157.88782 Gen ADV Loss: 8.69699 Dis Loss: 0.0004722919 |||| 9.079326 -13.633942 -5.977274\n",
      "Iter: 841 Gen Loss: 152.24559 Recon Loss: 152.34993 Gen ADV Loss: 9.353268 Dis Loss: 0.000679184 |||| 9.976416 -13.484373 -3.0329742\n",
      "Iter: 842 Gen Loss: 155.98022 Recon Loss: 156.08823 Gen ADV Loss: 9.371821 Dis Loss: 0.00050689356 |||| 9.6265545 -12.217246 -4.9687514\n",
      "Iter: 843 Gen Loss: 152.29942 Recon Loss: 152.40338 Gen ADV Loss: 9.688313 Dis Loss: 0.00030351 |||| 10.439731 -13.563239 -6.741238\n",
      "Iter: 844 Gen Loss: 152.36137 Recon Loss: 152.46591 Gen ADV Loss: 9.126817 Dis Loss: 0.0005767965 |||| 10.187625 -12.041122 -5.5764256\n",
      "Iter: 845 Gen Loss: 168.50041 Recon Loss: 168.62178 Gen ADV Loss: 8.40697 Dis Loss: 0.00048865407 |||| 10.168307 -12.717223 -4.31837\n",
      "Iter: 846 Gen Loss: 162.80351 Recon Loss: 162.91835 Gen ADV Loss: 9.17594 Dis Loss: 0.00064261217 |||| 9.196282 -14.2787285 -4.4490347\n",
      "Iter: 847 Gen Loss: 164.65967 Recon Loss: 164.77667 Gen ADV Loss: 8.858346 Dis Loss: 0.00041577226 |||| 10.216297 -14.930555 -5.1843843\n",
      "Iter: 848 Gen Loss: 158.08942 Recon Loss: 158.19962 Gen ADV Loss: 9.079293 Dis Loss: 0.00044966227 |||| 10.306593 -11.402073 -4.816902\n",
      "Iter: 849 Gen Loss: 158.79536 Recon Loss: 158.90642 Gen ADV Loss: 8.8672695 Dis Loss: 0.00057502126 |||| 9.265956 -12.839737 -5.8643003\n",
      "Iter: 850 Gen Loss: 156.1208 Recon Loss: 156.22833 Gen ADV Loss: 9.64356 Dis Loss: 0.0002301817 |||| 11.776312 -12.970587 -5.2046614\n",
      "Iter: 851 Gen Loss: 155.18509 Recon Loss: 155.29185 Gen ADV Loss: 9.451783 Dis Loss: 0.00052687025 |||| 9.083066 -12.837547 -6.731024\n",
      "Iter: 852 Gen Loss: 158.34961 Recon Loss: 158.46014 Gen ADV Loss: 8.796216 Dis Loss: 0.00041666016 |||| 10.083723 -13.033062 -5.423801\n",
      "Iter: 853 Gen Loss: 147.37712 Recon Loss: 147.47588 Gen ADV Loss: 9.5279875 Dis Loss: 0.00050014554 |||| 10.80107 -13.63736 -3.8251338\n",
      "Iter: 854 Gen Loss: 151.6495 Recon Loss: 151.75275 Gen ADV Loss: 9.293888 Dis Loss: 0.00036169437 |||| 10.119462 -13.260292 -5.2454605\n",
      "Iter: 855 Gen Loss: 158.99663 Recon Loss: 159.1078 Gen ADV Loss: 8.673828 Dis Loss: 0.0004405197 |||| 9.722025 -12.800578 -5.0316377\n",
      "Iter: 856 Gen Loss: 145.98018 Recon Loss: 146.07748 Gen ADV Loss: 9.4331875 Dis Loss: 0.0005141699 |||| 9.778808 -13.661582 -5.519629\n",
      "Iter: 857 Gen Loss: 148.05664 Recon Loss: 148.15591 Gen ADV Loss: 9.484884 Dis Loss: 0.0009724302 |||| 8.518511 -14.038415 -5.371058\n",
      "Iter: 858 Gen Loss: 156.50471 Recon Loss: 156.61244 Gen ADV Loss: 9.509187 Dis Loss: 0.00022400991 |||| 11.944724 -13.265794 -5.9509335\n",
      "Iter: 859 Gen Loss: 148.84012 Recon Loss: 148.94038 Gen ADV Loss: 9.229619 Dis Loss: 0.00034897242 |||| 10.912001 -13.868965 -5.984051\n",
      "Iter: 860 Gen Loss: 149.043 Recon Loss: 149.14352 Gen ADV Loss: 9.130653 Dis Loss: 0.0006128502 |||| 11.019003 -13.1380825 -3.2974658\n",
      "Iter: 861 Gen Loss: 146.97545 Recon Loss: 147.07484 Gen ADV Loss: 8.150583 Dis Loss: 0.00051714445 |||| 10.615917 -11.667887 -3.9826813\n",
      "Iter: 862 Gen Loss: 150.64172 Recon Loss: 150.74435 Gen ADV Loss: 8.529186 Dis Loss: 0.00032750343 |||| 10.6607 -12.608797 -5.1394\n",
      "Iter: 863 Gen Loss: 158.01741 Recon Loss: 158.12634 Gen ADV Loss: 9.597568 Dis Loss: 0.00034238992 |||| 10.636589 -12.831097 -5.3092484\n",
      "Iter: 864 Gen Loss: 154.6178 Recon Loss: 154.72401 Gen ADV Loss: 8.871648 Dis Loss: 0.0010313129 |||| 10.821193 -11.0396595 -2.7673714\n",
      "Iter: 865 Gen Loss: 156.99966 Recon Loss: 157.10893 Gen ADV Loss: 8.157612 Dis Loss: 0.00031323085 |||| 10.798022 -12.987744 -5.3744097\n",
      "Iter: 866 Gen Loss: 152.91696 Recon Loss: 153.02147 Gen ADV Loss: 8.792157 Dis Loss: 0.00092998915 |||| 9.678978 -11.9438305 -5.6375847\n",
      "Iter: 867 Gen Loss: 160.47737 Recon Loss: 160.5888 Gen ADV Loss: 9.405496 Dis Loss: 0.00038141847 |||| 10.6587925 -12.926405 -6.0279913\n",
      "Iter: 868 Gen Loss: 162.45752 Recon Loss: 162.57118 Gen ADV Loss: 9.171181 Dis Loss: 0.0003506028 |||| 10.324335 -12.366943 -5.551004\n",
      "Iter: 869 Gen Loss: 151.86034 Recon Loss: 151.96332 Gen ADV Loss: 9.17426 Dis Loss: 0.0004984513 |||| 9.997155 -14.057406 -4.5703254\n",
      "Iter: 870 Gen Loss: 161.46315 Recon Loss: 161.57588 Gen ADV Loss: 8.957043 Dis Loss: 0.00052235165 |||| 10.096771 -13.217238 -4.361915\n",
      "Iter: 871 Gen Loss: 158.58315 Recon Loss: 158.69208 Gen ADV Loss: 9.849659 Dis Loss: 0.00044744366 |||| 9.21041 -13.897534 -6.6008234\n",
      "Iter: 872 Gen Loss: 161.02243 Recon Loss: 161.13448 Gen ADV Loss: 9.161652 Dis Loss: 0.00029682738 |||| 10.886255 -11.945168 -5.60931\n",
      "Iter: 873 Gen Loss: 157.34001 Recon Loss: 157.44823 Gen ADV Loss: 9.240779 Dis Loss: 0.00035245524 |||| 10.043383 -12.212324 -6.254945\n",
      "Iter: 874 Gen Loss: 161.07455 Recon Loss: 161.18674 Gen ADV Loss: 8.97977 Dis Loss: 0.0008843295 |||| 11.036015 -10.2383175 -4.186193\n",
      "Iter: 875 Gen Loss: 159.57921 Recon Loss: 159.69087 Gen ADV Loss: 7.964739 Dis Loss: 0.00043055008 |||| 11.047926 -12.47954 -5.7079635\n",
      "Iter: 876 Gen Loss: 159.01686 Recon Loss: 159.12735 Gen ADV Loss: 8.531017 Dis Loss: 0.0002982713 |||| 10.957058 -12.284616 -4.9299374\n",
      "Iter: 877 Gen Loss: 158.15709 Recon Loss: 158.26607 Gen ADV Loss: 9.142245 Dis Loss: 0.0011909155 |||| 9.298729 -12.955841 -6.3464127\n",
      "Iter: 878 Gen Loss: 152.07018 Recon Loss: 152.17305 Gen ADV Loss: 9.082727 Dis Loss: 0.0005194968 |||| 9.726833 -14.658536 -5.662827\n",
      "Iter: 879 Gen Loss: 156.60324 Recon Loss: 156.70993 Gen ADV Loss: 9.778614 Dis Loss: 0.000591474 |||| 9.728855 -13.738454 -6.456534\n",
      "Iter: 880 Gen Loss: 153.75238 Recon Loss: 153.85657 Gen ADV Loss: 9.389348 Dis Loss: 0.00040642353 |||| 10.244913 -14.038688 -5.4751754\n",
      "Iter: 881 Gen Loss: 155.27673 Recon Loss: 155.38235 Gen ADV Loss: 9.458369 Dis Loss: 0.00043908774 |||| 9.448086 -14.48844 -5.683476\n",
      "Iter: 882 Gen Loss: 150.33989 Recon Loss: 150.4402 Gen ADV Loss: 9.770185 Dis Loss: 0.00041992863 |||| 9.849373 -14.964483 -6.273129\n",
      "Iter: 883 Gen Loss: 154.96538 Recon Loss: 155.07033 Gen ADV Loss: 9.746958 Dis Loss: 0.00030842528 |||| 11.526712 -13.966256 -5.4085774\n",
      "Iter: 884 Gen Loss: 151.85902 Recon Loss: 151.96164 Gen ADV Loss: 8.934826 Dis Loss: 0.0004413257 |||| 10.277655 -12.559675 -5.2819605\n",
      "Iter: 885 Gen Loss: 148.8874 Recon Loss: 148.9866 Gen ADV Loss: 9.326977 Dis Loss: 0.00022826435 |||| 10.810922 -13.460314 -6.665372\n",
      "Iter: 886 Gen Loss: 161.21748 Recon Loss: 161.32925 Gen ADV Loss: 9.025861 Dis Loss: 0.0005367072 |||| 10.7554455 -12.464177 -5.332857\n",
      "Iter: 887 Gen Loss: 151.78624 Recon Loss: 151.88863 Gen ADV Loss: 8.932665 Dis Loss: 0.00040061664 |||| 11.545716 -11.714199 -5.4497027\n",
      "Iter: 888 Gen Loss: 159.64099 Recon Loss: 159.75156 Gen ADV Loss: 8.573833 Dis Loss: 0.00033306138 |||| 11.167257 -14.001629 -5.213811\n",
      "Iter: 889 Gen Loss: 150.69897 Recon Loss: 150.79944 Gen ADV Loss: 9.683662 Dis Loss: 0.0002352632 |||| 11.666722 -13.057308 -5.577004\n",
      "Iter: 890 Gen Loss: 145.8697 Recon Loss: 145.96573 Gen ADV Loss: 9.269164 Dis Loss: 0.00030002533 |||| 10.424087 -12.3532 -5.516033\n",
      "Iter: 891 Gen Loss: 152.55351 Recon Loss: 152.65654 Gen ADV Loss: 8.872179 Dis Loss: 0.00034151287 |||| 10.469276 -11.726605 -5.4641423\n",
      "Iter: 892 Gen Loss: 155.79857 Recon Loss: 155.90439 Gen ADV Loss: 9.290496 Dis Loss: 0.00035719926 |||| 11.212375 -12.064935 -4.3604336\n",
      "Iter: 893 Gen Loss: 152.03456 Recon Loss: 152.13638 Gen ADV Loss: 9.499706 Dis Loss: 0.0003435087 |||| 11.243451 -13.792618 -5.170394\n",
      "Iter: 894 Gen Loss: 142.65852 Recon Loss: 142.75133 Gen ADV Loss: 9.096615 Dis Loss: 0.00021034731 |||| 11.388059 -14.075725 -6.762662\n",
      "Iter: 895 Gen Loss: 150.30307 Recon Loss: 150.40265 Gen ADV Loss: 9.9082365 Dis Loss: 0.00028942342 |||| 10.98757 -14.286077 -5.990543\n",
      "Iter: 896 Gen Loss: 156.74437 Recon Loss: 156.85062 Gen ADV Loss: 9.626382 Dis Loss: 0.0005259693 |||| 9.978146 -12.668305 -6.642683\n",
      "Iter: 897 Gen Loss: 149.8178 Recon Loss: 149.91702 Gen ADV Loss: 9.698373 Dis Loss: 0.00039358658 |||| 9.781556 -14.916957 -5.5074286\n",
      "Iter: 898 Gen Loss: 146.2548 Recon Loss: 146.34999 Gen ADV Loss: 10.162023 Dis Loss: 0.00033730114 |||| 10.074316 -14.595979 -5.1334124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 899 Gen Loss: 150.54066 Recon Loss: 150.64056 Gen ADV Loss: 9.710702 Dis Loss: 0.0002869934 |||| 11.019917 -13.624638 -5.044737\n",
      "========================================================================\n",
      "8.991085 -0.9567404\n",
      "7.226412 -0.67630315\n",
      "7.736996 -0.84961814\n",
      "7.662523 -0.67337483\n",
      "5.8641734 -0.56935954\n",
      "5.7251315 -0.5034167\n",
      "7.1842427 0.0\n",
      "10.408506 0.0\n",
      "14.520626 0.0\n",
      "28.040615 0.0\n",
      "2.34277 -2.1880639\n",
      "0.9817133 -0.9751644\n",
      "141.18755 0.0002794982\n",
      "=========================================================================\n",
      "Iter: 900 Gen Loss: 155.43051 Recon Loss: 155.53577 Gen ADV Loss: 9.171854 Dis Loss: 0.00028315344 |||| 10.940809 -12.805637 -5.2566442\n",
      "Iter: 901 Gen Loss: 157.98792 Recon Loss: 158.09521 Gen ADV Loss: 9.596785 Dis Loss: 0.00025975175 |||| 11.263089 -12.803368 -6.885309\n",
      "Iter: 902 Gen Loss: 156.03264 Recon Loss: 156.13754 Gen ADV Loss: 10.005672 Dis Loss: 0.0004640459 |||| 11.120091 -13.531548 -4.209031\n",
      "Iter: 903 Gen Loss: 150.97734 Recon Loss: 151.07869 Gen ADV Loss: 8.488302 Dis Loss: 0.00042098403 |||| 10.807095 -11.582016 -5.1187215\n",
      "Iter: 904 Gen Loss: 153.15033 Recon Loss: 153.25351 Gen ADV Loss: 8.814946 Dis Loss: 0.00035167567 |||| 10.852991 -12.108893 -5.9521523\n",
      "Iter: 905 Gen Loss: 150.95451 Recon Loss: 151.05411 Gen ADV Loss: 10.175118 Dis Loss: 0.00021125205 |||| 10.426018 -14.825988 -6.3949842\n",
      "Iter: 906 Gen Loss: 145.43242 Recon Loss: 145.5265 Gen ADV Loss: 10.109659 Dis Loss: 0.00048481405 |||| 9.727571 -14.372965 -5.974032\n",
      "Iter: 907 Gen Loss: 163.29732 Recon Loss: 163.40971 Gen ADV Loss: 9.661042 Dis Loss: 0.00022909074 |||| 11.079242 -12.866861 -6.5446734\n",
      "Iter: 908 Gen Loss: 155.13512 Recon Loss: 155.23895 Gen ADV Loss: 9.989051 Dis Loss: 0.00024417925 |||| 11.456049 -13.068569 -5.5258994\n",
      "Iter: 909 Gen Loss: 152.14772 Recon Loss: 152.24857 Gen ADV Loss: 9.97015 Dis Loss: 0.00017764221 |||| 11.145592 -13.69931 -6.1479387\n",
      "Iter: 910 Gen Loss: 151.22122 Recon Loss: 151.32161 Gen ADV Loss: 9.454255 Dis Loss: 0.00022213135 |||| 10.92722 -12.985893 -6.050807\n",
      "Iter: 911 Gen Loss: 153.09024 Recon Loss: 153.19188 Gen ADV Loss: 10.058726 Dis Loss: 0.00019037892 |||| 11.859051 -14.7590685 -6.103046\n",
      "Iter: 912 Gen Loss: 149.03383 Recon Loss: 149.1313 Gen ADV Loss: 10.100751 Dis Loss: 0.0003626324 |||| 11.776464 -12.806548 -4.9193387\n",
      "Iter: 913 Gen Loss: 146.85564 Recon Loss: 146.95215 Gen ADV Loss: 8.866978 Dis Loss: 0.00036035766 |||| 11.549795 -13.999394 -4.6688\n",
      "Iter: 914 Gen Loss: 156.13731 Recon Loss: 156.24341 Gen ADV Loss: 8.549995 Dis Loss: 0.00046634203 |||| 9.918969 -13.920535 -5.6123085\n",
      "Iter: 915 Gen Loss: 147.24854 Recon Loss: 147.34421 Gen ADV Loss: 10.018241 Dis Loss: 0.00034671344 |||| 10.600794 -13.331345 -4.307886\n",
      "Iter: 916 Gen Loss: 158.74817 Recon Loss: 158.85559 Gen ADV Loss: 9.78087 Dis Loss: 0.00021517577 |||| 11.804425 -12.524727 -5.548077\n",
      "Iter: 917 Gen Loss: 149.42542 Recon Loss: 149.5235 Gen ADV Loss: 9.734156 Dis Loss: 0.0002907998 |||| 11.366581 -12.0825405 -5.124069\n",
      "Iter: 918 Gen Loss: 157.54851 Recon Loss: 157.65546 Gen ADV Loss: 8.967849 Dis Loss: 0.00023570536 |||| 11.090402 -13.213901 -6.1506505\n",
      "Iter: 919 Gen Loss: 147.07404 Recon Loss: 147.16937 Gen ADV Loss: 10.077275 Dis Loss: 0.00025826483 |||| 11.073424 -12.684909 -5.9050775\n",
      "Iter: 920 Gen Loss: 157.73552 Recon Loss: 157.8421 Gen ADV Loss: 9.450256 Dis Loss: 0.00033906274 |||| 11.4902115 -12.648774 -4.8945293\n",
      "Iter: 921 Gen Loss: 157.88065 Recon Loss: 157.98799 Gen ADV Loss: 8.802874 Dis Loss: 0.00026129963 |||| 11.359502 -12.400213 -6.269664\n",
      "Iter: 922 Gen Loss: 152.15349 Recon Loss: 152.2549 Gen ADV Loss: 8.926904 Dis Loss: 0.00021060262 |||| 11.226459 -12.993586 -5.4112997\n",
      "Iter: 923 Gen Loss: 149.94772 Recon Loss: 150.04565 Gen ADV Loss: 10.133254 Dis Loss: 0.00018021531 |||| 10.744701 -13.886476 -6.8563523\n",
      "Iter: 924 Gen Loss: 146.9122 Recon Loss: 147.00699 Gen ADV Loss: 10.238944 Dis Loss: 0.0002405523 |||| 10.907248 -14.282388 -6.8967004\n",
      "Iter: 925 Gen Loss: 141.41273 Recon Loss: 141.5024 Gen ADV Loss: 9.800454 Dis Loss: 0.00033661583 |||| 10.821725 -12.598407 -5.4287567\n",
      "Iter: 926 Gen Loss: 151.4555 Recon Loss: 151.55576 Gen ADV Loss: 9.214245 Dis Loss: 0.00038499635 |||| 10.718792 -11.922477 -5.0975456\n",
      "Iter: 927 Gen Loss: 154.10835 Recon Loss: 154.21118 Gen ADV Loss: 9.280147 Dis Loss: 0.00040540218 |||| 10.494097 -12.312974 -6.3813214\n",
      "Iter: 928 Gen Loss: 149.58965 Recon Loss: 149.68745 Gen ADV Loss: 9.76878 Dis Loss: 0.00026230144 |||| 11.068599 -13.355794 -5.281491\n",
      "Iter: 929 Gen Loss: 151.50648 Recon Loss: 151.6069 Gen ADV Loss: 9.050768 Dis Loss: 0.00025952465 |||| 11.112151 -12.001785 -5.953469\n",
      "Iter: 930 Gen Loss: 151.24342 Recon Loss: 151.34332 Gen ADV Loss: 9.24857 Dis Loss: 0.00041697256 |||| 10.873579 -13.643545 -4.8553247\n",
      "Iter: 931 Gen Loss: 153.80655 Recon Loss: 153.90942 Gen ADV Loss: 8.76915 Dis Loss: 0.0005594881 |||| 9.625503 -13.976301 -4.4555287\n",
      "Iter: 932 Gen Loss: 146.58965 Recon Loss: 146.68443 Gen ADV Loss: 9.565483 Dis Loss: 0.00036743496 |||| 10.862845 -13.678619 -4.692215\n",
      "Iter: 933 Gen Loss: 148.09267 Recon Loss: 148.1897 Gen ADV Loss: 8.812478 Dis Loss: 0.00028196856 |||| 11.256135 -13.605853 -5.656937\n",
      "Iter: 934 Gen Loss: 146.32677 Recon Loss: 146.4211 Gen ADV Loss: 9.728439 Dis Loss: 0.0003034805 |||| 10.805996 -13.846377 -6.3147836\n",
      "Iter: 935 Gen Loss: 148.09415 Recon Loss: 148.1908 Gen ADV Loss: 9.171099 Dis Loss: 0.00034095265 |||| 12.066424 -12.379074 -4.5302114\n",
      "Iter: 936 Gen Loss: 157.2665 Recon Loss: 157.37254 Gen ADV Loss: 8.910397 Dis Loss: 0.00027806082 |||| 10.558004 -14.298789 -6.1304255\n",
      "Iter: 937 Gen Loss: 147.34973 Recon Loss: 147.44466 Gen ADV Loss: 10.049242 Dis Loss: 0.00032390066 |||| 10.275006 -14.235292 -5.02372\n",
      "Iter: 938 Gen Loss: 149.17865 Recon Loss: 149.27573 Gen ADV Loss: 9.707248 Dis Loss: 0.00029624207 |||| 11.235379 -13.319772 -4.855266\n",
      "Iter: 939 Gen Loss: 152.64429 Recon Loss: 152.7453 Gen ADV Loss: 9.174549 Dis Loss: 0.00034700907 |||| 10.967015 -12.345451 -5.8987656\n",
      "Iter: 940 Gen Loss: 146.6433 Recon Loss: 146.73846 Gen ADV Loss: 8.946567 Dis Loss: 0.0003798884 |||| 10.798903 -11.323719 -5.8498554\n",
      "Iter: 941 Gen Loss: 146.4934 Recon Loss: 146.58807 Gen ADV Loss: 9.255423 Dis Loss: 0.00033446966 |||| 10.741228 -13.884419 -6.1422405\n",
      "Iter: 942 Gen Loss: 149.86328 Recon Loss: 149.96121 Gen ADV Loss: 9.371147 Dis Loss: 0.00042736324 |||| 10.68975 -12.710978 -5.2164426\n",
      "Iter: 943 Gen Loss: 148.20895 Recon Loss: 148.3052 Gen ADV Loss: 9.323641 Dis Loss: 0.00032349554 |||| 11.31115 -14.283102 -5.2717977\n",
      "Iter: 944 Gen Loss: 152.67941 Recon Loss: 152.77975 Gen ADV Loss: 9.685226 Dis Loss: 0.00038577884 |||| 9.980991 -12.721712 -5.6188335\n",
      "Iter: 945 Gen Loss: 150.32893 Recon Loss: 150.427 Gen ADV Loss: 9.535009 Dis Loss: 0.00032827238 |||| 10.440712 -14.68436 -5.54294\n",
      "Iter: 946 Gen Loss: 146.22978 Recon Loss: 146.32297 Gen ADV Loss: 10.296623 Dis Loss: 0.00018857568 |||| 11.005066 -14.542517 -6.731562\n",
      "Iter: 947 Gen Loss: 153.81693 Recon Loss: 153.91785 Gen ADV Loss: 10.1367855 Dis Loss: 0.00019227606 |||| 12.196914 -12.745061 -6.4087043\n",
      "Iter: 948 Gen Loss: 145.72166 Recon Loss: 145.81534 Gen ADV Loss: 9.248894 Dis Loss: 0.0004929311 |||| 10.220331 -14.290283 -6.132671\n",
      "Iter: 949 Gen Loss: 148.87965 Recon Loss: 148.97604 Gen ADV Loss: 9.649264 Dis Loss: 0.00025184982 |||| 10.941865 -12.75172 -5.628639\n",
      "Iter: 950 Gen Loss: 154.02623 Recon Loss: 154.1279 Gen ADV Loss: 9.480826 Dis Loss: 0.00023283035 |||| 11.164811 -13.139522 -5.9368167\n",
      "Iter: 951 Gen Loss: 152.61098 Recon Loss: 152.71126 Gen ADV Loss: 9.4641 Dis Loss: 0.00025642884 |||| 10.3338375 -12.513581 -6.5516973\n",
      "Iter: 952 Gen Loss: 147.90952 Recon Loss: 148.00455 Gen ADV Loss: 9.934706 Dis Loss: 0.00020087573 |||| 10.76795 -13.39599 -6.3982553\n",
      "Iter: 953 Gen Loss: 142.47668 Recon Loss: 142.56624 Gen ADV Loss: 9.9418125 Dis Loss: 0.00023769576 |||| 11.029477 -14.258802 -5.8830605\n",
      "Iter: 954 Gen Loss: 146.22365 Recon Loss: 146.31691 Gen ADV Loss: 9.986079 Dis Loss: 0.00023645916 |||| 10.540404 -14.684359 -6.1828084\n",
      "Iter: 955 Gen Loss: 148.26187 Recon Loss: 148.35728 Gen ADV Loss: 9.820714 Dis Loss: 0.00028835688 |||| 11.076511 -13.242351 -5.55454\n",
      "Iter: 956 Gen Loss: 143.45169 Recon Loss: 143.5421 Gen ADV Loss: 9.968941 Dis Loss: 0.00018184376 |||| 10.839839 -14.4031725 -6.4173036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 957 Gen Loss: 153.9444 Recon Loss: 154.04538 Gen ADV Loss: 9.857883 Dis Loss: 0.00041471992 |||| 10.403604 -15.252094 -6.3757715\n",
      "Iter: 958 Gen Loss: 162.33609 Recon Loss: 162.4445 Gen ADV Loss: 10.769714 Dis Loss: 0.00018719905 |||| 11.5255995 -15.185993 -6.888441\n",
      "Iter: 959 Gen Loss: 167.30327 Recon Loss: 167.41765 Gen ADV Loss: 9.772399 Dis Loss: 0.00035792452 |||| 10.147052 -14.788721 -6.748448\n",
      "Iter: 960 Gen Loss: 145.63872 Recon Loss: 145.7309 Gen ADV Loss: 10.212786 Dis Loss: 0.00021246668 |||| 10.82401 -13.669999 -6.6329837\n",
      "Iter: 961 Gen Loss: 142.80685 Recon Loss: 142.8956 Gen ADV Loss: 10.767474 Dis Loss: 0.00020971155 |||| 11.269585 -14.893591 -6.5168595\n",
      "Iter: 962 Gen Loss: 148.38937 Recon Loss: 148.4842 Gen ADV Loss: 10.236274 Dis Loss: 0.00016857599 |||| 11.08439 -14.432352 -7.0743256\n",
      "Iter: 963 Gen Loss: 146.51096 Recon Loss: 146.60391 Gen ADV Loss: 10.2504 Dis Loss: 0.00024347095 |||| 10.772383 -12.974642 -6.219415\n",
      "Iter: 964 Gen Loss: 149.51566 Recon Loss: 149.61267 Gen ADV Loss: 9.123651 Dis Loss: 0.00032621832 |||| 12.2455 -11.617657 -5.4585247\n",
      "Iter: 965 Gen Loss: 151.73164 Recon Loss: 151.8309 Gen ADV Loss: 9.041758 Dis Loss: 0.00022437691 |||| 11.400958 -13.651213 -5.6319733\n",
      "Iter: 966 Gen Loss: 150.33865 Recon Loss: 150.43582 Gen ADV Loss: 9.729001 Dis Loss: 0.00021599952 |||| 12.243183 -11.86338 -5.806616\n",
      "Iter: 967 Gen Loss: 141.84866 Recon Loss: 141.9373 Gen ADV Loss: 9.756939 Dis Loss: 0.00021567283 |||| 10.782562 -14.2886915 -7.2160606\n",
      "Iter: 968 Gen Loss: 153.20137 Recon Loss: 153.30157 Gen ADV Loss: 9.549847 Dis Loss: 0.00029493868 |||| 12.149402 -12.081507 -5.169512\n",
      "Iter: 969 Gen Loss: 156.09409 Recon Loss: 156.19725 Gen ADV Loss: 9.40476 Dis Loss: 0.00039489364 |||| 12.442627 -12.01656 -4.3663874\n",
      "Iter: 970 Gen Loss: 148.6892 Recon Loss: 148.7857 Gen ADV Loss: 8.620601 Dis Loss: 0.0003265846 |||| 10.556535 -13.190839 -4.986747\n",
      "Iter: 971 Gen Loss: 152.30756 Recon Loss: 152.40697 Gen ADV Loss: 9.324842 Dis Loss: 0.00025360743 |||| 11.050372 -14.085365 -5.755918\n",
      "Iter: 972 Gen Loss: 151.5493 Recon Loss: 151.64813 Gen ADV Loss: 9.135418 Dis Loss: 0.00035737734 |||| 11.829678 -12.413265 -5.436327\n",
      "Iter: 973 Gen Loss: 150.70003 Recon Loss: 150.79788 Gen ADV Loss: 9.193534 Dis Loss: 0.0003929575 |||| 11.109422 -13.254432 -4.053447\n",
      "Iter: 974 Gen Loss: 147.96371 Recon Loss: 148.05853 Gen ADV Loss: 9.455589 Dis Loss: 0.0002030502 |||| 10.737439 -16.139818 -6.8091106\n",
      "Iter: 975 Gen Loss: 151.25484 Recon Loss: 151.35216 Gen ADV Loss: 10.2387705 Dis Loss: 0.00017816083 |||| 11.297196 -15.444905 -7.238574\n",
      "Iter: 976 Gen Loss: 150.18185 Recon Loss: 150.27814 Gen ADV Loss: 10.177583 Dis Loss: 0.00016903866 |||| 11.393438 -13.118437 -6.332707\n",
      "Iter: 977 Gen Loss: 147.85274 Recon Loss: 147.94666 Gen ADV Loss: 10.128559 Dis Loss: 0.0002998683 |||| 10.707585 -14.554505 -4.6638675\n",
      "Iter: 978 Gen Loss: 146.86763 Recon Loss: 146.96062 Gen ADV Loss: 10.018336 Dis Loss: 0.00018570642 |||| 10.745559 -13.750283 -6.930604\n",
      "Iter: 979 Gen Loss: 149.7989 Recon Loss: 149.89471 Gen ADV Loss: 10.116515 Dis Loss: 0.00017180174 |||| 11.309116 -14.307737 -7.086799\n",
      "Iter: 980 Gen Loss: 150.25864 Recon Loss: 150.35468 Gen ADV Loss: 10.334425 Dis Loss: 0.00014716276 |||| 11.798756 -13.699653 -7.260344\n",
      "Iter: 981 Gen Loss: 141.54166 Recon Loss: 141.62918 Gen ADV Loss: 10.103461 Dis Loss: 0.0004611803 |||| 10.817428 -12.276185 -5.1828055\n",
      "Iter: 982 Gen Loss: 161.84518 Recon Loss: 161.95393 Gen ADV Loss: 9.15362 Dis Loss: 0.0002422775 |||| 10.937887 -13.53375 -6.293976\n",
      "Iter: 983 Gen Loss: 145.49066 Recon Loss: 145.58276 Gen ADV Loss: 9.355425 Dis Loss: 0.00036841817 |||| 10.493919 -13.467163 -7.002714\n",
      "Iter: 984 Gen Loss: 164.81049 Recon Loss: 164.92111 Gen ADV Loss: 10.101095 Dis Loss: 0.00018062696 |||| 12.051649 -14.460895 -7.149748\n",
      "Iter: 985 Gen Loss: 142.5537 Recon Loss: 142.64247 Gen ADV Loss: 9.68583 Dis Loss: 0.00025240652 |||| 11.104606 -12.326137 -5.9518986\n",
      "Iter: 986 Gen Loss: 133.7365 Recon Loss: 133.8166 Gen ADV Loss: 9.513405 Dis Loss: 0.00024183355 |||| 10.981056 -12.743481 -6.3745584\n",
      "Iter: 987 Gen Loss: 142.18143 Recon Loss: 142.27017 Gen ADV Loss: 9.289879 Dis Loss: 0.00034809 |||| 10.779877 -12.940707 -5.707287\n",
      "Iter: 988 Gen Loss: 143.36746 Recon Loss: 143.45656 Gen ADV Loss: 10.088644 Dis Loss: 0.0003355137 |||| 10.9816675 -13.664537 -6.2465935\n",
      "Iter: 989 Gen Loss: 151.67671 Recon Loss: 151.77455 Gen ADV Loss: 9.646036 Dis Loss: 0.00022536752 |||| 11.022095 -13.73517 -6.1137996\n",
      "Iter: 990 Gen Loss: 143.36218 Recon Loss: 143.4515 Gen ADV Loss: 9.772111 Dis Loss: 0.00022770165 |||| 11.396893 -15.384721 -6.763374\n",
      "Iter: 991 Gen Loss: 139.86734 Recon Loss: 139.95224 Gen ADV Loss: 10.664926 Dis Loss: 0.0002919449 |||| 10.0062 -15.467478 -6.6910477\n",
      "Iter: 992 Gen Loss: 143.05026 Recon Loss: 143.13826 Gen ADV Loss: 10.7170315 Dis Loss: 0.00022942797 |||| 10.622724 -14.86014 -6.7846694\n",
      "Iter: 993 Gen Loss: 151.65535 Recon Loss: 151.75267 Gen ADV Loss: 9.95926 Dis Loss: 0.00030268752 |||| 12.204669 -13.117585 -5.681304\n",
      "Iter: 994 Gen Loss: 148.67607 Recon Loss: 148.7713 Gen ADV Loss: 9.03852 Dis Loss: 0.00031791872 |||| 10.358757 -13.861191 -7.007803\n",
      "Iter: 995 Gen Loss: 154.06589 Recon Loss: 154.16539 Gen ADV Loss: 10.102164 Dis Loss: 0.00018815426 |||| 12.393102 -15.209248 -6.676279\n",
      "Iter: 996 Gen Loss: 146.90039 Recon Loss: 146.99301 Gen ADV Loss: 9.804349 Dis Loss: 0.0003122256 |||| 11.641279 -15.205556 -4.7180595\n",
      "Iter: 997 Gen Loss: 150.80319 Recon Loss: 150.90039 Gen ADV Loss: 9.102765 Dis Loss: 0.00032502657 |||| 11.823213 -13.48808 -4.49881\n",
      "Iter: 998 Gen Loss: 160.07506 Recon Loss: 160.18161 Gen ADV Loss: 8.990606 Dis Loss: 0.00030646578 |||| 12.478333 -11.9894 -5.3817706\n",
      "Iter: 999 Gen Loss: 147.96239 Recon Loss: 148.05649 Gen ADV Loss: 9.286312 Dis Loss: 0.00024864933 |||| 10.441392 -14.005382 -6.097693\n",
      "========================================================================\n",
      "9.170376 -0.8091527\n",
      "8.025114 -0.677565\n",
      "9.207361 -0.78362244\n",
      "7.019518 -0.80261296\n",
      "6.5427046 -0.5316013\n",
      "5.080696 -0.47959328\n",
      "6.8661127 0.0\n",
      "8.316918 0.0\n",
      "15.199013 0.0\n",
      "23.023388 0.0\n",
      "2.240638 -1.8215411\n",
      "0.9776154 -0.9489918\n",
      "140.88023 0.00019541642\n",
      "=========================================================================\n",
      "Iter: 1000 Gen Loss: 152.27766 Recon Loss: 152.3754 Gen ADV Loss: 9.954346 Dis Loss: 0.0002159958 |||| 10.810308 -13.451019 -5.7534595\n",
      "Iter: 1001 Gen Loss: 152.3302 Recon Loss: 152.42728 Gen ADV Loss: 10.593218 Dis Loss: 0.00013098307 |||| 12.402114 -14.480888 -7.2175655\n",
      "Iter: 1002 Gen Loss: 149.15788 Recon Loss: 149.2517 Gen ADV Loss: 10.653163 Dis Loss: 0.00021891319 |||| 11.28808 -14.3890085 -4.9090185\n",
      "Iter: 1003 Gen Loss: 145.9698 Recon Loss: 146.06123 Gen ADV Loss: 9.8129635 Dis Loss: 0.00025664223 |||| 11.771001 -13.40904 -4.826413\n",
      "Iter: 1004 Gen Loss: 155.9567 Recon Loss: 156.05843 Gen ADV Loss: 9.422727 Dis Loss: 0.000198797 |||| 11.802798 -13.197847 -6.4444265\n",
      "Iter: 1005 Gen Loss: 145.86552 Recon Loss: 145.9569 Gen ADV Loss: 9.673256 Dis Loss: 0.00028773502 |||| 11.018591 -13.325143 -6.0085945\n",
      "Iter: 1006 Gen Loss: 148.92285 Recon Loss: 149.01642 Gen ADV Loss: 10.526348 Dis Loss: 0.0002307268 |||| 10.529816 -14.879834 -6.5961947\n",
      "Iter: 1007 Gen Loss: 145.83752 Recon Loss: 145.92822 Gen ADV Loss: 10.285854 Dis Loss: 0.00018697155 |||| 12.1379 -12.915404 -6.277494\n",
      "Iter: 1008 Gen Loss: 152.06213 Recon Loss: 152.15973 Gen ADV Loss: 9.5820875 Dis Loss: 0.000424426 |||| 9.632017 -13.677297 -6.690642\n",
      "Iter: 1009 Gen Loss: 150.6521 Recon Loss: 150.74753 Gen ADV Loss: 10.281406 Dis Loss: 0.00015013838 |||| 11.925024 -14.268378 -6.8657665\n",
      "Iter: 1010 Gen Loss: 146.56165 Recon Loss: 146.65305 Gen ADV Loss: 10.186588 Dis Loss: 0.00020198947 |||| 10.766712 -13.756489 -7.1239557\n",
      "Iter: 1011 Gen Loss: 149.47758 Recon Loss: 149.57175 Gen ADV Loss: 10.299168 Dis Loss: 0.0001795823 |||| 12.2913475 -13.518965 -6.1425333\n",
      "Iter: 1012 Gen Loss: 152.26527 Recon Loss: 152.36298 Gen ADV Loss: 9.523716 Dis Loss: 0.0003664723 |||| 11.128061 -12.882042 -5.42104\n",
      "Iter: 1013 Gen Loss: 158.06944 Recon Loss: 158.17247 Gen ADV Loss: 9.960689 Dis Loss: 0.00021719706 |||| 12.346384 -13.054056 -5.769109\n",
      "Iter: 1014 Gen Loss: 148.23781 Recon Loss: 148.33127 Gen ADV Loss: 9.666691 Dis Loss: 0.00016999693 |||| 12.061493 -12.625623 -7.0589976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1015 Gen Loss: 141.02248 Recon Loss: 141.10896 Gen ADV Loss: 9.412934 Dis Loss: 0.0002854096 |||| 10.662675 -13.876015 -7.032523\n",
      "Iter: 1016 Gen Loss: 147.15804 Recon Loss: 147.24982 Gen ADV Loss: 10.186514 Dis Loss: 0.00020392705 |||| 10.832191 -14.72034 -6.962552\n",
      "Iter: 1017 Gen Loss: 159.16946 Recon Loss: 159.27345 Gen ADV Loss: 9.947832 Dis Loss: 0.00018413438 |||| 11.9315405 -13.646805 -6.457824\n",
      "Iter: 1018 Gen Loss: 150.56813 Recon Loss: 150.66382 Gen ADV Loss: 9.6090355 Dis Loss: 0.0004276226 |||| 10.631129 -12.678532 -4.8552556\n",
      "Iter: 1019 Gen Loss: 150.47629 Recon Loss: 150.5718 Gen ADV Loss: 9.641269 Dis Loss: 0.00023445532 |||| 12.814283 -12.877184 -5.782468\n",
      "Iter: 1020 Gen Loss: 161.78487 Recon Loss: 161.89308 Gen ADV Loss: 8.201838 Dis Loss: 0.00030487607 |||| 10.944788 -13.271041 -6.432478\n",
      "Iter: 1021 Gen Loss: 141.35364 Recon Loss: 141.43953 Gen ADV Loss: 10.020763 Dis Loss: 0.0006719288 |||| 9.467629 -14.708489 -7.0054355\n",
      "Iter: 1022 Gen Loss: 155.67358 Recon Loss: 155.77295 Gen ADV Loss: 10.846394 Dis Loss: 0.00014727062 |||| 11.667759 -14.530065 -6.8905835\n",
      "Iter: 1023 Gen Loss: 153.43726 Recon Loss: 153.53488 Gen ADV Loss: 10.345892 Dis Loss: 0.00018761461 |||| 12.227007 -14.175539 -6.198548\n",
      "Iter: 1024 Gen Loss: 154.04803 Recon Loss: 154.147 Gen ADV Loss: 9.599221 Dis Loss: 0.00027957043 |||| 12.038622 -12.420972 -5.313073\n",
      "Iter: 1025 Gen Loss: 151.59193 Recon Loss: 151.68907 Gen ADV Loss: 8.914834 Dis Loss: 0.0005983453 |||| 12.071382 -13.786722 -5.3991175\n",
      "Iter: 1026 Gen Loss: 151.96521 Recon Loss: 152.06216 Gen ADV Loss: 9.383662 Dis Loss: 0.00017237126 |||| 12.124917 -14.436 -6.2112894\n",
      "Iter: 1027 Gen Loss: 140.46793 Recon Loss: 140.55267 Gen ADV Loss: 10.086357 Dis Loss: 0.00017849723 |||| 11.402889 -14.832502 -7.111689\n",
      "Iter: 1028 Gen Loss: 148.94646 Recon Loss: 149.03992 Gen ADV Loss: 9.842442 Dis Loss: 0.0002456863 |||| 12.360542 -13.871601 -5.77417\n",
      "Iter: 1029 Gen Loss: 147.70486 Recon Loss: 147.79776 Gen ADV Loss: 9.157886 Dis Loss: 0.00038396823 |||| 11.835346 -12.430033 -5.2171893\n",
      "Iter: 1030 Gen Loss: 158.5996 Recon Loss: 158.70366 Gen ADV Loss: 8.831219 Dis Loss: 0.00018614218 |||| 11.843719 -13.462632 -6.366927\n",
      "Iter: 1031 Gen Loss: 152.26778 Recon Loss: 152.3638 Gen ADV Loss: 10.451477 Dis Loss: 0.00013312245 |||| 12.311012 -13.768624 -7.4448156\n",
      "Iter: 1032 Gen Loss: 152.658 Recon Loss: 152.75427 Gen ADV Loss: 10.635637 Dis Loss: 0.00015432603 |||| 11.999319 -14.524213 -7.003973\n",
      "Iter: 1033 Gen Loss: 151.88773 Recon Loss: 151.98358 Gen ADV Loss: 10.264924 Dis Loss: 0.00016844619 |||| 11.86022 -15.252198 -6.325129\n",
      "Iter: 1034 Gen Loss: 148.54321 Recon Loss: 148.63602 Gen ADV Loss: 9.933338 Dis Loss: 0.00023585011 |||| 12.39924 -13.308004 -5.664988\n",
      "Iter: 1035 Gen Loss: 158.1787 Recon Loss: 158.28128 Gen ADV Loss: 9.7410755 Dis Loss: 0.00023765535 |||| 11.748264 -13.690689 -5.3541236\n",
      "Iter: 1036 Gen Loss: 144.39366 Recon Loss: 144.4816 Gen ADV Loss: 10.525222 Dis Loss: 0.0002599999 |||| 10.113506 -14.886451 -7.2935405\n",
      "Iter: 1037 Gen Loss: 149.97281 Recon Loss: 150.06602 Gen ADV Loss: 10.790398 Dis Loss: 0.0001524208 |||| 11.351262 -14.8057575 -7.12436\n",
      "Iter: 1038 Gen Loss: 148.75621 Recon Loss: 148.8484 Gen ADV Loss: 10.596476 Dis Loss: 0.00015907467 |||| 11.706585 -14.43465 -6.895586\n",
      "Iter: 1039 Gen Loss: 158.41298 Recon Loss: 158.51488 Gen ADV Loss: 10.522112 Dis Loss: 0.00017702617 |||| 11.004103 -13.611925 -6.4052887\n",
      "Iter: 1040 Gen Loss: 148.73196 Recon Loss: 148.82399 Gen ADV Loss: 10.62918 Dis Loss: 0.00027040168 |||| 12.189514 -13.861953 -5.846484\n",
      "Iter: 1041 Gen Loss: 143.86649 Recon Loss: 143.95404 Gen ADV Loss: 10.190732 Dis Loss: 0.00026066322 |||| 11.882973 -14.285294 -5.709282\n",
      "Iter: 1042 Gen Loss: 145.40483 Recon Loss: 145.49445 Gen ADV Loss: 9.638703 Dis Loss: 0.00022407588 |||| 12.660748 -13.729079 -5.886496\n",
      "Iter: 1043 Gen Loss: 148.75607 Recon Loss: 148.84966 Gen ADV Loss: 9.012637 Dis Loss: 0.00015330355 |||| 12.30637 -13.995582 -6.3096423\n",
      "Iter: 1044 Gen Loss: 138.65903 Recon Loss: 138.74103 Gen ADV Loss: 10.46291 Dis Loss: 0.00022783484 |||| 10.390031 -14.191283 -7.4547586\n",
      "Iter: 1045 Gen Loss: 141.09888 Recon Loss: 141.18274 Gen ADV Loss: 10.988743 Dis Loss: 0.00014810762 |||| 11.6524935 -14.210732 -6.9191694\n",
      "Iter: 1046 Gen Loss: 154.05135 Recon Loss: 154.14865 Gen ADV Loss: 10.468898 Dis Loss: 0.00014577959 |||| 12.560402 -14.2820425 -7.417466\n",
      "Iter: 1047 Gen Loss: 147.59497 Recon Loss: 147.68637 Gen ADV Loss: 9.910077 Dis Loss: 0.00017792659 |||| 11.2052555 -15.334736 -6.4102306\n",
      "Iter: 1048 Gen Loss: 151.3229 Recon Loss: 151.4173 Gen ADV Loss: 10.588057 Dis Loss: 0.00012857307 |||| 11.992489 -14.45187 -7.828658\n",
      "Iter: 1049 Gen Loss: 147.8517 Recon Loss: 147.94238 Gen ADV Loss: 10.797351 Dis Loss: 0.00017443854 |||| 10.995866 -14.83203 -7.4705963\n",
      "Iter: 1050 Gen Loss: 146.99997 Recon Loss: 147.0908 Gen ADV Loss: 9.759434 Dis Loss: 0.00016649549 |||| 12.203126 -13.681431 -6.3294888\n",
      "Iter: 1051 Gen Loss: 141.7936 Recon Loss: 141.8786 Gen ADV Loss: 10.326709 Dis Loss: 0.0001885012 |||| 11.479911 -13.021838 -7.0044546\n",
      "Iter: 1052 Gen Loss: 140.49786 Recon Loss: 140.5816 Gen ADV Loss: 10.235614 Dis Loss: 0.0004915107 |||| 9.941092 -14.602267 -6.296956\n",
      "Iter: 1053 Gen Loss: 148.2895 Recon Loss: 148.38144 Gen ADV Loss: 9.797291 Dis Loss: 0.00019892122 |||| 11.397577 -14.015154 -5.817102\n",
      "Iter: 1054 Gen Loss: 146.76074 Recon Loss: 146.85104 Gen ADV Loss: 9.893209 Dis Loss: 0.00015467568 |||| 11.889343 -14.308836 -6.3439665\n",
      "Iter: 1055 Gen Loss: 143.83308 Recon Loss: 143.91971 Gen ADV Loss: 10.59399 Dis Loss: 0.00022819176 |||| 10.281368 -14.753765 -7.4994054\n",
      "Iter: 1056 Gen Loss: 147.4585 Recon Loss: 147.54874 Gen ADV Loss: 10.565016 Dis Loss: 0.00024133327 |||| 11.803986 -14.617502 -5.15648\n",
      "Iter: 1057 Gen Loss: 148.23407 Recon Loss: 148.3253 Gen ADV Loss: 10.315098 Dis Loss: 0.00022404495 |||| 12.566828 -13.828394 -6.02645\n",
      "Iter: 1058 Gen Loss: 143.56082 Recon Loss: 143.64767 Gen ADV Loss: 9.991075 Dis Loss: 0.00030912226 |||| 12.789865 -13.032007 -5.2638083\n",
      "Iter: 1059 Gen Loss: 145.06763 Recon Loss: 145.15665 Gen ADV Loss: 9.344038 Dis Loss: 0.00017265255 |||| 10.884953 -15.047947 -6.520484\n",
      "Iter: 1060 Gen Loss: 145.01855 Recon Loss: 145.10631 Gen ADV Loss: 10.5283985 Dis Loss: 0.00013527999 |||| 12.567318 -16.208147 -7.367076\n",
      "Iter: 1061 Gen Loss: 151.56323 Recon Loss: 151.6575 Gen ADV Loss: 10.508437 Dis Loss: 0.00027356885 |||| 10.247353 -14.846606 -6.0749826\n",
      "Iter: 1062 Gen Loss: 134.28053 Recon Loss: 134.35727 Gen ADV Loss: 10.660057 Dis Loss: 0.00023584632 |||| 10.807191 -13.717318 -6.5169454\n",
      "Iter: 1063 Gen Loss: 154.69821 Recon Loss: 154.79543 Gen ADV Loss: 10.629041 Dis Loss: 0.00023289444 |||| 12.594124 -13.82952 -4.392574\n",
      "Iter: 1064 Gen Loss: 141.31662 Recon Loss: 141.40141 Gen ADV Loss: 9.646307 Dis Loss: 0.00032406286 |||| 11.171955 -12.917247 -4.9813323\n",
      "Iter: 1065 Gen Loss: 143.089 Recon Loss: 143.17543 Gen ADV Loss: 9.714255 Dis Loss: 0.00024549672 |||| 12.679784 -13.2237425 -5.8674254\n",
      "Iter: 1066 Gen Loss: 146.34679 Recon Loss: 146.43694 Gen ADV Loss: 9.222176 Dis Loss: 0.00018402838 |||| 10.920874 -14.131391 -6.5573792\n",
      "Iter: 1067 Gen Loss: 152.02565 Recon Loss: 152.12022 Gen ADV Loss: 10.450413 Dis Loss: 0.00018383519 |||| 11.705303 -14.500224 -6.2352967\n",
      "Iter: 1068 Gen Loss: 151.64838 Recon Loss: 151.74316 Gen ADV Loss: 9.814899 Dis Loss: 0.00016144302 |||| 12.33187 -13.277355 -6.84841\n",
      "Iter: 1069 Gen Loss: 151.78069 Recon Loss: 151.8751 Gen ADV Loss: 10.264368 Dis Loss: 0.00017242722 |||| 11.750452 -13.841642 -6.9952383\n",
      "Iter: 1070 Gen Loss: 151.75475 Recon Loss: 151.84924 Gen ADV Loss: 10.142507 Dis Loss: 0.00013974277 |||| 11.460579 -16.659678 -7.153909\n",
      "Iter: 1071 Gen Loss: 146.35681 Recon Loss: 146.44493 Gen ADV Loss: 11.084181 Dis Loss: 0.00014116683 |||| 11.548278 -14.232278 -7.6886063\n",
      "Iter: 1072 Gen Loss: 144.44203 Recon Loss: 144.52832 Gen ADV Loss: 10.95394 Dis Loss: 0.0002457155 |||| 11.2990465 -14.400307 -4.9085035\n",
      "Iter: 1073 Gen Loss: 153.85777 Recon Loss: 153.95413 Gen ADV Loss: 10.264739 Dis Loss: 0.00018505109 |||| 11.842495 -13.545036 -6.4839463\n",
      "Iter: 1074 Gen Loss: 146.59381 Recon Loss: 146.68333 Gen ADV Loss: 9.784344 Dis Loss: 0.00021686999 |||| 11.356841 -13.436483 -5.978569\n",
      "Iter: 1075 Gen Loss: 145.38853 Recon Loss: 145.47694 Gen ADV Loss: 9.693051 Dis Loss: 0.00018341435 |||| 12.6937685 -13.156415 -6.6118584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1076 Gen Loss: 149.05096 Recon Loss: 149.14218 Gen ADV Loss: 10.527899 Dis Loss: 0.00014215997 |||| 12.675734 -13.756787 -6.7092104\n",
      "Iter: 1077 Gen Loss: 150.83327 Recon Loss: 150.92755 Gen ADV Loss: 9.211954 Dis Loss: 0.00014923495 |||| 12.691178 -14.403037 -6.9593396\n",
      "Iter: 1078 Gen Loss: 145.49356 Recon Loss: 145.58107 Gen ADV Loss: 10.600328 Dis Loss: 0.00012865734 |||| 11.993404 -14.223935 -7.8025646\n",
      "Iter: 1079 Gen Loss: 144.22232 Recon Loss: 144.30855 Gen ADV Loss: 10.568204 Dis Loss: 0.00016886383 |||| 11.891836 -14.429914 -6.3815813\n",
      "Iter: 1080 Gen Loss: 154.16902 Recon Loss: 154.26556 Gen ADV Loss: 10.190439 Dis Loss: 0.00020734513 |||| 11.64482 -13.639249 -5.9123335\n",
      "Iter: 1081 Gen Loss: 147.41193 Recon Loss: 147.50171 Gen ADV Loss: 10.103124 Dis Loss: 0.00017521065 |||| 10.980573 -14.7405815 -7.1247077\n",
      "Iter: 1082 Gen Loss: 151.7425 Recon Loss: 151.83582 Gen ADV Loss: 10.870029 Dis Loss: 0.00013257553 |||| 11.772996 -15.074013 -8.171711\n",
      "Iter: 1083 Gen Loss: 153.62584 Recon Loss: 153.72115 Gen ADV Loss: 10.748157 Dis Loss: 0.00016495268 |||| 12.315847 -13.794046 -7.1392655\n",
      "Iter: 1084 Gen Loss: 140.23042 Recon Loss: 140.31215 Gen ADV Loss: 10.852226 Dis Loss: 0.0001721295 |||| 10.9889345 -13.277285 -6.9187865\n",
      "Iter: 1085 Gen Loss: 148.3769 Recon Loss: 148.46744 Gen ADV Loss: 10.21539 Dis Loss: 0.00023660195 |||| 11.912819 -14.151005 -7.1724563\n",
      "Iter: 1086 Gen Loss: 139.30487 Recon Loss: 139.38564 Gen ADV Loss: 10.858297 Dis Loss: 0.00015394803 |||| 11.442548 -15.7423115 -7.1964436\n",
      "Iter: 1087 Gen Loss: 141.59341 Recon Loss: 141.677 Gen ADV Loss: 10.262742 Dis Loss: 0.0001893492 |||| 11.948725 -13.647996 -5.9933715\n",
      "Iter: 1088 Gen Loss: 141.69817 Recon Loss: 141.78227 Gen ADV Loss: 9.842465 Dis Loss: 0.000316842 |||| 11.459552 -13.152289 -5.0655556\n",
      "Iter: 1089 Gen Loss: 148.54985 Recon Loss: 148.64139 Gen ADV Loss: 9.261175 Dis Loss: 0.00022595807 |||| 11.193836 -12.903445 -6.412682\n",
      "Iter: 1090 Gen Loss: 144.01112 Recon Loss: 144.09747 Gen ADV Loss: 9.825931 Dis Loss: 0.00020818357 |||| 11.625392 -13.203189 -6.1408644\n",
      "Iter: 1091 Gen Loss: 145.73653 Recon Loss: 145.82439 Gen ADV Loss: 10.007035 Dis Loss: 0.00015710077 |||| 11.929645 -15.135793 -5.9898868\n",
      "Iter: 1092 Gen Loss: 138.40503 Recon Loss: 138.48476 Gen ADV Loss: 10.797411 Dis Loss: 0.00024777706 |||| 10.677883 -15.2333145 -5.881551\n",
      "Iter: 1093 Gen Loss: 144.01945 Recon Loss: 144.10535 Gen ADV Loss: 10.2381315 Dis Loss: 0.00025279928 |||| 10.990348 -14.789001 -7.413512\n",
      "Iter: 1094 Gen Loss: 151.30736 Recon Loss: 151.40054 Gen ADV Loss: 10.183563 Dis Loss: 0.00015576824 |||| 12.355522 -13.334918 -6.0092998\n",
      "Iter: 1095 Gen Loss: 139.27312 Recon Loss: 139.35442 Gen ADV Loss: 9.962583 Dis Loss: 0.00017038074 |||| 11.852242 -14.641848 -6.3080297\n",
      "Iter: 1096 Gen Loss: 148.02472 Recon Loss: 148.11421 Gen ADV Loss: 10.514125 Dis Loss: 0.00016586034 |||| 11.458297 -14.891525 -6.486561\n",
      "Iter: 1097 Gen Loss: 139.2698 Recon Loss: 139.3511 Gen ADV Loss: 9.916637 Dis Loss: 0.00025466311 |||| 11.086435 -13.498693 -5.6798797\n",
      "Iter: 1098 Gen Loss: 138.28345 Recon Loss: 138.3635 Gen ADV Loss: 10.128535 Dis Loss: 0.00043040758 |||| 11.491919 -13.848608 -4.5156326\n",
      "Iter: 1099 Gen Loss: 139.76941 Recon Loss: 139.85133 Gen ADV Loss: 9.70993 Dis Loss: 0.00028049562 |||| 10.895017 -15.04736 -6.524833\n",
      "========================================================================\n",
      "8.205742 -0.8932662\n",
      "7.5061297 -0.8859134\n",
      "8.288283 -0.76978797\n",
      "8.336636 -0.61034256\n",
      "7.2391744 -0.5376927\n",
      "5.3777432 -0.5386696\n",
      "7.457797 0.0\n",
      "10.021436 0.0\n",
      "17.179468 0.0\n",
      "29.128693 0.0\n",
      "2.1581101 -2.0623672\n",
      "0.9736513 -0.96817887\n",
      "136.90877 0.00015609425\n",
      "=========================================================================\n",
      "Iter: 1100 Gen Loss: 153.63971 Recon Loss: 153.73549 Gen ADV Loss: 9.705627 Dis Loss: 0.000177093 |||| 12.673349 -12.621155 -5.939336\n",
      "Iter: 1101 Gen Loss: 146.75586 Recon Loss: 146.8442 Gen ADV Loss: 10.208427 Dis Loss: 0.0001654092 |||| 12.709042 -14.32448 -6.602878\n",
      "Iter: 1102 Gen Loss: 135.91782 Recon Loss: 135.99551 Gen ADV Loss: 9.977054 Dis Loss: 0.00025839382 |||| 10.989887 -13.846402 -6.209797\n",
      "Iter: 1103 Gen Loss: 146.7048 Recon Loss: 146.79326 Gen ADV Loss: 9.994074 Dis Loss: 0.00022794146 |||| 12.041315 -12.898349 -4.63649\n",
      "Iter: 1104 Gen Loss: 142.43372 Recon Loss: 142.51794 Gen ADV Loss: 9.893412 Dis Loss: 0.00023790641 |||| 12.3233595 -14.704711 -6.2368917\n",
      "Iter: 1105 Gen Loss: 140.98193 Recon Loss: 141.06424 Gen ADV Loss: 10.344809 Dis Loss: 0.0003950593 |||| 11.029771 -14.197271 -4.8910303\n",
      "Iter: 1106 Gen Loss: 147.83194 Recon Loss: 147.92122 Gen ADV Loss: 10.208376 Dis Loss: 0.00015060665 |||| 13.096226 -13.906425 -6.424912\n",
      "Iter: 1107 Gen Loss: 149.89561 Recon Loss: 149.98671 Gen ADV Loss: 10.444571 Dis Loss: 0.000184676 |||| 12.516057 -13.041153 -5.876188\n",
      "Iter: 1108 Gen Loss: 145.98105 Recon Loss: 146.06873 Gen ADV Loss: 9.927372 Dis Loss: 0.00020724392 |||| 12.932233 -12.56567 -5.5927\n",
      "Iter: 1109 Gen Loss: 145.39859 Recon Loss: 145.4865 Gen ADV Loss: 9.083 Dis Loss: 0.00027853664 |||| 11.918405 -12.671516 -5.727828\n",
      "Iter: 1110 Gen Loss: 148.96175 Recon Loss: 149.05318 Gen ADV Loss: 9.07059 Dis Loss: 0.00027374097 |||| 12.436362 -12.644097 -5.48193\n",
      "Iter: 1111 Gen Loss: 160.79424 Recon Loss: 160.89728 Gen ADV Loss: 9.25484 Dis Loss: 0.00027416865 |||| 10.3052845 -13.880632 -5.761776\n",
      "Iter: 1112 Gen Loss: 153.59358 Recon Loss: 153.6883 Gen ADV Loss: 10.354383 Dis Loss: 0.00018683757 |||| 12.221703 -14.531047 -5.0403337\n",
      "Iter: 1113 Gen Loss: 147.60754 Recon Loss: 147.69606 Gen ADV Loss: 10.544941 Dis Loss: 0.00018430487 |||| 12.243374 -13.740383 -7.091835\n",
      "Iter: 1114 Gen Loss: 146.5615 Recon Loss: 146.64845 Gen ADV Loss: 11.012052 Dis Loss: 0.00014565219 |||| 11.768985 -15.136271 -6.539626\n",
      "Iter: 1115 Gen Loss: 147.24931 Recon Loss: 147.33725 Gen ADV Loss: 10.681232 Dis Loss: 0.00013920502 |||| 11.543109 -13.62968 -7.016229\n",
      "Iter: 1116 Gen Loss: 143.22585 Recon Loss: 143.3099 Gen ADV Loss: 10.461883 Dis Loss: 0.0001801071 |||| 12.095812 -14.261353 -6.7071824\n",
      "Iter: 1117 Gen Loss: 148.7364 Recon Loss: 148.82614 Gen ADV Loss: 10.270496 Dis Loss: 0.00019109779 |||| 12.547172 -13.231821 -5.6350007\n",
      "Iter: 1118 Gen Loss: 144.71347 Recon Loss: 144.79991 Gen ADV Loss: 9.521335 Dis Loss: 0.00025162735 |||| 11.1137 -13.087881 -6.53029\n",
      "Iter: 1119 Gen Loss: 154.31252 Recon Loss: 154.40797 Gen ADV Loss: 10.106505 Dis Loss: 0.00021334038 |||| 12.759967 -14.233938 -4.8538814\n",
      "Iter: 1120 Gen Loss: 146.38957 Recon Loss: 146.47705 Gen ADV Loss: 10.111559 Dis Loss: 0.00016421024 |||| 10.925916 -15.018317 -7.5118318\n",
      "Iter: 1121 Gen Loss: 149.85959 Recon Loss: 149.94965 Gen ADV Loss: 10.984446 Dis Loss: 0.00019801715 |||| 10.721888 -14.998972 -6.335782\n",
      "Iter: 1122 Gen Loss: 148.0171 Recon Loss: 148.10529 Gen ADV Loss: 10.945749 Dis Loss: 0.00014382703 |||| 12.127356 -14.405476 -7.928445\n",
      "Iter: 1123 Gen Loss: 146.70703 Recon Loss: 146.79408 Gen ADV Loss: 10.722431 Dis Loss: 0.00018681271 |||| 10.932887 -14.686807 -7.0944695\n",
      "Iter: 1124 Gen Loss: 151.58002 Recon Loss: 151.6719 Gen ADV Loss: 10.759891 Dis Loss: 0.00018702658 |||| 12.273313 -15.341336 -5.894974\n",
      "Iter: 1125 Gen Loss: 154.4562 Recon Loss: 154.5514 Gen ADV Loss: 10.315661 Dis Loss: 0.00015040075 |||| 13.250509 -13.244697 -6.7342257\n",
      "Iter: 1126 Gen Loss: 139.9025 Recon Loss: 139.9829 Gen ADV Loss: 10.475442 Dis Loss: 0.00015621894 |||| 11.565232 -15.032674 -6.7853837\n",
      "Iter: 1127 Gen Loss: 141.16841 Recon Loss: 141.25017 Gen ADV Loss: 10.34657 Dis Loss: 0.00015654604 |||| 11.967594 -14.443545 -6.8970537\n",
      "Iter: 1128 Gen Loss: 139.83018 Recon Loss: 139.91031 Gen ADV Loss: 10.611032 Dis Loss: 0.00016095414 |||| 12.643231 -13.649789 -7.0993714\n",
      "Iter: 1129 Gen Loss: 147.16476 Recon Loss: 147.2525 Gen ADV Loss: 10.31236 Dis Loss: 0.0002630466 |||| 11.260731 -13.8539915 -6.110628\n",
      "Iter: 1130 Gen Loss: 151.89072 Recon Loss: 151.9834 Gen ADV Loss: 10.036264 Dis Loss: 0.00016129443 |||| 11.711103 -15.317549 -6.6464815\n",
      "Iter: 1131 Gen Loss: 153.42067 Recon Loss: 153.51405 Gen ADV Loss: 10.84187 Dis Loss: 0.00042890376 |||| 10.783678 -15.7667465 -8.2087755\n",
      "Iter: 1132 Gen Loss: 144.10461 Recon Loss: 144.18825 Gen ADV Loss: 11.250064 Dis Loss: 0.00013441837 |||| 11.481056 -14.798443 -6.9509664\n",
      "Iter: 1133 Gen Loss: 143.95056 Recon Loss: 144.03476 Gen ADV Loss: 10.466717 Dis Loss: 0.00017217975 |||| 11.356182 -13.963552 -6.4343452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1134 Gen Loss: 143.44093 Recon Loss: 143.52469 Gen ADV Loss: 10.360974 Dis Loss: 0.00022282405 |||| 11.24608 -12.9950485 -6.2548223\n",
      "Iter: 1135 Gen Loss: 145.36053 Recon Loss: 145.44647 Gen ADV Loss: 10.072977 Dis Loss: 0.00016387044 |||| 12.120364 -14.969773 -5.9005775\n",
      "Iter: 1136 Gen Loss: 145.63454 Recon Loss: 145.72018 Gen ADV Loss: 10.602364 Dis Loss: 0.0001954785 |||| 12.39126 -14.390532 -5.6252093\n",
      "Iter: 1137 Gen Loss: 145.75175 Recon Loss: 145.83765 Gen ADV Loss: 10.449344 Dis Loss: 0.00016957315 |||| 12.455471 -14.038645 -6.3406715\n",
      "Iter: 1138 Gen Loss: 143.24841 Recon Loss: 143.33157 Gen ADV Loss: 10.615048 Dis Loss: 0.00022093298 |||| 11.921981 -15.723904 -5.9624095\n",
      "Iter: 1139 Gen Loss: 145.9087 Recon Loss: 145.99487 Gen ADV Loss: 10.247671 Dis Loss: 0.00016512148 |||| 13.074041 -14.217605 -5.4591336\n",
      "Iter: 1140 Gen Loss: 143.99823 Recon Loss: 144.08217 Gen ADV Loss: 10.5295925 Dis Loss: 0.00017164637 |||| 11.751737 -14.797561 -7.4721646\n",
      "Iter: 1141 Gen Loss: 148.34319 Recon Loss: 148.43141 Gen ADV Loss: 10.570224 Dis Loss: 0.0001464663 |||| 12.029288 -15.886805 -6.3714166\n",
      "Iter: 1142 Gen Loss: 141.55638 Recon Loss: 141.63794 Gen ADV Loss: 10.421269 Dis Loss: 0.00015117318 |||| 11.939244 -13.773196 -6.812531\n",
      "Iter: 1143 Gen Loss: 143.67424 Recon Loss: 143.7582 Gen ADV Loss: 10.137403 Dis Loss: 0.00019373237 |||| 12.394269 -14.941632 -6.6766663\n",
      "Iter: 1144 Gen Loss: 152.20946 Recon Loss: 152.30182 Gen ADV Loss: 10.240761 Dis Loss: 0.00016220409 |||| 13.181772 -14.524201 -5.6669335\n",
      "Iter: 1145 Gen Loss: 149.94585 Recon Loss: 150.03593 Gen ADV Loss: 10.21042 Dis Loss: 0.00015069294 |||| 11.450506 -14.594766 -6.8744493\n",
      "Iter: 1146 Gen Loss: 144.14192 Recon Loss: 144.22589 Gen ADV Loss: 10.46377 Dis Loss: 0.0001288609 |||| 12.230953 -14.739621 -7.2868686\n",
      "Iter: 1147 Gen Loss: 144.77417 Recon Loss: 144.8584 Gen ADV Loss: 10.811997 Dis Loss: 0.00013855318 |||| 12.051377 -14.870608 -6.7732425\n",
      "Iter: 1148 Gen Loss: 139.31172 Recon Loss: 139.39005 Gen ADV Loss: 11.209769 Dis Loss: 0.00016570704 |||| 11.544231 -15.908443 -7.8472867\n",
      "Iter: 1149 Gen Loss: 150.74141 Recon Loss: 150.83171 Gen ADV Loss: 10.643215 Dis Loss: 0.00028048642 |||| 13.196883 -13.702938 -5.4794273\n",
      "Iter: 1150 Gen Loss: 154.1098 Recon Loss: 154.20522 Gen ADV Loss: 8.896902 Dis Loss: 0.00024133545 |||| 11.898003 -13.3296585 -5.1722975\n",
      "Iter: 1151 Gen Loss: 144.92644 Recon Loss: 145.01094 Gen ADV Loss: 10.547116 Dis Loss: 0.00013692892 |||| 11.477143 -16.877867 -7.61787\n",
      "Iter: 1152 Gen Loss: 149.8959 Recon Loss: 149.98473 Gen ADV Loss: 11.188679 Dis Loss: 0.00011755001 |||| 12.79878 -14.03842 -7.549913\n",
      "Iter: 1153 Gen Loss: 149.84653 Recon Loss: 149.9357 Gen ADV Loss: 10.789354 Dis Loss: 0.00013852265 |||| 12.407019 -13.8689 -6.7943234\n",
      "Iter: 1154 Gen Loss: 151.56197 Recon Loss: 151.65285 Gen ADV Loss: 10.741214 Dis Loss: 0.00012703676 |||| 12.894898 -14.031902 -6.9875355\n",
      "Iter: 1155 Gen Loss: 142.85052 Recon Loss: 142.93343 Gen ADV Loss: 9.96336 Dis Loss: 0.00018346614 |||| 12.180973 -13.252483 -5.3941708\n",
      "Iter: 1156 Gen Loss: 148.81898 Recon Loss: 148.90791 Gen ADV Loss: 9.896428 Dis Loss: 0.00016269737 |||| 11.990554 -13.485054 -7.374515\n",
      "Iter: 1157 Gen Loss: 142.66794 Recon Loss: 142.74974 Gen ADV Loss: 10.823684 Dis Loss: 0.00015367594 |||| 11.633722 -14.347047 -5.6151114\n",
      "Iter: 1158 Gen Loss: 140.96677 Recon Loss: 141.0471 Gen ADV Loss: 10.565721 Dis Loss: 0.00016969319 |||| 11.964239 -13.951762 -6.1846724\n",
      "Iter: 1159 Gen Loss: 141.80116 Recon Loss: 141.88203 Gen ADV Loss: 10.799646 Dis Loss: 0.00015247453 |||| 12.261866 -13.598994 -5.628056\n",
      "Iter: 1160 Gen Loss: 150.09262 Recon Loss: 150.18231 Gen ADV Loss: 10.271267 Dis Loss: 0.00014587847 |||| 12.622256 -13.233087 -7.271484\n",
      "Iter: 1161 Gen Loss: 146.45656 Recon Loss: 146.5424 Gen ADV Loss: 10.405622 Dis Loss: 0.00037228403 |||| 10.030775 -14.171701 -8.026809\n",
      "Iter: 1162 Gen Loss: 149.10036 Recon Loss: 149.18872 Gen ADV Loss: 10.47845 Dis Loss: 0.00016137127 |||| 11.797366 -15.127347 -6.6829815\n",
      "Iter: 1163 Gen Loss: 138.08533 Recon Loss: 138.16252 Gen ADV Loss: 10.641221 Dis Loss: 0.00019643095 |||| 11.54526 -15.411622 -6.5676513\n",
      "Iter: 1164 Gen Loss: 145.23422 Recon Loss: 145.31888 Gen ADV Loss: 10.272525 Dis Loss: 0.00020648801 |||| 12.436651 -12.981203 -6.4012733\n",
      "Iter: 1165 Gen Loss: 147.8623 Recon Loss: 147.95004 Gen ADV Loss: 9.812594 Dis Loss: 0.00017279049 |||| 10.952256 -13.687953 -6.8791375\n",
      "Iter: 1166 Gen Loss: 138.25394 Recon Loss: 138.33136 Gen ADV Loss: 10.461351 Dis Loss: 0.00015121727 |||| 12.042381 -14.864871 -6.144639\n",
      "Iter: 1167 Gen Loss: 144.15222 Recon Loss: 144.23566 Gen ADV Loss: 10.31485 Dis Loss: 0.00017138 |||| 11.270342 -13.718886 -6.186808\n",
      "Iter: 1168 Gen Loss: 143.04639 Recon Loss: 143.12822 Gen ADV Loss: 10.765188 Dis Loss: 0.00050769217 |||| 10.18647 -14.275805 -7.8168187\n",
      "Iter: 1169 Gen Loss: 152.0875 Recon Loss: 152.1791 Gen ADV Loss: 10.031249 Dis Loss: 0.00022172091 |||| 12.81048 -13.020892 -5.9841785\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "n_epochs = 30\n",
    "learning_rate_val = 0.002\n",
    "weight_decay_rate =  0.00001\n",
    "momentum = 0.9\n",
    "batch_size = 100\n",
    "lambda_recon = 0.999\n",
    "lambda_adv = 0.001\n",
    "\n",
    "overlap_size = 7\n",
    "hiding_size = 64\n",
    "\n",
    "trainset_path = '../data/dogs_trainset.pickle'\n",
    "testset_path  = '../data/dogs_testset.pickle'\n",
    "dataset_path = 'C:/Users/91709/Desktop/CV/dogs'\n",
    "model_path = 'C:/Users/91709/Desktop/CV/models/dogs/'\n",
    "result_path= '../results/dogs/'\n",
    "pretrained_model_path = None\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs( model_path )\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs( result_path )\n",
    "\n",
    "if not os.path.exists( trainset_path ) or not os.path.exists( testset_path ):\n",
    "\n",
    "    trainset_dir = os.path.join( dataset_path, 'dogs_train' )\n",
    "    testset_dir = os.path.join( dataset_path, 'dogs_eval' )\n",
    "\n",
    "    trainset = pd.DataFrame({'image_path': map(lambda x: os.path.join( trainset_dir, x ), os.listdir(trainset_dir))})\n",
    "    testset = pd.DataFrame({'image_path': map(lambda x: os.path.join( testset_dir, x ), os.listdir(testset_dir))})\n",
    "\n",
    "    trainset.to_pickle( trainset_path )\n",
    "    testset.to_pickle( testset_path )\n",
    "else:\n",
    "    trainset = pd.read_pickle( trainset_path )\n",
    "    testset = pd.read_pickle( testset_path )\n",
    "\n",
    "testset.index = range(len(testset))\n",
    "testset = testset.loc[np.random.permutation(len(testset))]\n",
    "is_train = tf.placeholder( tf.bool )\n",
    "\n",
    "learning_rate = tf.placeholder( tf.float32, [])\n",
    "images_tf = tf.placeholder( tf.float32, [batch_size, 128, 128, 3], name=\"images\")\n",
    "\n",
    "labels_D = tf.concat([tf.ones([batch_size]), tf.zeros([batch_size])],0)\n",
    "labels_G = tf.ones([batch_size])\n",
    "images_hiding = tf.placeholder( tf.float32, [batch_size, hiding_size, hiding_size, 3], name='images_hiding')\n",
    "\n",
    "model = Model()\n",
    "\n",
    "bn1, bn2, bn3, bn4, bn5, bn6, debn4, debn3, debn2, debn1, reconstruction_ori, reconstruction = model.build_reconstruction(images_tf, is_train)\n",
    "adversarial_pos = model.build_adversarial(images_hiding, is_train)\n",
    "adversarial_neg = model.build_adversarial(reconstruction, is_train, reuse=True)\n",
    "adversarial_all = tf.concat([adversarial_pos, adversarial_neg],0)\n",
    "\n",
    "# Applying bigger loss for overlapping region\n",
    "mask_recon = tf.pad(tf.ones([hiding_size - 2*overlap_size, hiding_size - 2*overlap_size]), [[overlap_size,overlap_size], [overlap_size,overlap_size]])\n",
    "mask_recon = tf.reshape(mask_recon, [hiding_size, hiding_size, 1])\n",
    "mask_recon = tf.concat([mask_recon]*3,2)\n",
    "mask_overlap = 1 - mask_recon\n",
    "\n",
    "loss_recon_ori = tf.square( images_hiding - reconstruction )\n",
    "loss_recon_center = tf.reduce_mean(tf.sqrt( 1e-5 + tf.reduce_sum(loss_recon_ori * mask_recon, [1,2,3])))  # Loss for non-overlapping region\n",
    "loss_recon_overlap = tf.reduce_mean(tf.sqrt( 1e-5 + tf.reduce_sum(loss_recon_ori * mask_overlap, [1,2,3]))) * 10. # Loss for overlapping region\n",
    "loss_recon = loss_recon_center + loss_recon_overlap\n",
    "\n",
    "loss_adv_D = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=adversarial_all, labels=labels_D))\n",
    "loss_adv_G = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=adversarial_neg, labels=labels_G))\n",
    "\n",
    "loss_G = loss_adv_G * lambda_adv + loss_recon * lambda_recon\n",
    "loss_D = loss_adv_D # * lambda_adv\n",
    "\n",
    "var_G = list(filter( lambda x: x.name.startswith('GEN'), tf.trainable_variables()))\n",
    "var_D = list(filter( lambda x: x.name.startswith('DIS'), tf.trainable_variables()))\n",
    "\n",
    "W_G = filter(lambda x: x.name.endswith('W:0'), var_G)\n",
    "W_D = filter(lambda x: x.name.endswith('W:0'), var_D)\n",
    "\n",
    "loss_G += weight_decay_rate * tf.reduce_mean(tf.stack( list(map(lambda x: tf.nn.l2_loss(x), W_G))))\n",
    "loss_D += weight_decay_rate * tf.reduce_mean(tf.stack( list(map(lambda x: tf.nn.l2_loss(x), W_D))))\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "optimizer_G = tf.train.AdamOptimizer( learning_rate=learning_rate, beta1=0.5 )\n",
    "grads_vars_G = optimizer_G.compute_gradients( loss_G, var_list=var_G )\n",
    "#grads_vars_G = map(lambda gv: [tf.clip_by_value(gv[0], -10., 10.), gv[1]], grads_vars_G)\n",
    "train_op_G = optimizer_G.apply_gradients( grads_vars_G )\n",
    "\n",
    "optimizer_D = tf.train.AdamOptimizer( learning_rate=learning_rate, beta1=0.5 )\n",
    "grads_vars_D = optimizer_D.compute_gradients( loss_D, var_list=var_D )\n",
    "#grads_vars_D = map(lambda gv: [tf.clip_by_value(gv[0], -10., 10.), gv[1]], grads_vars_D)\n",
    "train_op_D = optimizer_D.apply_gradients( grads_vars_D )\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "if pretrained_model_path is not None and os.path.exists( pretrained_model_path ):\n",
    "    saver.restore( sess, pretrained_model_path )\n",
    "\n",
    "iters = 0\n",
    "\n",
    "loss_D_val = 0.\n",
    "loss_G_val = 0.\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    trainset.index = range(len(trainset))\n",
    "    trainset = trainset.loc[np.random.permutation(len(trainset))]\n",
    "\n",
    "    for start,end in zip(\n",
    "            range(0, len(trainset), batch_size),\n",
    "            range(batch_size, len(trainset), batch_size)):\n",
    "\n",
    "        image_paths = trainset[start:end]['image_path'].values\n",
    "        images_ori = list(map(lambda x: load_image( x ), image_paths))\n",
    "\n",
    "        if iters % 2 == 0:\n",
    "            images_ori = list(map(lambda img: img[:,::-1,:], images_ori))\n",
    "\n",
    "        is_none = np.sum(list(map(lambda x: x is None, images_ori)))\n",
    "        if is_none > 0: continue\n",
    "\n",
    "        images_crops = list(map(lambda x: crop_random(x, x=32, y=32), images_ori))\n",
    "        images, crops,_,_ = zip(*images_crops)\n",
    "\n",
    "        # Printing activations every 100 iterations\n",
    "        if iters % 100 == 0:\n",
    "            test_image_paths = testset[:batch_size]['image_path'].values\n",
    "            test_images_ori = map(lambda x: load_image(x), test_image_paths)\n",
    "\n",
    "            test_images_crop = map(lambda x: crop_random(x, x=32, y=32), test_images_ori)\n",
    "            test_images, test_crops, xs,ys = zip(*test_images_crop)\n",
    "\n",
    "            reconstruction_vals, recon_ori_vals, bn1_val,bn2_val,bn3_val,bn4_val,bn5_val,bn6_val,debn4_val, debn3_val, debn2_val, debn1_val, loss_G_val, loss_D_val = sess.run(\n",
    "                    [reconstruction, reconstruction_ori, bn1,bn2,bn3,bn4,bn5,bn6,debn4, debn3, debn2, debn1, loss_G, loss_D],\n",
    "                    feed_dict={\n",
    "                        images_tf: test_images,\n",
    "                        images_hiding: test_crops,\n",
    "                        is_train: False\n",
    "                        })\n",
    "\n",
    "            # Generate result every 1000 iterations\n",
    "            if iters % 100 == 0:\n",
    "                ii = 0\n",
    "                for rec_val, img,x,y in zip(reconstruction_vals, test_images, xs, ys):\n",
    "                    rec_hid = (255. * (rec_val+1)/2.).astype(int)\n",
    "                    rec_con = (255. * (img+1)/2.).astype(int)\n",
    "\n",
    "                    rec_con[y:y+64, x:x+64] = rec_hid\n",
    "                    cv2.imwrite( os.path.join(result_path, 'img_'+str(ii)+'.'+str(int(iters/100))+'.jpg'), rec_con)\n",
    "                    ii += 1\n",
    "\n",
    "                if iters == 0:\n",
    "                    for test_image in test_images_ori:\n",
    "                        test_image = (255. * (test_image+1)/2.).astype(int)\n",
    "                        test_image[32:32+64,32:32+64] = 0\n",
    "                        cv2.imwrite( os.path.join(result_path, 'img_'+str(ii)+'.ori.jpg'), test_image)\n",
    "\n",
    "            print( \"========================================================================\")\n",
    "            print (bn1_val.max(), bn1_val.min())\n",
    "            print (bn2_val.max(), bn2_val.min())\n",
    "            print (bn3_val.max(), bn3_val.min())\n",
    "            print (bn4_val.max(), bn4_val.min())\n",
    "            print (bn5_val.max(), bn5_val.min())\n",
    "            print (bn6_val.max(), bn6_val.min())\n",
    "            print (debn4_val.max(), debn4_val.min())\n",
    "            print (debn3_val.max(), debn3_val.min())\n",
    "            print (debn2_val.max(), debn2_val.min())\n",
    "            print (debn1_val.max(), debn1_val.min())\n",
    "            print (recon_ori_vals.max(), recon_ori_vals.min())\n",
    "            print (reconstruction_vals.max(), reconstruction_vals.min())\n",
    "            print (loss_G_val, loss_D_val)\n",
    "            print (\"=========================================================================\")\n",
    "\n",
    "            if np.isnan(reconstruction_vals.min() ) or np.isnan(reconstruction_vals.max()):\n",
    "                print (\"NaN detected!!\")\n",
    "                ipdb.set_trace()\n",
    "\n",
    "        # Generative Part is updated every iteration\n",
    "        _, loss_G_val, adv_pos_val, adv_neg_val, loss_recon_val, loss_adv_G_val, reconstruction_vals, recon_ori_vals, bn1_val,bn2_val,bn3_val,bn4_val,bn5_val,bn6_val,debn4_val, debn3_val, debn2_val, debn1_val = sess.run([train_op_G, loss_G, adversarial_pos, adversarial_neg, loss_recon, loss_adv_G, reconstruction, reconstruction_ori, bn1,bn2,bn3,bn4,bn5,bn6,debn4, debn3, debn2, debn1],feed_dict={images_tf: images,images_hiding: crops,learning_rate: learning_rate_val,is_train: True})\n",
    "\n",
    "        _, loss_D_val, adv_pos_val, adv_neg_val = sess.run([train_op_D, loss_D, adversarial_pos, adversarial_neg],feed_dict={images_tf: images,images_hiding: crops,learning_rate: learning_rate_val/10.,is_train: True})\n",
    "\n",
    "        print (\"Iter:\", iters, \"Gen Loss:\", loss_G_val, \"Recon Loss:\", loss_recon_val, \"Gen ADV Loss:\", loss_adv_G_val,  \"Dis Loss:\", loss_D_val, \"||||\", adv_pos_val.mean(), adv_neg_val.min(), adv_neg_val.max())\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "\n",
    "    saver.save(sess, model_path + 'model', global_step=epoch)\n",
    "    learning_rate_val *= 0.99"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
